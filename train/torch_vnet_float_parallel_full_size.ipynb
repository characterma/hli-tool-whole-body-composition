{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eba3718-1945-428a-a81d-3e7f64b1bedc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collecting loguru\n",
       "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
       "Collecting awscli\n",
       "  Downloading awscli-1.27.4-py3-none-any.whl (3.9 MB)\n",
       "Collecting nibabel\n",
       "  Downloading nibabel-4.0.2-py3-none-any.whl (3.3 MB)\n",
       "Collecting PyYAML<5.5,>=3.10\n",
       "  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
       "Collecting s3transfer<0.7.0,>=0.6.0\n",
       "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
       "Collecting docutils<0.17,>=0.10\n",
       "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
       "Collecting rsa<4.8,>=3.1.2\n",
       "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
       "Collecting colorama<0.4.5,>=0.2.5\n",
       "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
       "Collecting botocore==1.29.4\n",
       "  Downloading botocore-1.29.4-py3-none-any.whl (9.8 MB)\n",
       "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.9/site-packages (from botocore==1.29.4->awscli) (2.8.2)\n",
       "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /databricks/python3/lib/python3.9/site-packages (from botocore==1.29.4->awscli) (1.26.7)\n",
       "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.9/site-packages (from botocore==1.29.4->awscli) (0.10.0)\n",
       "Requirement already satisfied: packaging>=17.0 in /databricks/python3/lib/python3.9/site-packages (from nibabel) (21.0)\n",
       "Requirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.9/site-packages (from nibabel) (1.20.3)\n",
       "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nibabel) (58.0.4)\n",
       "Requirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=17.0->nibabel) (3.0.4)\n",
       "Requirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.29.4->awscli) (1.16.0)\n",
       "Requirement already satisfied: pyasn1>=0.1.3 in /databricks/python3/lib/python3.9/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
       "Installing collected packages: botocore, s3transfer, rsa, PyYAML, docutils, colorama, nibabel, loguru, awscli\n",
       "  Attempting uninstall: botocore\n",
       "    Found existing installation: botocore 1.24.18\n",
       "    Uninstalling botocore-1.24.18:\n",
       "      Successfully uninstalled botocore-1.24.18\n",
       "  Attempting uninstall: s3transfer\n",
       "    Found existing installation: s3transfer 0.5.2\n",
       "    Uninstalling s3transfer-0.5.2:\n",
       "      Successfully uninstalled s3transfer-0.5.2\n",
       "  Attempting uninstall: rsa\n",
       "    Found existing installation: rsa 4.8\n",
       "    Uninstalling rsa-4.8:\n",
       "      Successfully uninstalled rsa-4.8\n",
       "  Attempting uninstall: PyYAML\n",
       "    Found existing installation: PyYAML 6.0\n",
       "    Uninstalling PyYAML-6.0:\n",
       "      Successfully uninstalled PyYAML-6.0\n",
       "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
       "boto3 1.21.18 requires botocore<1.25.0,>=1.24.18, but you have botocore 1.29.4 which is incompatible.\n",
       "boto3 1.21.18 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.6.0 which is incompatible.\n",
       "Successfully installed PyYAML-5.4.1 awscli-1.27.4 botocore-1.29.4 colorama-0.4.4 docutils-0.16 loguru-0.6.0 nibabel-4.0.2 rsa-4.7.2 s3transfer-0.6.0\n",
       "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
       "You should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Collecting loguru\n  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\nCollecting awscli\n  Downloading awscli-1.27.4-py3-none-any.whl (3.9 MB)\nCollecting nibabel\n  Downloading nibabel-4.0.2-py3-none-any.whl (3.3 MB)\nCollecting PyYAML<5.5,>=3.10\n  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\nCollecting s3transfer<0.7.0,>=0.6.0\n  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\nCollecting docutils<0.17,>=0.10\n  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\nCollecting rsa<4.8,>=3.1.2\n  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\nCollecting colorama<0.4.5,>=0.2.5\n  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\nCollecting botocore==1.29.4\n  Downloading botocore-1.29.4-py3-none-any.whl (9.8 MB)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.9/site-packages (from botocore==1.29.4->awscli) (2.8.2)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /databricks/python3/lib/python3.9/site-packages (from botocore==1.29.4->awscli) (1.26.7)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.9/site-packages (from botocore==1.29.4->awscli) (0.10.0)\nRequirement already satisfied: packaging>=17.0 in /databricks/python3/lib/python3.9/site-packages (from nibabel) (21.0)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.9/site-packages (from nibabel) (1.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nibabel) (58.0.4)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=17.0->nibabel) (3.0.4)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.29.4->awscli) (1.16.0)\nRequirement already satisfied: pyasn1>=0.1.3 in /databricks/python3/lib/python3.9/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\nInstalling collected packages: botocore, s3transfer, rsa, PyYAML, docutils, colorama, nibabel, loguru, awscli\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.24.18\n    Uninstalling botocore-1.24.18:\n      Successfully uninstalled botocore-1.24.18\n  Attempting uninstall: s3transfer\n    Found existing installation: s3transfer 0.5.2\n    Uninstalling s3transfer-0.5.2:\n      Successfully uninstalled s3transfer-0.5.2\n  Attempting uninstall: rsa\n    Found existing installation: rsa 4.8\n    Uninstalling rsa-4.8:\n      Successfully uninstalled rsa-4.8\n  Attempting uninstall: PyYAML\n    Found existing installation: PyYAML 6.0\n    Uninstalling PyYAML-6.0:\n      Successfully uninstalled PyYAML-6.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nboto3 1.21.18 requires botocore<1.25.0,>=1.24.18, but you have botocore 1.29.4 which is incompatible.\nboto3 1.21.18 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.6.0 which is incompatible.\nSuccessfully installed PyYAML-5.4.1 awscli-1.27.4 botocore-1.29.4 colorama-0.4.4 docutils-0.16 loguru-0.6.0 nibabel-4.0.2 rsa-4.7.2 s3transfer-0.6.0\nWARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "/databricks/python/bin/pip install loguru awscli nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83270c34-a1e2-4f63-9881-9ad746daee46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import cat\n",
    "from loguru import logger\n",
    "# import deepspeed\n",
    "# from apex import amp\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os, time, sys\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "035dfd35-280d-46a1-b0b3-431171c794dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# (1) base version model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8564b49-50d6-4e49-aee6-4184c0e4b562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3D-UNet model.\n",
    "# x: 128x128 resolution for 32 frames.\n",
    "# https://github.com/huangzhii/FCN-3D-pytorch/blob/master/main3d.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "def passthrough(x, **kwargs):\n",
    "    return x\n",
    "\n",
    "\n",
    "def ELUCons(elu, nchan):\n",
    "    if elu:\n",
    "        return nn.ELU(inplace=True)\n",
    "    else:\n",
    "        return nn.PReLU(nchan)\n",
    "\n",
    "\n",
    "class LUConv(nn.Module):\n",
    "    def __init__(self, nchan, elu):\n",
    "        super(LUConv, self).__init__()\n",
    "        self.relu1 = ELUCons(elu, nchan)\n",
    "        self.conv1 = nn.Conv3d(nchan, nchan, kernel_size=5, padding=2)\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm3d(nchan)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "def _make_nConv(nchan, depth, elu):\n",
    "    layers = []\n",
    "    for _ in range(depth):\n",
    "        layers.append(LUConv(nchan, elu))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class InputTransition(nn.Module):\n",
    "    def __init__(self, in_channels, elu):\n",
    "        super(InputTransition, self).__init__()\n",
    "        self.num_features = 16\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.conv1 = nn.Conv3d(self.in_channels, self.num_features, kernel_size=5, padding=2)\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm3d(self.num_features)\n",
    "\n",
    "        self.relu1 = ELUCons(elu, self.num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        repeat_rate = int(self.num_features / self.in_channels)\n",
    "        out = self.bn1(out)\n",
    "        x16 = x.repeat(1, repeat_rate, 1, 1, 1)\n",
    "        return self.relu1(torch.add(out, x16))\n",
    "\n",
    "\n",
    "class DownTransition(nn.Module):\n",
    "    def __init__(self, inChans, nConvs, elu, dropout=False):\n",
    "        super(DownTransition, self).__init__()\n",
    "        outChans = 2 * inChans\n",
    "        self.down_conv = nn.Conv3d(inChans, outChans, kernel_size=2, stride=2)\n",
    "        self.bn1 = torch.nn.BatchNorm3d(outChans)\n",
    "\n",
    "        self.do1 = passthrough\n",
    "        self.relu1 = ELUCons(elu, outChans)\n",
    "        self.relu2 = ELUCons(elu, outChans)\n",
    "        if dropout:\n",
    "            self.do1 = nn.Dropout3d()\n",
    "        self.ops = _make_nConv(outChans, nConvs, elu)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down = self.relu1(self.bn1(self.down_conv(x)))\n",
    "        out = self.do1(down)\n",
    "        out = self.ops(out)\n",
    "        out = self.relu2(torch.add(out, down))\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpTransition(nn.Module):\n",
    "    def __init__(self, inChans, outChans, nConvs, elu, dropout=False):\n",
    "        super(UpTransition, self).__init__()\n",
    "        self.up_conv = nn.ConvTranspose3d(inChans, outChans // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm3d(outChans // 2)\n",
    "        self.do1 = passthrough\n",
    "        self.do2 = nn.Dropout3d()\n",
    "        self.relu1 = ELUCons(elu, outChans // 2)\n",
    "        self.relu2 = ELUCons(elu, outChans)\n",
    "        if dropout:\n",
    "            self.do1 = nn.Dropout3d()\n",
    "        self.ops = _make_nConv(outChans, nConvs, elu)\n",
    "\n",
    "    def forward(self, x, skipx):\n",
    "        out = self.do1(x)\n",
    "        skipxdo = self.do2(skipx)\n",
    "        out = self.relu1(self.bn1(self.up_conv(out)))\n",
    "        xcat = torch.cat((out, skipxdo), 1)\n",
    "        out = self.ops(xcat)\n",
    "        out = self.relu2(torch.add(out, xcat))\n",
    "        return out\n",
    "\n",
    "\n",
    "class OutputTransition(nn.Module):\n",
    "    def __init__(self, in_channels, classes, elu):\n",
    "        super(OutputTransition, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.conv1 = nn.Conv3d(in_channels, classes, kernel_size=5, padding=2)\n",
    "        self.bn1 = torch.nn.BatchNorm3d(classes)\n",
    "\n",
    "        self.conv2 = nn.Conv3d(classes, classes, kernel_size=1)\n",
    "        self.relu1 = ELUCons(elu, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convolve 32 down to channels as the desired classes\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.conv2(out)\n",
    "        return out\n",
    "\n",
    "class UpTransitionNoConv(nn.Module):\n",
    "    def __init__(self, inChans, outChans, nConvs, elu, dropout=False):\n",
    "        # inChans=62, outChans=32, nConvs=1, elu=True\n",
    "        super(UpTransitionNoConv, self).__init__()\n",
    "        self.up_conv = nn.ConvTranspose3d(inChans, outChans // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm3d(outChans // 2)\n",
    "        self.do1 = passthrough\n",
    "        self.do2 = nn.Dropout3d()\n",
    "        self.relu1 = ELUCons(elu, outChans // 2)\n",
    "        if dropout:\n",
    "            self.do1 = nn.Dropout3d()\n",
    "        \n",
    "\n",
    "    def forward(self, x, skipx):\n",
    "        out = self.do1(x)\n",
    "        skipxdo = self.do2(skipx)\n",
    "        out = self.relu1(self.bn1(self.up_conv(out)))\n",
    "        xcat = torch.cat((out, skipxdo), 1)\n",
    "        return xcat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ad26a8-b844-4d1c-97c3-1e5f9b356473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# --- 1.基础的vnet3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2676dc89-aa4f-41c2-ac5e-07097a36fd8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class VNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementations based on the Vnet paper: https://arxiv.org/abs/1606.04797\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, elu=True, in_channels=1, classes=1):\n",
    "        super(VNet, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.in_tr = InputTransition(in_channels, elu=elu)\n",
    "        self.down_tr32 = DownTransition(16, 1, elu)\n",
    "        self.down_tr64 = DownTransition(32, 2, elu)\n",
    "        self.down_tr128 = DownTransition(64, 3, elu, dropout=False)\n",
    "        self.down_tr256 = DownTransition(128, 2, elu, dropout=False)\n",
    "        self.up_tr256 = UpTransition(256, 256, 2, elu, dropout=False)\n",
    "        self.up_tr128 = UpTransition(256, 128, 2, elu, dropout=False)\n",
    "        self.up_tr64 = UpTransition(128, 64, 1, elu)\n",
    "        self.up_tr32 = UpTransition(64, 32, 1, elu)\n",
    "        self.out_tr = OutputTransition(32, classes, elu)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out16 = self.in_tr(x)\n",
    "        out32 = self.down_tr32(out16)\n",
    "        out64 = self.down_tr64(out32)\n",
    "        out128 = self.down_tr128(out64)\n",
    "        out256 = self.down_tr256(out128)\n",
    "        out = self.up_tr256(out256, out128)\n",
    "        out = self.up_tr128(out, out64)\n",
    "        out = self.up_tr64(out, out32)\n",
    "        out = self.up_tr32(out, out16)\n",
    "        out = self.out_tr(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "184ff85d-67fe-4c39-afe5-0f582a9b0617",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# --- 2.模型并行VNet3d版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "826c781b-9632-4fd1-b46b-3c36cdb98d10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devices = ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cpu']\n",
    "\n",
    "class VNet_Parallelism(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementations based on the Vnet paper: https://arxiv.org/abs/1606.04797\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, elu=True, in_channels=1, classes=1):\n",
    "        super(VNet_Parallelism, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.in_channels = in_channels\n",
    "        logger.info(\"begin initialize model struction\")\n",
    "        self.in_tr = InputTransition(in_channels, elu=elu).to(devices[0])\n",
    "        self.down_tr32 = DownTransition(16, 1, elu).to(devices[0])\n",
    "        self.down_tr64 = DownTransition(32, 2, elu).to(devices[0])\n",
    "        self.down_tr128 = DownTransition(64, 3, elu, dropout=False).to(devices[1])\n",
    "        self.down_tr256 = DownTransition(128, 2, elu, dropout=False).to(devices[1])\n",
    "        self.up_tr256 = UpTransition(256, 256, 2, elu, dropout=False).to(devices[1])        \n",
    "        self.up_tr128 = UpTransition(256, 128, 2, elu, dropout=False).to(devices[2])\n",
    "        self.up_tr64 = UpTransition(128, 64, 1, elu).to(devices[2])\n",
    "        # 版本1\n",
    "        # self.up_tr32 = UpTransition(64, 32, 1, elu).to(devices[3])\n",
    "        \n",
    "        # 版本2\n",
    "        self.up_tr32 = UpTransitionNoConv(64, 32, 1, elu).to(devices[3])\n",
    "        self.up_tr32_ops = _make_nConv(32, 1, elu).to(devices[2])\n",
    "        self.up_tr32_relu2 = ELUCons(elu, 32).to(devices[2])\n",
    "\n",
    "        \n",
    "        self.out_tr = OutputTransition(32, classes, elu).to(devices[1])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out16 = self.in_tr(x.to(devices[0]))\n",
    "        logger.debug(f\"out16: {out16.shape}\")\n",
    "        out32 = self.down_tr32(out16)\n",
    "        logger.debug(f\"out32: {out32.shape}\")\n",
    "        out64 = self.down_tr64(out32)\n",
    "        # print(f\"out64: {out64.shape}, self.down_tr128: {next(self.down_tr128.parameters()).device}\")\n",
    "        out128 = self.down_tr128(out64.to(devices[1]))\n",
    "        logger.debug(f\"out128: {out128.shape}\")\n",
    "        out256 = self.down_tr256(out128)\n",
    "        logger.debug(f\"out256: {out256.shape}, out128: {out128.shape}\")\n",
    "        out = self.up_tr256(out256, out128)\n",
    "        logger.debug(f\"out: {out.shape}， out64: {out64.shape}\")\n",
    "        out = self.up_tr128(out.to(devices[2]), out64.to(devices[2]))\n",
    "        logger.debug(f\"out: {out.shape}, out32: {out32.shape}\")\n",
    "        out = self.up_tr64(out.to(devices[2]), out32.to(devices[2]))\n",
    "        logger.debug(f\"out: {out.shape}, out16: {out16.shape}\")\n",
    "        # 版本1\n",
    "        # out = self.up_tr32(out.to(devices[3]), out16.to(devices[3]))\n",
    "\n",
    "        # 版本2\n",
    "        out_uptr32 = self.up_tr32(out.to(devices[3]), out16.to(devices[3]))\n",
    "        out_uptr32_ops_val = self.up_tr32_ops(out_uptr32.to(devices[2]))\n",
    "        out = self.up_tr32_relu2(torch.add(out_uptr32_ops_val, out_uptr32.to(devices[2])))\n",
    "\n",
    "        logger.debug(f\"out: {out.shape}\")\n",
    "        out = self.out_tr(out.to(devices[1]))\n",
    "        logger.debug(f\"out: {out.shape}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1b5cd1c-7410-4237-8462-89359034d3f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# --- 3. version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e189196d-b0b4-4af0-a944-9ef6383f4426",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devices = ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cpu']\n",
    "\n",
    "class VNet_Parallelism(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementations based on the Vnet paper: https://arxiv.org/abs/1606.04797\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, elu=True, in_channels=1, classes=1):\n",
    "        super(VNet_Parallelism, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.in_channels = in_channels\n",
    "        logger.info(\"begin initialize model struction\")\n",
    "        self.in_tr = InputTransition(in_channels, elu=elu).to(devices[0])\n",
    "        self.down_tr32 = DownTransition(16, 1, elu).to(devices[0])\n",
    "        self.down_tr64 = DownTransition(32, 2, elu).to(devices[0])\n",
    "        self.down_tr128 = DownTransition(64, 3, elu, dropout=False).to(devices[1])\n",
    "        self.down_tr256 = DownTransition(128, 2, elu, dropout=False).to(devices[1])\n",
    "        self.up_tr256 = UpTransition(256, 256, 2, elu, dropout=False).to(devices[1])        \n",
    "        self.up_tr128 = UpTransition(256, 128, 2, elu, dropout=False).to(devices[2])\n",
    "        self.up_tr64 = UpTransition(128, 64, 1, elu).to(devices[2])\n",
    "        # 版本1\n",
    "        # self.up_tr32 = UpTransition(64, 32, 1, elu).to(devices[3])\n",
    "        \n",
    "        # 版本2\n",
    "        self.up_tr32 = UpTransitionNoConv(64, 32, 1, elu).to(devices[3])\n",
    "        self.up_tr32_ops = _make_nConv(32, 1, elu).to(devices[2])\n",
    "        self.up_tr32_relu2 = ELUCons(elu, 32).to(devices[0])\n",
    "\n",
    "        \n",
    "        self.out_tr = OutputTransition(32, classes, elu).to(devices[1])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out16 = self.in_tr(x.to(devices[0]))\n",
    "        logger.debug(f\"out16: {out16.shape}\")\n",
    "        out32 = self.down_tr32(out16)\n",
    "        logger.debug(f\"out32: {out32.shape}\")\n",
    "        out64 = self.down_tr64(out32)\n",
    "        # print(f\"out64: {out64.shape}, self.down_tr128: {next(self.down_tr128.parameters()).device}\")\n",
    "        out128 = self.down_tr128(out64.to(devices[1]))\n",
    "        logger.debug(f\"out128: {out128.shape}\")\n",
    "        out256 = self.down_tr256(out128)\n",
    "        logger.debug(f\"out256: {out256.shape}, out128: {out128.shape}\")\n",
    "        out = self.up_tr256(out256, out128)\n",
    "        logger.debug(f\"out: {out.shape}， out64: {out64.shape}\")\n",
    "        out = self.up_tr128(out.to(devices[2]), out64.to(devices[2]))\n",
    "        logger.debug(f\"out: {out.shape}, out32: {out32.shape}\")\n",
    "        out = self.up_tr64(out.to(devices[2]), out32.to(devices[2]))\n",
    "        logger.debug(f\"out: {out.shape}, out16: {out16.shape}\")\n",
    "        # 版本1\n",
    "        # out = self.up_tr32(out.to(devices[3]), out16.to(devices[3]))\n",
    "\n",
    "        # 版本2\n",
    "        out_uptr32 = self.up_tr32(out.to(devices[3]), out16.to(devices[3]))\n",
    "        out_uptr32_ops_val = self.up_tr32_ops(out_uptr32.to(devices[2]))\n",
    "        out = self.up_tr32_relu2(torch.add(out_uptr32_ops_val.to(devices[0]), out_uptr32.to(devices[0])))\n",
    "\n",
    "        logger.debug(f\"out: {out.shape}\")\n",
    "        out = self.out_tr(out.to(devices[1]))\n",
    "        logger.debug(f\"out: {out.shape}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a0a0305-905d-4666-a386-c6e42d97e8b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "--- cpu loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4829df59-ef26-4227-8ea2-4eb401b9b29f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class VNet_Parallelism(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementations based on the Vnet paper: https://arxiv.org/abs/1606.04797\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, elu=True, in_channels=1, classes=1):\n",
    "        super(VNet_Parallelism, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.in_channels = in_channels\n",
    "        logger.info(\"begin initialize model struction\")\n",
    "        self.in_tr = InputTransition(in_channels, elu=elu)\n",
    "        self.down_tr32 = DownTransition(16, 1, elu)\n",
    "        self.down_tr64 = DownTransition(32, 2, elu)\n",
    "        self.down_tr128 = DownTransition(64, 3, elu, dropout=False)\n",
    "        self.down_tr256 = DownTransition(128, 2, elu, dropout=False)\n",
    "        self.up_tr256 = UpTransition(256, 256, 2, elu, dropout=False)       \n",
    "        self.up_tr128 = UpTransition(256, 128, 2, elu, dropout=False)\n",
    "        self.up_tr64 = UpTransition(128, 64, 1, elu)\n",
    "        # 版本1\n",
    "        # self.up_tr32 = UpTransition(64, 32, 1, elu).to(devices[3])\n",
    "        \n",
    "        # 版本2\n",
    "        self.up_tr32 = UpTransitionNoConv(64, 32, 1, elu)\n",
    "        self.up_tr32_ops = _make_nConv(32, 1, elu)\n",
    "        self.up_tr32_relu2 = ELUCons(elu, 32)\n",
    "\n",
    "        \n",
    "        self.out_tr = OutputTransition(32, classes, elu)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out16 = self.in_tr(x)\n",
    "        logger.debug(f\"out16: {out16.shape}\")\n",
    "        out32 = self.down_tr32(out16)\n",
    "        logger.debug(f\"out32: {out32.shape}\")\n",
    "        out64 = self.down_tr64(out32)\n",
    "        # print(f\"out64: {out64.shape}, self.down_tr128: {next(self.down_tr128.parameters()).device}\")\n",
    "        out128 = self.down_tr128(out64)\n",
    "        logger.debug(f\"out128: {out128.shape}\")\n",
    "        out256 = self.down_tr256(out128)\n",
    "        logger.debug(f\"out256: {out256.shape}, out128: {out128.shape}\")\n",
    "        out = self.up_tr256(out256, out128)\n",
    "        logger.debug(f\"out: {out.shape}， out64: {out64.shape}\")\n",
    "        out = self.up_tr128(out, out64)\n",
    "        logger.debug(f\"out: {out.shape}, out32: {out32.shape}\")\n",
    "        out = self.up_tr64(out, out32)\n",
    "        logger.debug(f\"out: {out.shape}, out16: {out16.shape}\")\n",
    "        # 版本1\n",
    "        # out = self.up_tr32(out.to(devices[3]), out16.to(devices[3]))\n",
    "\n",
    "        # 版本2\n",
    "        out_uptr32 = self.up_tr32(out, out16)\n",
    "        out_uptr32_ops_val = self.up_tr32_ops(out_uptr32)\n",
    "        out = self.up_tr32_relu2(torch.add(out_uptr32_ops_val, out_uptr32))\n",
    "\n",
    "        logger.debug(f\"out: {out.shape}\")\n",
    "        out = self.out_tr(out)\n",
    "        logger.debug(f\"out: {out.shape}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22222fb9-f685-4a54-9ced-1d7cc6db2310",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# (2) data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54fe2af8-aed3-406c-bfd8-e289fc6eaac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SliceAndResize:\n",
    "    \n",
    "    @staticmethod\n",
    "    def mask2onehot(mask:np.array, num_classes:int):\n",
    "        \"\"\"\n",
    "        Converts a segmentation mask (H,W) to (channel,H,W) where the last dim is a one\n",
    "        hot encoding vector\n",
    "        Convert class index tensor to one hot encoding tensor.\n",
    "        Args:\n",
    "             input: A tensor of shape [1, *], * represents a complete dimension, [channel, height, width, length]\n",
    "             num_classes: An int of number of class\n",
    "        Returns:\n",
    "            A tensor of shape [num_classes, *]\n",
    "\n",
    "        \"\"\"\n",
    "        _mask = [mask == i for i in range(num_classes)]\n",
    "        return np.array(_mask).astype(np.uint8)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def resize_images(img: np.array, resize_ratio=None):\n",
    "        if resize_ratio is None:\n",
    "            resize_ratio = [0.5] * len(img.shape)\n",
    "        logger.debug(f\"resize ratio: {resize_ratio}\")\n",
    "        img = zoom(img, resize_ratio, order=0, mode='nearest')\n",
    "        return img\n",
    "    \n",
    "    @staticmethod\n",
    "    def slice_and_resize(x:np.array, y:np.array, slice_interval:list, resize_shape:list, num_classes:int):\n",
    "        logger.debug(f\"x: {x.shape}, y: {y.shape}, slice_interval: {slice_interval}, resize_shape: {resize_shape}, num_classes: {num_classes}\")\n",
    "        slice_x = x\n",
    "        slice_y = y\n",
    "#         slice_x = x[slice_interval[0]:slice_interval[1], :, :]\n",
    "#         slice_y = y[slice_interval[0]:slice_interval[1], :, :]\n",
    "#         slice_x = x[slice_interval[0]:slice_interval[1], 2:-2, :]\n",
    "#         slice_y = y[slice_interval[0]:slice_interval[1], 2:-2, :]\n",
    "        # draw(slice_x[278-slice_interval[0]], slice_y[278-slice_interval[0]])\n",
    "        assert slice_x.shape == slice_y.shape, Exception(\"The dimensions of x and y should be the same\")\n",
    "        resize_ratio = [i/j for i, j in zip(resize_shape, slice_x.shape)]\n",
    "#         resize_x = SliceAndResize.resize_images(slice_x, resize_ratio)\n",
    "#         resize_y = SliceAndResize.resize_images(slice_y, resize_ratio)\n",
    "        resize_x = slice_x\n",
    "        resize_y = slice_y\n",
    "        # draw(resize_x[278-slice_interval[0]], resize_y[278-slice_interval[0]])\n",
    "        output_x = np.expand_dims(resize_x, axis=0)\n",
    "        output_y = SliceAndResize.mask2onehot(resize_y, num_classes)\n",
    "        logger.debug(f'slice x: {slice_x.shape}, slice y: {slice_y.shape}, resize_x: {resize_x.shape}, resize_y: {resize_y.shape}, output x: {output_x.shape}, output_y: {output_y.shape}')\n",
    "        # draw_multi(output_x[0][278-slice_interval[0]], output_y[1][278-slice_interval[0]], output_y[2][278-slice_interval[0]])\n",
    "        return output_x, output_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e3005f3-f2b2-4d15-992b-71a9ebc71d60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    define data generator based on Sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 list_IDs,\n",
    "                 file_path=\"./data/\",\n",
    "                 num_classes=3,\n",
    "                 resize_shape:list=[208, 96, 112],\n",
    "                 slice_interval:list=[220, 428],\n",
    "                 to_fit=True,\n",
    "                 x_prefix: str='fat', \n",
    "                 y_prefix: str='fat_label',\n",
    "                 x_postfix: str = \"npy\", \n",
    "                 y_postfix: str = \"npy\"\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        data generateor tool\n",
    "        :param list_IDs: user id list\n",
    "        :param file_path: data file path\n",
    "        :param resize_shape: resize image shape \n",
    "        :param slice_interval: slice interval range\n",
    "        :param num_classes: classifier number \n",
    "        :param to_fit: if True, return (x, y), if False only return y\n",
    "        :param x_prefix: feature data prefix\n",
    "        :param x_postfix: feature data postfix\n",
    "        :param y_prefix: label data prefix\n",
    "        :param y_postfix: label data postfix\n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.resize_shape = resize_shape\n",
    "        self.slice_interval = slice_interval\n",
    "        self.num_classes = num_classes\n",
    "        self.file_path = file_path\n",
    "        self.to_fit = to_fit\n",
    "        self.x_prefix = x_prefix\n",
    "        self.y_prefix = y_prefix\n",
    "        self.x_postfix = x_postfix\n",
    "        self.y_postfix = y_postfix\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        data number\n",
    "        \"\"\"\n",
    "#         logger.info(f'list ids number: {len(self.list_IDs)}')\n",
    "        return len(self.list_IDs)\n",
    "        \n",
    "\n",
    "    def __generate_x_y_data(self, index):\n",
    "        \"\"\"\n",
    "        generate x and y data through batch_user_file_path\n",
    "        create completed x and y file path\n",
    "        :param x_prefix: feature data prefix\n",
    "        :param x_postfix: feature data postfix\n",
    "        :param y_prefix: label data prefix\n",
    "        :param y_postfix: label data postfix\n",
    "        :return: all user data file path info about this batch\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ids = self.list_IDs[index]\n",
    "        x_path = os.path.join(os.path.join(self.file_path, ids), f\"{self.x_prefix}.{self.x_postfix}\")\n",
    "        y_path = os.path.join(os.path.join(self.file_path, ids), f\"{self.y_prefix}.{self.y_postfix}\")\n",
    "        x = np.load(x_path)\n",
    "        y = np.load(y_path)\n",
    "#         x = np.random.random([1] + self.resize_shape).astype(np.float32)\n",
    "#         y = np.random.random([3] + self.resize_shape).astype(np.float32)\n",
    "#         slice = 278\n",
    "        # draw(x[slice], y[slice])\n",
    "        logger.debug(f'ids: {ids}, origin x shape: {x.shape}, {type(x)}， origin y shape: {y.shape}, {type(y)}')\n",
    "        x, y = SliceAndResize.slice_and_resize(x=x, y=y, slice_interval=self.slice_interval, resize_shape=self.resize_shape, num_classes=self.num_classes)\n",
    "        # draw_multi(x[0][slice-slice_interval[0]], y[1][slice-slice_interval[0]], y[2][slice-slice_interval[0]])\n",
    "\n",
    "        if self.to_fit:\n",
    "            logger.debug(f'train status：{self.to_fit}, ids: {ids}, x shape: {x.shape}， y shape: {y.shape}')\n",
    "            return ids, x, y \n",
    "        else:\n",
    "            logger.debug(f'test status：{self.to_fit}, ids: {ids}, x shape: {x.shape}')\n",
    "            return ids, x\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        :param item: example index\n",
    "        :param x_prefix: feature data prefix\n",
    "        :param x_postfix: feature data postfix\n",
    "        :param y_prefix: label data prefix\n",
    "        :param y_postfix: label data postfix\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logger.debug(f'example index: {item}')\n",
    "        if self.to_fit:\n",
    "            logger.debug('begin generate x and y')\n",
    "            ids, x, y = self.__generate_x_y_data(item)\n",
    "            return ids, x, y\n",
    "        else:\n",
    "            ids, x = self.__generate_x_y_data(item)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0dbed71-0956-45f2-bceb-a7fdf341589c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# (3) eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e268fcd-964c-49b8-b940-8b2d7c76250e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class EvalMetric:\n",
    "    def __init__(self, dim, batch_size, n_classes, smooth=1):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.smooth = smooth\n",
    "        # self.ones_arr = K.ones((self.batch_size, *self.dim, self.n_classes))\n",
    "\n",
    "        \n",
    "    def dice_coef(self, y_true, y_pred):\n",
    "        assert len(y_true.shape) == len(y_pred.shape), Exception(\"y_Ture and Y_PRED has different dimensions\")\n",
    "        \n",
    "#         y_true, y_pred = y_true.float(), y_pred.float()\n",
    "        y_true, y_pred = y_true.type(torch.float32), y_pred.type(torch.float32)\n",
    "        \n",
    "        logger.debug(f\"y_true: {y_true.shape}, y_pred: {y_pred.shape}\")\n",
    "        \n",
    "        y_true = torch.flatten(y_true)\n",
    "        y_pred = torch.flatten(y_pred)\n",
    "        logger.debug(f\"y_true: {y_true.shape}, y_pred: {y_pred.shape}\")\n",
    "        \n",
    "        intersection = torch.sum(y_true * y_pred)\n",
    "        val = (2. * intersection + self.smooth) / (torch.sum(y_true) + torch.sum(y_pred) + self.smooth)\n",
    "#         print('jwu dice_coef intersection: ', intersection)\n",
    "        logger.debug(f\"intersection: {intersection}, val: {val}\")\n",
    "        return val\n",
    "\n",
    "      \n",
    "    def dice_coef_multilabel(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        exclude background\n",
    "        \"\"\"\n",
    "#         print('jwu dice_coef_multilabel: ', torch.max(y_true), torch.min(y_true), torch.max(y_pred), torch.min(y_pred))\n",
    "        \n",
    "        dice = 0\n",
    "        for index in range(1, self.n_classes):\n",
    "            dice += self.dice_coef(y_true[:, index, :, :, :], y_pred[:, index, :, :, :])\n",
    "        return dice / (self.n_classes - 1)\n",
    "\n",
    "      \n",
    "    def dice_coef_loss(self, y_true, y_pred):\n",
    "        return 1 - self.dice_coef(y_true, y_pred)\n",
    "\n",
    "      \n",
    "    def tversky_loss(self, y_true, y_pred):\n",
    "        alpha = 0.3\n",
    "        beta = 0.7\n",
    "#         y_true, y_pred = y_true.float(), y_pred.float()\n",
    "        y_true, y_pred = y_true.type(torch.float32), y_pred.type(torch.float32)\n",
    "#         print('jwu tversky_loss: ', torch.max(y_true), torch.min(y_true), torch.max(y_pred), torch.min(y_pred))\n",
    "        \n",
    "        p0 = y_pred  # proba that voxels are class i\n",
    "        p1 = 1 - y_pred  # proba that voxels are not class i\n",
    "        g0 = y_true\n",
    "        g1 = 1 - y_true\n",
    "        # logger.info(\n",
    "        #     f\"y_true shape: {y_true.shape}, y_pred: {y_pred.shape}, p0: {p0.shape}, p1: {p1.shape}, g0: {g0.shape}, g1: {g1.shape}\")\n",
    "\n",
    "        num = torch.sum(p0 * g0, (0, 2, 3, 4))\n",
    "        den = num + alpha * torch.sum(p0 * g1, (0, 2, 3, 4)) + beta * torch.sum(p1 * g0, (0, 2, 3, 4))\n",
    "\n",
    "        T = torch.sum(num / den)  # when summing over classes, T has dynamic range [0 Ncl]\n",
    "\n",
    "        Ncl = float(y_true.shape[1])\n",
    "        return Ncl - T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33e8a777-b583-4cb2-be5b-0a092b518b29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# (4) 这里是数据参数，需要修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f2ac55f-84e4-493c-8dc3-0891ff69de0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# --- water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60286059-785c-49c8-8c80-f51a041c3a93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "900\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "s3_path = \"/dbfs/mnt/hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/labeled_data_finalized_1/\"\n",
    "ids = [x.name for x in os.scandir(s3_path) if os.path.isdir(s3_path + x.name)]\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01b277eb-1261-454d-84d0-e6e20bdfdf56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "male_val_id = ['BJ00000075', 'BJ00000060', 'BJ00000054', 'BJ00000050', 'BJ00000096']\n",
    "female_val_id = ['BJ00000071', 'BJ00000150', 'BJ00000077', 'BJ00000033', 'BJ00000120']\n",
    "test_ids = male_val_id + female_val_id\n",
    "\n",
    "# male_test_id = ['BJ00000016', 'BJ00000125', 'BJ00000147', 'BJ00000044', 'BJ00000114']\n",
    "# female_test_id = ['BJ00000100', 'BJ00000090', 'BJ00000110', 'BJ00000108', 'BJ00000002']\n",
    "male_test_id = ['BJ00000016', 'BJ00000044', 'BJ00000114']\n",
    "female_test_id = ['BJ00000100', 'BJ00000002']\n",
    "test_ids_2 = male_test_id + female_test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAT  ->   VAT/ASAT\n",
    "\n",
    "log_level = \"INFO\"\n",
    "# log_level = 'DEBUG'\n",
    "logger.remove()\n",
    "handler_id = logger.add(sys.stderr, level=log_level)\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "lr = 0.001\n",
    "\n",
    "dim = [240, 256, 320]\n",
    "slice_interval = [120, 360]\n",
    "n_classes = 3\n",
    "x_prefix='fat'\n",
    "y_prefix='fat_label'\n",
    "x_postfix= \"npy\" \n",
    "y_postfix= \"npy\"\n",
    "\n",
    "\n",
    "data_path = '/dbfs/mnt/hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/labeled_data_finalized_1'\n",
    "\n",
    "model_path = './TEST/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "# x_prefix='water_288_256_320'\n",
    "# y_prefix='water_label_288_256_320'\n",
    "\n",
    "\n",
    "# train_ids = [user_id for user_id in os.listdir(data_path) if user_id != \".DS_Store\" and (user_id not in test_ids) and user_id.startswith('BJ') and (user_id not in test_ids_2)]\n",
    "filter_1 = [x for x in set(ids) if (('_' not in x) or (x.split('_')[1] not in set(test_ids) | set(test_ids_2))) and (x.startswith('BJ'))]\n",
    "train_ids = list(set(filter_1) - set(test_ids) - set(test_ids_2))\n",
    "logger.info(f\"train ids: {len(train_ids)}, test ids: {len(test_ids)}\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# model = UNet3d(in_channels=1, n_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77c3f42d-8bb4-4505-8d73-d36e7d47df19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022-11-09 03:35:55.007 | INFO     | __main__:<cell line: 45>:45 - train ids: 135, test ids: 10\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "2022-11-09 03:35:55.007 | INFO     | __main__:<cell line: 45>:45 - train ids: 135, test ids: 10\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# water  ->   muscle\n",
    "\n",
    "log_level = \"INFO\"\n",
    "# log_level = 'DEBUG'\n",
    "logger.remove()\n",
    "handler_id = logger.add(sys.stderr, level=log_level)\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "lr = 0.001\n",
    "# slice_interval = [209, 489]\n",
    "# slice_interval = [205, 493]\n",
    "# dim = [240, 128, 176]\n",
    "# dim = [240, 96, 112]\n",
    "\n",
    "dim = [304, 256, 320]\n",
    "slice_interval = [197, 501]\n",
    "\n",
    "\n",
    "# dim = [288, 224+32, 288+32] # vnet 最大尺寸\n",
    "\n",
    "# dim = [288, 224-16, 288-16-16] # [1, 0.8, 0.8] 4*16g\n",
    "# dim = [288, 128, 176] # vnet 尺寸\n",
    "\n",
    "\n",
    "# dim = [280, 200, 264] # unet 最大尺寸\n",
    "n_classes = 2\n",
    "# data_path = '/mnt/sdma/data/processed_image_data_1/original_nii_format_data'\n",
    "data_path = '/dbfs/mnt/hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/labeled_data_finalized_1'\n",
    "# test_ids = ['BJ00000019', 'BJ00000124', 'BJ00000150', 'BJ00000036']\n",
    "\n",
    "# model_path = \"/mnt/sdma/data/medical_segmetation_log/model/water/\"\n",
    "model_path = './TEST/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "# x_prefix='water_288_256_320'\n",
    "# y_prefix='water_label_288_256_320'\n",
    "x_prefix='water_304_256_320'\n",
    "y_prefix='water_label_304_256_320'\n",
    "x_postfix= \"npy\" \n",
    "y_postfix= \"npy\"\n",
    "\n",
    "\n",
    "# train_ids = [user_id for user_id in os.listdir(data_path) if user_id != \".DS_Store\" and (user_id not in test_ids) and user_id.startswith('BJ') and (user_id not in test_ids_2)]\n",
    "filter_1 = [x for x in set(ids) if (('_' not in x) or (x.split('_')[1] not in set(test_ids) | set(test_ids_2))) and (x.startswith('BJ'))]\n",
    "train_ids = list(set(filter_1) - set(test_ids) - set(test_ids_2))\n",
    "logger.info(f\"train ids: {len(train_ids)}, test ids: {len(test_ids)}\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# model = UNet3d(in_channels=1, n_classes=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23da47d1-b099-45d1-ad7b-2961e84d36a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# (5) model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d7cd8b-7295-4c32-bb3a-846dcd9619b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainset = DataGenerator(list_IDs=train_ids, \n",
    "                         file_path=data_path, \n",
    "                        num_classes=n_classes,\n",
    "                         resize_shape=dim,\n",
    "                         slice_interval=slice_interval,\n",
    "                         x_prefix=x_prefix, \n",
    "                         y_prefix=y_prefix,\n",
    "                         x_postfix=x_postfix, \n",
    "                         y_postfix=y_postfix\n",
    "                         \n",
    "                        )\n",
    "testset = DataGenerator(list_IDs=test_ids, \n",
    "                        file_path=data_path,\n",
    "                        num_classes=n_classes,\n",
    "                         resize_shape=dim,\n",
    "                         slice_interval=slice_interval,\n",
    "                         x_prefix=x_prefix, \n",
    "                         y_prefix=y_prefix,\n",
    "                         x_postfix=x_postfix, \n",
    "                         y_postfix=y_postfix)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size)\n",
    "\n",
    "evalmetric = EvalMetric(dim, batch_size, n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d1d9884-626d-4a20-a833-1e1fabc2ebf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022-11-09 03:26:11.686 | INFO     | __main__:__init__:12 - begin initialize model struction\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "2022-11-09 03:26:11.686 | INFO     | __main__:__init__:12 - begin initialize model struction\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = VNet_Parallelism(in_channels=1, classes=n_classes)\n",
    "# model_dict = model.load_state_dict(torch.load('./TEST/epoch5_trainloss_0.066_validacc_0.937.pth'))\n",
    "\n",
    "# model = VNet(in_channels=1, out_channels=n_classes)\n",
    "# model = model.to(device)\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b26e9b56-9331-4099-9d7e-73de37a424b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# --- 仅使用模型并行+单精度的VNET版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54cfaf61-25db-4181-99a2-fa185465ec66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022-11-03 06:44:53.185 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000107, Batch[1/130], Train loss:1.435, Train acc: 0.108\n",
       "2022-11-03 06:45:06.727 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000132, Batch[2/130], Train loss:1.097, Train acc: 0.269\n",
       "2022-11-03 06:45:20.267 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000039, Batch[3/130], Train loss:1.228, Train acc: 0.151\n",
       "2022-11-03 06:45:33.807 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000103, Batch[4/130], Train loss:1.126, Train acc: 0.222\n",
       "2022-11-03 06:45:47.394 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000137, Batch[5/130], Train loss:1.160, Train acc: 0.198\n",
       "2022-11-03 06:46:00.932 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000027, Batch[6/130], Train loss:1.173, Train acc: 0.188\n",
       "2022-11-03 06:46:14.477 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000029, Batch[7/130], Train loss:1.128, Train acc: 0.215\n",
       "2022-11-03 06:46:28.030 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000112, Batch[8/130], Train loss:1.148, Train acc: 0.198\n",
       "2022-11-03 06:46:41.610 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000095, Batch[9/130], Train loss:1.205, Train acc: 0.161\n",
       "2022-11-03 06:46:55.152 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000122, Batch[10/130], Train loss:1.146, Train acc: 0.200\n",
       "2022-11-03 06:47:08.702 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000001, Batch[11/130], Train loss:1.137, Train acc: 0.208\n",
       "2022-11-03 06:47:22.267 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000129, Batch[12/130], Train loss:1.237, Train acc: 0.138\n",
       "2022-11-03 06:47:35.840 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000007, Batch[13/130], Train loss:1.058, Train acc: 0.264\n",
       "2022-11-03 06:47:49.396 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000073, Batch[14/130], Train loss:1.105, Train acc: 0.227\n",
       "2022-11-03 06:48:02.950 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000062, Batch[15/130], Train loss:1.150, Train acc: 0.194\n",
       "2022-11-03 06:48:16.506 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000115, Batch[16/130], Train loss:1.128, Train acc: 0.207\n",
       "2022-11-03 06:48:30.087 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000015, Batch[17/130], Train loss:1.161, Train acc: 0.184\n",
       "2022-11-03 06:48:43.656 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000043, Batch[18/130], Train loss:1.217, Train acc: 0.146\n",
       "2022-11-03 06:48:57.230 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000134, Batch[19/130], Train loss:1.177, Train acc: 0.173\n",
       "2022-11-03 06:49:10.785 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000066, Batch[20/130], Train loss:1.020, Train acc: 0.284\n",
       "2022-11-03 06:49:24.359 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000083, Batch[21/130], Train loss:1.045, Train acc: 0.266\n",
       "2022-11-03 06:49:37.921 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000140, Batch[22/130], Train loss:1.142, Train acc: 0.193\n",
       "2022-11-03 06:49:51.503 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000047, Batch[23/130], Train loss:1.171, Train acc: 0.173\n",
       "2022-11-03 06:50:05.073 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000106, Batch[24/130], Train loss:1.044, Train acc: 0.263\n",
       "2022-11-03 06:50:18.660 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000045, Batch[25/130], Train loss:1.173, Train acc: 0.170\n",
       "2022-11-03 06:50:32.241 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000086, Batch[26/130], Train loss:1.092, Train acc: 0.224\n",
       "2022-11-03 06:50:45.820 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000011, Batch[27/130], Train loss:1.157, Train acc: 0.180\n",
       "2022-11-03 06:50:59.387 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000068, Batch[28/130], Train loss:1.197, Train acc: 0.151\n",
       "2022-11-03 06:51:12.988 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000059, Batch[29/130], Train loss:1.194, Train acc: 0.153\n",
       "2022-11-03 06:51:26.553 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000053, Batch[30/130], Train loss:1.098, Train acc: 0.219\n",
       "2022-11-03 06:51:40.105 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000130, Batch[31/130], Train loss:1.026, Train acc: 0.270\n",
       "2022-11-03 06:51:53.659 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000084, Batch[32/130], Train loss:1.145, Train acc: 0.182\n",
       "2022-11-03 06:52:07.260 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000101, Batch[33/130], Train loss:1.175, Train acc: 0.162\n",
       "2022-11-03 06:52:20.810 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000080, Batch[34/130], Train loss:1.070, Train acc: 0.235\n",
       "2022-11-03 06:52:34.363 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000079, Batch[35/130], Train loss:1.088, Train acc: 0.220\n",
       "2022-11-03 06:52:47.911 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000034, Batch[36/130], Train loss:1.157, Train acc: 0.173\n",
       "2022-11-03 06:53:01.474 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000017, Batch[37/130], Train loss:1.148, Train acc: 0.177\n",
       "2022-11-03 06:53:15.019 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000004, Batch[38/130], Train loss:1.132, Train acc: 0.187\n",
       "2022-11-03 06:53:28.577 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000099, Batch[39/130], Train loss:1.141, Train acc: 0.180\n",
       "2022-11-03 06:53:42.162 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000133, Batch[40/130], Train loss:1.002, Train acc: 0.282\n",
       "2022-11-03 06:53:55.739 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000026, Batch[41/130], Train loss:1.091, Train acc: 0.213\n",
       "2022-11-03 06:54:09.302 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000102, Batch[42/130], Train loss:1.075, Train acc: 0.225\n",
       "2022-11-03 06:54:22.873 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000078, Batch[43/130], Train loss:1.115, Train acc: 0.194\n",
       "2022-11-03 06:54:36.421 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000098, Batch[44/130], Train loss:1.077, Train acc: 0.221\n",
       "2022-11-03 06:54:49.986 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000139, Batch[45/130], Train loss:1.015, Train acc: 0.265\n",
       "2022-11-03 06:55:03.531 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000087, Batch[46/130], Train loss:1.150, Train acc: 0.169\n",
       "2022-11-03 06:55:17.106 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000032, Batch[47/130], Train loss:1.024, Train acc: 0.256\n",
       "2022-11-03 06:55:30.659 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000126, Batch[48/130], Train loss:1.129, Train acc: 0.181\n",
       "2022-11-03 06:55:44.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000023, Batch[49/130], Train loss:1.137, Train acc: 0.174\n",
       "2022-11-03 06:55:57.800 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000037, Batch[50/130], Train loss:1.143, Train acc: 0.170\n",
       "2022-11-03 06:56:11.364 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000038, Batch[51/130], Train loss:1.143, Train acc: 0.169\n",
       "2022-11-03 06:56:24.936 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000018, Batch[52/130], Train loss:1.040, Train acc: 0.241\n",
       "2022-11-03 06:56:38.506 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000144, Batch[53/130], Train loss:1.046, Train acc: 0.236\n",
       "2022-11-03 06:56:52.089 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000020, Batch[54/130], Train loss:1.129, Train acc: 0.176\n",
       "2022-11-03 06:57:05.653 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000117, Batch[55/130], Train loss:1.093, Train acc: 0.201\n",
       "2022-11-03 06:57:19.220 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000136, Batch[56/130], Train loss:1.108, Train acc: 0.189\n",
       "2022-11-03 06:57:32.816 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000081, Batch[57/130], Train loss:1.006, Train acc: 0.264\n",
       "2022-11-03 06:57:46.369 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000057, Batch[58/130], Train loss:1.070, Train acc: 0.214\n",
       "2022-11-03 06:57:59.942 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000076, Batch[59/130], Train loss:0.985, Train acc: 0.277\n",
       "2022-11-03 06:58:13.503 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000036, Batch[60/130], Train loss:1.104, Train acc: 0.189\n",
       "2022-11-03 06:58:27.099 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000143, Batch[61/130], Train loss:1.077, Train acc: 0.208\n",
       "2022-11-03 06:58:40.653 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000030, Batch[62/130], Train loss:1.112, Train acc: 0.182\n",
       "2022-11-03 06:58:54.212 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000056, Batch[63/130], Train loss:0.992, Train acc: 0.267\n",
       "2022-11-03 06:59:07.765 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000074, Batch[64/130], Train loss:1.018, Train acc: 0.248\n",
       "2022-11-03 06:59:21.343 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000028, Batch[65/130], Train loss:0.946, Train acc: 0.302\n",
       "2022-11-03 06:59:34.897 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000149, Batch[66/130], Train loss:1.088, Train acc: 0.196\n",
       "2022-11-03 06:59:48.449 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000093, Batch[67/130], Train loss:1.013, Train acc: 0.249\n",
       "2022-11-03 07:00:02.027 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000092, Batch[68/130], Train loss:1.071, Train acc: 0.210\n",
       "2022-11-03 07:00:16.747 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000123, Batch[69/130], Train loss:1.002, Train acc: 0.256\n",
       "2022-11-03 07:00:30.297 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000128, Batch[70/130], Train loss:1.115, Train acc: 0.175\n",
       "2022-11-03 07:00:43.878 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000069, Batch[71/130], Train loss:1.021, Train acc: 0.241\n",
       "2022-11-03 07:00:57.424 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000052, Batch[72/130], Train loss:1.011, Train acc: 0.245\n",
       "2022-11-03 07:01:10.992 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000013, Batch[73/130], Train loss:1.063, Train acc: 0.207\n",
       "2022-11-03 07:01:24.550 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000041, Batch[74/130], Train loss:0.946, Train acc: 0.293\n",
       "2022-11-03 07:01:38.120 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000148, Batch[75/130], Train loss:0.968, Train acc: 0.275\n",
       "2022-11-03 07:01:51.677 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000116, Batch[76/130], Train loss:0.964, Train acc: 0.277\n",
       "2022-11-03 07:02:05.255 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000008, Batch[77/130], Train loss:0.892, Train acc: 0.334\n",
       "2022-11-03 07:02:18.815 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000145, Batch[78/130], Train loss:1.110, Train acc: 0.171\n",
       "2022-11-03 07:02:32.361 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000061, Batch[79/130], Train loss:1.027, Train acc: 0.228\n",
       "2022-11-03 07:02:45.937 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000118, Batch[80/130], Train loss:1.045, Train acc: 0.214\n",
       "2022-11-03 07:02:59.505 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000005, Batch[81/130], Train loss:1.029, Train acc: 0.225\n",
       "2022-11-03 07:03:13.060 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000051, Batch[82/130], Train loss:1.102, Train acc: 0.172\n",
       "2022-11-03 07:03:26.619 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000121, Batch[83/130], Train loss:0.939, Train acc: 0.291\n",
       "2022-11-03 07:03:40.178 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000124, Batch[84/130], Train loss:0.915, Train acc: 0.308\n",
       "2022-11-03 07:03:53.795 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000014, Batch[85/130], Train loss:1.000, Train acc: 0.242\n",
       "2022-11-03 07:04:07.366 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000055, Batch[86/130], Train loss:1.062, Train acc: 0.197\n",
       "2022-11-03 07:04:20.935 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000010, Batch[87/130], Train loss:1.020, Train acc: 0.228\n",
       "2022-11-03 07:04:34.507 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000146, Batch[88/130], Train loss:0.898, Train acc: 0.320\n",
       "2022-11-03 07:04:48.105 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000113, Batch[89/130], Train loss:0.951, Train acc: 0.275\n",
       "2022-11-03 07:05:01.667 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000024, Batch[90/130], Train loss:0.973, Train acc: 0.259\n",
       "2022-11-03 07:05:15.234 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000067, Batch[91/130], Train loss:0.912, Train acc: 0.304\n",
       "2022-11-03 07:05:28.804 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000003, Batch[92/130], Train loss:1.081, Train acc: 0.179\n",
       "2022-11-03 07:05:42.365 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000042, Batch[93/130], Train loss:0.897, Train acc: 0.314\n",
       "2022-11-03 07:05:55.914 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000119, Batch[94/130], Train loss:0.906, Train acc: 0.310\n",
       "2022-11-03 07:06:09.478 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000021, Batch[95/130], Train loss:1.006, Train acc: 0.232\n",
       "2022-11-03 07:06:23.033 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000142, Batch[96/130], Train loss:0.889, Train acc: 0.319\n",
       "2022-11-03 07:06:36.604 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000040, Batch[97/130], Train loss:0.863, Train acc: 0.342\n",
       "2022-11-03 07:06:50.156 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000141, Batch[98/130], Train loss:1.033, Train acc: 0.210\n",
       "2022-11-03 07:07:03.727 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000019, Batch[99/130], Train loss:0.917, Train acc: 0.296\n",
       "2022-11-03 07:07:17.275 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000025, Batch[100/130], Train loss:1.052, Train acc: 0.194\n",
       "2022-11-03 07:07:30.861 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000089, Batch[101/130], Train loss:0.921, Train acc: 0.291\n",
       "2022-11-03 07:07:44.419 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000097, Batch[102/130], Train loss:0.898, Train acc: 0.309\n",
       "2022-11-03 07:07:57.970 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000048, Batch[103/130], Train loss:0.911, Train acc: 0.296\n",
       "2022-11-03 07:08:11.527 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000127, Batch[104/130], Train loss:0.925, Train acc: 0.284\n",
       "2022-11-03 07:08:25.143 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000135, Batch[105/130], Train loss:0.982, Train acc: 0.240\n",
       "2022-11-03 07:08:38.718 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000082, Batch[106/130], Train loss:0.951, Train acc: 0.262\n",
       "2022-11-03 07:08:52.278 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000064, Batch[107/130], Train loss:0.964, Train acc: 0.252\n",
       "2022-11-03 07:09:05.838 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000006, Batch[108/130], Train loss:1.018, Train acc: 0.211\n",
       "2022-11-03 07:09:19.425 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000085, Batch[109/130], Train loss:1.005, Train acc: 0.220\n",
       "2022-11-03 07:09:33.000 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000058, Batch[110/130], Train loss:0.897, Train acc: 0.300\n",
       "2022-11-03 07:09:46.569 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000104, Batch[111/130], Train loss:0.872, Train acc: 0.318\n",
       "2022-11-03 07:10:00.125 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000138, Batch[112/130], Train loss:1.041, Train acc: 0.192\n",
       "2022-11-03 07:10:13.718 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000131, Batch[113/130], Train loss:1.064, Train acc: 0.175\n",
       "2022-11-03 07:10:27.276 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000031, Batch[114/130], Train loss:1.014, Train acc: 0.210\n",
       "2022-11-03 07:10:40.848 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000063, Batch[115/130], Train loss:1.029, Train acc: 0.199\n",
       "2022-11-03 07:10:54.398 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000111, Batch[116/130], Train loss:0.868, Train acc: 0.318\n",
       "2022-11-03 07:11:07.992 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000035, Batch[117/130], Train loss:0.919, Train acc: 0.276\n",
       "2022-11-03 07:11:21.542 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000094, Batch[118/130], Train loss:1.002, Train acc: 0.217\n",
       "2022-11-03 07:11:35.092 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000065, Batch[119/130], Train loss:0.872, Train acc: 0.313\n",
       "2022-11-03 07:11:48.666 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000109, Batch[120/130], Train loss:0.982, Train acc: 0.228\n",
       "2022-11-03 07:12:02.233 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000088, Batch[121/130], Train loss:0.784, Train acc: 0.382\n",
       "2022-11-03 07:12:15.784 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000070, Batch[122/130], Train loss:0.950, Train acc: 0.250\n",
       "2022-11-03 07:12:29.339 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000091, Batch[123/130], Train loss:0.859, Train acc: 0.320\n",
       "2022-11-03 07:12:42.895 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000105, Batch[124/130], Train loss:1.002, Train acc: 0.210\n",
       "2022-11-03 07:12:56.474 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000022, Batch[125/130], Train loss:0.953, Train acc: 0.244\n",
       "2022-11-03 07:13:10.020 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000049, Batch[126/130], Train loss:0.858, Train acc: 0.315\n",
       "2022-11-03 07:13:23.585 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000009, Batch[127/130], Train loss:0.807, Train acc: 0.356\n",
       "2022-11-03 07:13:37.150 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000072, Batch[128/130], Train loss:0.928, Train acc: 0.260\n",
       "2022-11-03 07:13:50.736 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000012, Batch[129/130], Train loss:0.921, Train acc: 0.267\n",
       "2022-11-03 07:14:04.283 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000046, Batch[130/130], Train loss:0.980, Train acc: 0.222\n",
       "2022-11-03 07:14:30.562 | INFO     | __main__:<cell line: 25>:70 - Epoch: 0, Train loss: 1.042, Epoch time=1790.634s, valid mean accuracy: 0.276, valid loss: 0.8815159797668457\n",
       "2022-11-03 07:14:44.062 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000036, Batch[1/130], Train loss:1.000, Train acc: 0.176\n",
       "2022-11-03 07:14:57.691 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000073, Batch[2/130], Train loss:1.001, Train acc: 0.258\n",
       "2022-11-03 07:15:11.307 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000026, Batch[3/130], Train loss:0.750, Train acc: 0.334\n",
       "2022-11-03 07:15:24.937 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000031, Batch[4/130], Train loss:0.556, Train acc: 0.425\n",
       "2022-11-03 07:15:38.590 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000009, Batch[5/130], Train loss:0.373, Train acc: 0.623\n",
       "2022-11-03 07:15:52.223 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000148, Batch[6/130], Train loss:0.389, Train acc: 0.583\n",
       "2022-11-03 07:16:05.846 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000137, Batch[7/130], Train loss:0.362, Train acc: 0.591\n",
       "2022-11-03 07:16:19.469 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000039, Batch[8/130], Train loss:0.383, Train acc: 0.639\n",
       "2022-11-03 07:16:33.147 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000027, Batch[9/130], Train loss:0.294, Train acc: 0.655\n",
       "2022-11-03 07:16:46.765 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000141, Batch[10/130], Train loss:0.265, Train acc: 0.681\n",
       "2022-11-03 07:17:00.389 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000066, Batch[11/130], Train loss:0.392, Train acc: 0.597\n",
       "2022-11-03 07:17:14.003 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000097, Batch[12/130], Train loss:0.305, Train acc: 0.672\n",
       "2022-11-03 07:17:27.641 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000098, Batch[13/130], Train loss:0.376, Train acc: 0.584\n",
       "2022-11-03 07:17:41.234 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000081, Batch[14/130], Train loss:0.356, Train acc: 0.616\n",
       "2022-11-03 07:17:54.840 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000042, Batch[15/130], Train loss:0.318, Train acc: 0.656\n",
       "2022-11-03 07:18:08.472 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000070, Batch[16/130], Train loss:0.334, Train acc: 0.649\n",
       "2022-11-03 07:18:22.112 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000025, Batch[17/130], Train loss:0.365, Train acc: 0.631\n",
       "2022-11-03 07:18:35.729 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000091, Batch[18/130], Train loss:0.266, Train acc: 0.706\n",
       "2022-11-03 07:18:49.356 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000053, Batch[19/130], Train loss:0.305, Train acc: 0.655\n",
       "2022-11-03 07:19:02.973 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000112, Batch[20/130], Train loss:0.332, Train acc: 0.616\n",
       "2022-11-03 07:19:16.630 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000001, Batch[21/130], Train loss:0.291, Train acc: 0.663\n",
       "2022-11-03 07:19:30.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000063, Batch[22/130], Train loss:0.289, Train acc: 0.650\n",
       "2022-11-03 07:19:43.886 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000149, Batch[23/130], Train loss:0.248, Train acc: 0.729\n",
       "2022-11-03 07:19:57.492 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000032, Batch[24/130], Train loss:0.234, Train acc: 0.727\n",
       "2022-11-03 07:20:11.137 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000119, Batch[25/130], Train loss:0.232, Train acc: 0.741\n",
       "2022-11-03 07:20:24.745 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000104, Batch[26/130], Train loss:0.263, Train acc: 0.707\n",
       "2022-11-03 07:20:38.373 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000124, Batch[27/130], Train loss:0.230, Train acc: 0.741\n",
       "2022-11-03 07:20:51.977 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000017, Batch[28/130], Train loss:0.530, Train acc: 0.536\n",
       "2022-11-03 07:21:05.605 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000083, Batch[29/130], Train loss:1.117, Train acc: 0.253\n",
       "2022-11-03 07:21:19.236 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000101, Batch[30/130], Train loss:0.377, Train acc: 0.560\n",
       "2022-11-03 07:21:32.871 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000085, Batch[31/130], Train loss:0.291, Train acc: 0.650\n",
       "2022-11-03 07:21:46.496 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000113, Batch[32/130], Train loss:0.355, Train acc: 0.608\n",
       "2022-11-03 07:22:00.151 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000006, Batch[33/130], Train loss:0.266, Train acc: 0.673\n",
       "2022-11-03 07:22:13.777 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000122, Batch[34/130], Train loss:0.441, Train acc: 0.526\n",
       "2022-11-03 07:22:27.398 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000037, Batch[35/130], Train loss:0.247, Train acc: 0.692\n",
       "2022-11-03 07:22:41.050 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000065, Batch[36/130], Train loss:0.287, Train acc: 0.675\n",
       "2022-11-03 07:22:54.720 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000058, Batch[37/130], Train loss:0.287, Train acc: 0.676\n",
       "2022-11-03 07:23:08.355 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000023, Batch[38/130], Train loss:0.240, Train acc: 0.702\n",
       "2022-11-03 07:23:21.974 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000012, Batch[39/130], Train loss:0.286, Train acc: 0.687\n",
       "2022-11-03 07:23:35.583 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000019, Batch[40/130], Train loss:0.317, Train acc: 0.648\n",
       "2022-11-03 07:23:49.216 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000047, Batch[41/130], Train loss:0.353, Train acc: 0.651\n",
       "2022-11-03 07:24:02.851 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000029, Batch[42/130], Train loss:0.267, Train acc: 0.695\n",
       "2022-11-03 07:24:16.466 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000015, Batch[43/130], Train loss:0.296, Train acc: 0.665\n",
       "2022-11-03 07:24:30.107 | INFO     | __main__:<cell line: 25>:60 - Epoc\n",
       "\n",
       "*** WARNING: max output size exceeded, skipping output. ***\n",
       "\n",
       "| __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000143, Batch[90/130], Train loss:0.056, Train acc: 0.927\n",
       "2022-11-03 21:02:18.319 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000065, Batch[91/130], Train loss:0.032, Train acc: 0.965\n",
       "2022-11-03 21:02:31.895 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000079, Batch[92/130], Train loss:0.049, Train acc: 0.934\n",
       "2022-11-03 21:02:45.524 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000011, Batch[93/130], Train loss:0.037, Train acc: 0.964\n",
       "2022-11-03 21:02:59.085 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000098, Batch[94/130], Train loss:0.042, Train acc: 0.945\n",
       "2022-11-03 21:03:12.663 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000136, Batch[95/130], Train loss:0.044, Train acc: 0.942\n",
       "2022-11-03 21:03:26.236 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000127, Batch[96/130], Train loss:0.037, Train acc: 0.956\n",
       "2022-11-03 21:03:39.867 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000141, Batch[97/130], Train loss:0.045, Train acc: 0.957\n",
       "2022-11-03 21:03:53.433 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000040, Batch[98/130], Train loss:0.039, Train acc: 0.969\n",
       "2022-11-03 21:04:07.010 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000056, Batch[99/130], Train loss:0.033, Train acc: 0.971\n",
       "2022-11-03 21:04:20.599 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000061, Batch[100/130], Train loss:0.030, Train acc: 0.964\n",
       "2022-11-03 21:04:34.187 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000103, Batch[101/130], Train loss:0.042, Train acc: 0.944\n",
       "2022-11-03 21:04:47.769 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000008, Batch[102/130], Train loss:0.025, Train acc: 0.975\n",
       "2022-11-03 21:05:01.389 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000078, Batch[103/130], Train loss:0.049, Train acc: 0.933\n",
       "2022-11-03 21:05:14.989 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000005, Batch[104/130], Train loss:0.031, Train acc: 0.963\n",
       "2022-11-03 21:05:28.620 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000039, Batch[105/130], Train loss:0.036, Train acc: 0.952\n",
       "2022-11-03 21:05:42.219 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000083, Batch[106/130], Train loss:0.037, Train acc: 0.956\n",
       "2022-11-03 21:05:55.826 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000087, Batch[107/130], Train loss:0.036, Train acc: 0.951\n",
       "2022-11-03 21:06:09.414 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000132, Batch[108/130], Train loss:0.034, Train acc: 0.960\n",
       "2022-11-03 21:06:23.039 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000149, Batch[109/130], Train loss:0.044, Train acc: 0.943\n",
       "2022-11-03 21:06:36.633 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000112, Batch[110/130], Train loss:0.037, Train acc: 0.960\n",
       "2022-11-03 21:06:50.223 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000026, Batch[111/130], Train loss:0.047, Train acc: 0.964\n",
       "2022-11-03 21:07:03.837 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000052, Batch[112/130], Train loss:0.038, Train acc: 0.968\n",
       "2022-11-03 21:07:17.428 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000119, Batch[113/130], Train loss:0.034, Train acc: 0.968\n",
       "2022-11-03 21:07:31.020 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000148, Batch[114/130], Train loss:0.038, Train acc: 0.965\n",
       "2022-11-03 21:07:44.605 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000113, Batch[115/130], Train loss:0.029, Train acc: 0.967\n",
       "2022-11-03 21:07:58.208 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000013, Batch[116/130], Train loss:0.028, Train acc: 0.970\n",
       "2022-11-03 21:08:11.843 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000067, Batch[117/130], Train loss:0.029, Train acc: 0.964\n",
       "2022-11-03 21:08:25.445 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000138, Batch[118/130], Train loss:0.048, Train acc: 0.933\n",
       "2022-11-03 21:08:39.048 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000088, Batch[119/130], Train loss:0.031, Train acc: 0.963\n",
       "2022-11-03 21:08:52.637 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000017, Batch[120/130], Train loss:0.030, Train acc: 0.967\n",
       "2022-11-03 21:09:06.265 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000099, Batch[121/130], Train loss:0.039, Train acc: 0.953\n",
       "2022-11-03 21:09:19.878 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000046, Batch[122/130], Train loss:0.036, Train acc: 0.952\n",
       "2022-11-03 21:09:33.474 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000014, Batch[123/130], Train loss:0.054, Train acc: 0.946\n",
       "2022-11-03 21:09:47.066 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000104, Batch[124/130], Train loss:0.034, Train acc: 0.960\n",
       "2022-11-03 21:10:00.686 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000086, Batch[125/130], Train loss:0.035, Train acc: 0.955\n",
       "2022-11-03 21:10:14.287 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000025, Batch[126/130], Train loss:0.037, Train acc: 0.963\n",
       "2022-11-03 21:10:27.868 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000070, Batch[127/130], Train loss:0.036, Train acc: 0.959\n",
       "2022-11-03 21:10:41.457 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000066, Batch[128/130], Train loss:0.029, Train acc: 0.970\n",
       "2022-11-03 21:10:55.070 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000133, Batch[129/130], Train loss:0.044, Train acc: 0.946\n",
       "2022-11-03 21:11:08.654 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000102, Batch[130/130], Train loss:0.041, Train acc: 0.950\n",
       "2022-11-03 21:11:34.921 | INFO     | __main__:<cell line: 25>:70 - Epoch: 28, Train loss: 0.038, Epoch time=1793.936s, valid mean accuracy: 0.959, valid loss: 0.03815590217709541\n",
       "2022-11-03 21:11:48.391 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000121, Batch[1/130], Train loss:0.030, Train acc: 0.967\n",
       "2022-11-03 21:12:01.962 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000124, Batch[2/130], Train loss:0.038, Train acc: 0.952\n",
       "2022-11-03 21:12:15.548 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000018, Batch[3/130], Train loss:0.044, Train acc: 0.965\n",
       "2022-11-03 21:12:29.149 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000128, Batch[4/130], Train loss:0.043, Train acc: 0.949\n",
       "2022-11-03 21:12:42.734 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000067, Batch[5/130], Train loss:0.027, Train acc: 0.968\n",
       "2022-11-03 21:12:56.336 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000026, Batch[6/130], Train loss:0.025, Train acc: 0.975\n",
       "2022-11-03 21:13:09.917 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000080, Batch[7/130], Train loss:0.042, Train acc: 0.947\n",
       "2022-11-03 21:13:23.515 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000073, Batch[8/130], Train loss:0.035, Train acc: 0.957\n",
       "2022-11-03 21:13:37.128 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000135, Batch[9/130], Train loss:0.033, Train acc: 0.956\n",
       "2022-11-03 21:13:50.732 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000143, Batch[10/130], Train loss:0.050, Train acc: 0.938\n",
       "2022-11-03 21:14:04.330 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000118, Batch[11/130], Train loss:0.039, Train acc: 0.949\n",
       "2022-11-03 21:14:17.920 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000010, Batch[12/130], Train loss:0.052, Train acc: 0.959\n",
       "2022-11-03 21:14:31.535 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000095, Batch[13/130], Train loss:0.043, Train acc: 0.943\n",
       "2022-11-03 21:14:45.112 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000133, Batch[14/130], Train loss:0.041, Train acc: 0.952\n",
       "2022-11-03 21:14:58.690 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000089, Batch[15/130], Train loss:0.026, Train acc: 0.970\n",
       "2022-11-03 21:15:12.285 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000069, Batch[16/130], Train loss:0.042, Train acc: 0.960\n",
       "2022-11-03 21:15:25.919 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000078, Batch[17/130], Train loss:0.041, Train acc: 0.947\n",
       "2022-11-03 21:15:39.516 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000015, Batch[18/130], Train loss:0.045, Train acc: 0.960\n",
       "2022-11-03 21:15:53.108 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000003, Batch[19/130], Train loss:0.032, Train acc: 0.960\n",
       "2022-11-03 21:16:06.707 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000119, Batch[20/130], Train loss:0.029, Train acc: 0.968\n",
       "2022-11-03 21:16:20.327 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000006, Batch[21/130], Train loss:0.034, Train acc: 0.961\n",
       "2022-11-03 21:16:33.917 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000126, Batch[22/130], Train loss:0.043, Train acc: 0.943\n",
       "2022-11-03 21:16:47.525 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000094, Batch[23/130], Train loss:0.041, Train acc: 0.947\n",
       "2022-11-03 21:17:01.128 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000046, Batch[24/130], Train loss:0.034, Train acc: 0.955\n",
       "2022-11-03 21:17:14.759 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000036, Batch[25/130], Train loss:0.040, Train acc: 0.951\n",
       "2022-11-03 21:17:28.330 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000116, Batch[26/130], Train loss:0.037, Train acc: 0.961\n",
       "2022-11-03 21:17:41.944 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000063, Batch[27/130], Train loss:0.037, Train acc: 0.965\n",
       "2022-11-03 21:17:55.558 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000001, Batch[28/130], Train loss:0.031, Train acc: 0.968\n",
       "2022-11-03 21:18:09.173 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000020, Batch[29/130], Train loss:0.037, Train acc: 0.966\n",
       "2022-11-03 21:18:22.762 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000103, Batch[30/130], Train loss:0.035, Train acc: 0.956\n",
       "2022-11-03 21:18:36.356 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000061, Batch[31/130], Train loss:0.028, Train acc: 0.970\n",
       "2022-11-03 21:18:49.965 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000101, Batch[32/130], Train loss:0.042, Train acc: 0.946\n",
       "2022-11-03 21:19:03.578 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000070, Batch[33/130], Train loss:0.036, Train acc: 0.961\n",
       "2022-11-03 21:19:17.196 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000056, Batch[34/130], Train loss:0.031, Train acc: 0.971\n",
       "2022-11-03 21:19:30.779 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000122, Batch[35/130], Train loss:0.034, Train acc: 0.958\n",
       "2022-11-03 21:19:44.372 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000146, Batch[36/130], Train loss:0.031, Train acc: 0.965\n",
       "2022-11-03 21:19:57.966 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000019, Batch[37/130], Train loss:0.028, Train acc: 0.968\n",
       "2022-11-03 21:20:11.559 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000029, Batch[38/130], Train loss:0.028, Train acc: 0.967\n",
       "2022-11-03 21:20:25.170 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000035, Batch[39/130], Train loss:0.031, Train acc: 0.968\n",
       "2022-11-03 21:20:38.775 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000097, Batch[40/130], Train loss:0.040, Train acc: 0.950\n",
       "2022-11-03 21:20:52.405 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000092, Batch[41/130], Train loss:0.044, Train acc: 0.947\n",
       "2022-11-03 21:21:05.993 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000086, Batch[42/130], Train loss:0.033, Train acc: 0.957\n",
       "2022-11-03 21:21:19.560 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000107, Batch[43/130], Train loss:0.036, Train acc: 0.959\n",
       "2022-11-03 21:21:33.153 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000131, Batch[44/130], Train loss:0.035, Train acc: 0.953\n",
       "2022-11-03 21:21:46.764 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000083, Batch[45/130], Train loss:0.036, Train acc: 0.961\n",
       "2022-11-03 21:22:00.332 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000032, Batch[46/130], Train loss:0.026, Train acc: 0.975\n",
       "2022-11-03 21:22:13.932 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000027, Batch[47/130], Train loss:0.038, Train acc: 0.961\n",
       "2022-11-03 21:22:27.553 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000052, Batch[48/130], Train loss:0.029, Train acc: 0.972\n",
       "2022-11-03 21:22:41.154 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000049, Batch[49/130], Train loss:0.026, Train acc: 0.970\n",
       "2022-11-03 21:22:54.717 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000042, Batch[50/130], Train loss:0.030, Train acc: 0.969\n",
       "2022-11-03 21:23:08.305 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000023, Batch[51/130], Train loss:0.038, Train acc: 0.966\n",
       "2022-11-03 21:23:21.905 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000087, Batch[52/130], Train loss:0.038, Train acc: 0.949\n",
       "2022-11-03 21:23:35.547 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000136, Batch[53/130], Train loss:0.043, Train acc: 0.942\n",
       "2022-11-03 21:23:49.131 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000041, Batch[54/130], Train loss:0.027, Train acc: 0.969\n",
       "2022-11-03 21:24:02.760 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000030, Batch[55/130], Train loss:0.031, Train acc: 0.958\n",
       "2022-11-03 21:24:16.350 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000088, Batch[56/130], Train loss:0.029, Train acc: 0.967\n",
       "2022-11-03 21:24:29.975 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000085, Batch[57/130], Train loss:0.045, Train acc: 0.954\n",
       "2022-11-03 21:24:43.567 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000068, Batch[58/130], Train loss:0.044, Train acc: 0.950\n",
       "2022-11-03 21:24:57.160 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000022, Batch[59/130], Train loss:0.034, Train acc: 0.970\n",
       "2022-11-03 21:25:10.769 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000062, Batch[60/130], Train loss:0.035, Train acc: 0.965\n",
       "2022-11-03 21:25:24.401 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000140, Batch[61/130], Train loss:0.041, Train acc: 0.950\n",
       "2022-11-03 21:25:38.006 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000093, Batch[62/130], Train loss:0.035, Train acc: 0.956\n",
       "2022-11-03 21:25:51.581 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000007, Batch[63/130], Train loss:0.025, Train acc: 0.977\n",
       "2022-11-03 21:26:05.170 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000109, Batch[64/130], Train loss:0.040, Train acc: 0.945\n",
       "2022-11-03 21:26:18.780 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000134, Batch[65/130], Train loss:0.048, Train acc: 0.934\n",
       "2022-11-03 21:26:32.382 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000065, Batch[66/130], Train loss:0.033, Train acc: 0.963\n",
       "2022-11-03 21:26:45.978 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000037, Batch[67/130], Train loss:0.029, Train acc: 0.967\n",
       "2022-11-03 21:26:59.586 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000129, Batch[68/130], Train loss:0.040, Train acc: 0.952\n",
       "2022-11-03 21:27:13.229 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000076, Batch[69/130], Train loss:0.051, Train acc: 0.936\n",
       "2022-11-03 21:27:26.814 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000039, Batch[70/130], Train loss:0.034, Train acc: 0.956\n",
       "2022-11-03 21:27:40.387 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000139, Batch[71/130], Train loss:0.033, Train acc: 0.965\n",
       "2022-11-03 21:27:53.979 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000072, Batch[72/130], Train loss:0.034, Train acc: 0.966\n",
       "2022-11-03 21:28:07.618 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000025, Batch[73/130], Train loss:0.045, Train acc: 0.960\n",
       "2022-11-03 21:28:21.181 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000008, Batch[74/130], Train loss:0.031, Train acc: 0.973\n",
       "2022-11-03 21:28:34.760 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000066, Batch[75/130], Train loss:0.030, Train acc: 0.971\n",
       "2022-11-03 21:28:48.372 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000004, Batch[76/130], Train loss:0.053, Train acc: 0.930\n",
       "2022-11-03 21:29:02.003 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000005, Batch[77/130], Train loss:0.031, Train acc: 0.966\n",
       "2022-11-03 21:29:15.606 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000145, Batch[78/130], Train loss:0.045, Train acc: 0.951\n",
       "2022-11-03 21:29:29.215 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000149, Batch[79/130], Train loss:0.052, Train acc: 0.929\n",
       "2022-11-03 21:29:42.804 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000106, Batch[80/130], Train loss:0.030, Train acc: 0.960\n",
       "2022-11-03 21:29:56.424 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000045, Batch[81/130], Train loss:0.031, Train acc: 0.966\n",
       "2022-11-03 21:30:10.001 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000113, Batch[82/130], Train loss:0.030, Train acc: 0.962\n",
       "2022-11-03 21:30:23.630 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000013, Batch[83/130], Train loss:0.026, Train acc: 0.971\n",
       "2022-11-03 21:30:37.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000038, Batch[84/130], Train loss:0.032, Train acc: 0.963\n",
       "2022-11-03 21:30:50.863 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000137, Batch[85/130], Train loss:0.037, Train acc: 0.953\n",
       "2022-11-03 21:31:04.464 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000059, Batch[86/130], Train loss:0.054, Train acc: 0.942\n",
       "2022-11-03 21:31:18.058 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000024, Batch[87/130], Train loss:0.039, Train acc: 0.966\n",
       "2022-11-03 21:31:31.642 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000127, Batch[88/130], Train loss:0.036, Train acc: 0.955\n",
       "2022-11-03 21:31:45.273 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000012, Batch[89/130], Train loss:0.045, Train acc: 0.962\n",
       "2022-11-03 21:31:58.885 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000040, Batch[90/130], Train loss:0.030, Train acc: 0.973\n",
       "2022-11-03 21:32:12.468 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000011, Batch[91/130], Train loss:0.033, Train acc: 0.965\n",
       "2022-11-03 21:32:26.051 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000082, Batch[92/130], Train loss:0.044, Train acc: 0.941\n",
       "2022-11-03 21:32:39.658 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000148, Batch[93/130], Train loss:0.032, Train acc: 0.963\n",
       "2022-11-03 21:32:53.225 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000112, Batch[94/130], Train loss:0.047, Train acc: 0.940\n",
       "2022-11-03 21:33:06.816 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000047, Batch[95/130], Train loss:0.053, Train acc: 0.938\n",
       "2022-11-03 21:33:20.389 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000079, Batch[96/130], Train loss:0.056, Train acc: 0.925\n",
       "2022-11-03 21:33:34.030 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000048, Batch[97/130], Train loss:0.031, Train acc: 0.964\n",
       "2022-11-03 21:33:47.609 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000098, Batch[98/130], Train loss:0.048, Train acc: 0.935\n",
       "2022-11-03 21:34:01.198 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000064, Batch[99/130], Train loss:0.036, Train acc: 0.955\n",
       "2022-11-03 21:34:14.789 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000014, Batch[100/130], Train loss:0.052, Train acc: 0.948\n",
       "2022-11-03 21:34:28.406 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000102, Batch[101/130], Train loss:0.041, Train acc: 0.949\n",
       "2022-11-03 21:34:42.002 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000074, Batch[102/130], Train loss:0.037, Train acc: 0.961\n",
       "2022-11-03 21:34:55.593 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000117, Batch[103/130], Train loss:0.048, Train acc: 0.936\n",
       "2022-11-03 21:35:09.190 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000123, Batch[104/130], Train loss:0.035, Train acc: 0.960\n",
       "2022-11-03 21:35:22.804 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000099, Batch[105/130], Train loss:0.043, Train acc: 0.956\n",
       "2022-11-03 21:35:36.387 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000111, Batch[106/130], Train loss:0.034, Train acc: 0.960\n",
       "2022-11-03 21:35:49.983 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000051, Batch[107/130], Train loss:0.030, Train acc: 0.969\n",
       "2022-11-03 21:36:03.551 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000084, Batch[108/130], Train loss:0.035, Train acc: 0.955\n",
       "2022-11-03 21:36:17.157 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000058, Batch[109/130], Train loss:0.038, Train acc: 0.969\n",
       "2022-11-03 21:36:30.768 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000021, Batch[110/130], Train loss:0.031, Train acc: 0.972\n",
       "2022-11-03 21:36:44.392 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000141, Batch[111/130], Train loss:0.038, Train acc: 0.958\n",
       "2022-11-03 21:36:57.984 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000132, Batch[112/130], Train loss:0.036, Train acc: 0.957\n",
       "2022-11-03 21:37:11.604 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000043, Batch[113/130], Train loss:0.032, Train acc: 0.967\n",
       "2022-11-03 21:37:25.208 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000138, Batch[114/130], Train loss:0.050, Train acc: 0.929\n",
       "2022-11-03 21:37:38.820 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000091, Batch[115/130], Train loss:0.041, Train acc: 0.948\n",
       "2022-11-03 21:37:52.408 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000081, Batch[116/130], Train loss:0.046, Train acc: 0.940\n",
       "2022-11-03 21:38:06.050 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000057, Batch[117/130], Train loss:0.031, Train acc: 0.963\n",
       "2022-11-03 21:38:19.692 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000034, Batch[118/130], Train loss:0.034, Train acc: 0.958\n",
       "2022-11-03 21:38:33.261 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000130, Batch[119/130], Train loss:0.037, Train acc: 0.958\n",
       "2022-11-03 21:38:46.872 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000017, Batch[120/130], Train loss:0.033, Train acc: 0.969\n",
       "2022-11-03 21:39:00.492 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000105, Batch[121/130], Train loss:0.038, Train acc: 0.949\n",
       "2022-11-03 21:39:14.065 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000104, Batch[122/130], Train loss:0.038, Train acc: 0.963\n",
       "2022-11-03 21:39:27.661 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000031, Batch[123/130], Train loss:0.034, Train acc: 0.966\n",
       "2022-11-03 21:39:41.241 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000115, Batch[124/130], Train loss:0.035, Train acc: 0.958\n",
       "2022-11-03 21:39:54.879 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000144, Batch[125/130], Train loss:0.033, Train acc: 0.959\n",
       "2022-11-03 21:40:08.473 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000055, Batch[126/130], Train loss:0.028, Train acc: 0.972\n",
       "2022-11-03 21:40:22.050 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000028, Batch[127/130], Train loss:0.029, Train acc: 0.975\n",
       "2022-11-03 21:40:35.644 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000142, Batch[128/130], Train loss:0.034, Train acc: 0.959\n",
       "2022-11-03 21:40:49.269 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000053, Batch[129/130], Train loss:0.043, Train acc: 0.959\n",
       "2022-11-03 21:41:02.853 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000009, Batch[130/130], Train loss:0.044, Train acc: 0.965\n",
       "2022-11-03 21:41:29.124 | INFO     | __main__:<cell line: 25>:70 - Epoch: 29, Train loss: 0.037, Epoch time=1793.938s, valid mean accuracy: 0.954, valid loss: 0.03832871839404106\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "2022-11-03 06:44:53.185 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000107, Batch[1/130], Train loss:1.435, Train acc: 0.108\n2022-11-03 06:45:06.727 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000132, Batch[2/130], Train loss:1.097, Train acc: 0.269\n2022-11-03 06:45:20.267 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000039, Batch[3/130], Train loss:1.228, Train acc: 0.151\n2022-11-03 06:45:33.807 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000103, Batch[4/130], Train loss:1.126, Train acc: 0.222\n2022-11-03 06:45:47.394 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000137, Batch[5/130], Train loss:1.160, Train acc: 0.198\n2022-11-03 06:46:00.932 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000027, Batch[6/130], Train loss:1.173, Train acc: 0.188\n2022-11-03 06:46:14.477 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000029, Batch[7/130], Train loss:1.128, Train acc: 0.215\n2022-11-03 06:46:28.030 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000112, Batch[8/130], Train loss:1.148, Train acc: 0.198\n2022-11-03 06:46:41.610 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000095, Batch[9/130], Train loss:1.205, Train acc: 0.161\n2022-11-03 06:46:55.152 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000122, Batch[10/130], Train loss:1.146, Train acc: 0.200\n2022-11-03 06:47:08.702 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000001, Batch[11/130], Train loss:1.137, Train acc: 0.208\n2022-11-03 06:47:22.267 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000129, Batch[12/130], Train loss:1.237, Train acc: 0.138\n2022-11-03 06:47:35.840 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000007, Batch[13/130], Train loss:1.058, Train acc: 0.264\n2022-11-03 06:47:49.396 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000073, Batch[14/130], Train loss:1.105, Train acc: 0.227\n2022-11-03 06:48:02.950 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000062, Batch[15/130], Train loss:1.150, Train acc: 0.194\n2022-11-03 06:48:16.506 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000115, Batch[16/130], Train loss:1.128, Train acc: 0.207\n2022-11-03 06:48:30.087 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000015, Batch[17/130], Train loss:1.161, Train acc: 0.184\n2022-11-03 06:48:43.656 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000043, Batch[18/130], Train loss:1.217, Train acc: 0.146\n2022-11-03 06:48:57.230 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000134, Batch[19/130], Train loss:1.177, Train acc: 0.173\n2022-11-03 06:49:10.785 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000066, Batch[20/130], Train loss:1.020, Train acc: 0.284\n2022-11-03 06:49:24.359 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000083, Batch[21/130], Train loss:1.045, Train acc: 0.266\n2022-11-03 06:49:37.921 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000140, Batch[22/130], Train loss:1.142, Train acc: 0.193\n2022-11-03 06:49:51.503 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000047, Batch[23/130], Train loss:1.171, Train acc: 0.173\n2022-11-03 06:50:05.073 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000106, Batch[24/130], Train loss:1.044, Train acc: 0.263\n2022-11-03 06:50:18.660 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000045, Batch[25/130], Train loss:1.173, Train acc: 0.170\n2022-11-03 06:50:32.241 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000086, Batch[26/130], Train loss:1.092, Train acc: 0.224\n2022-11-03 06:50:45.820 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000011, Batch[27/130], Train loss:1.157, Train acc: 0.180\n2022-11-03 06:50:59.387 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000068, Batch[28/130], Train loss:1.197, Train acc: 0.151\n2022-11-03 06:51:12.988 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000059, Batch[29/130], Train loss:1.194, Train acc: 0.153\n2022-11-03 06:51:26.553 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000053, Batch[30/130], Train loss:1.098, Train acc: 0.219\n2022-11-03 06:51:40.105 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000130, Batch[31/130], Train loss:1.026, Train acc: 0.270\n2022-11-03 06:51:53.659 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000084, Batch[32/130], Train loss:1.145, Train acc: 0.182\n2022-11-03 06:52:07.260 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000101, Batch[33/130], Train loss:1.175, Train acc: 0.162\n2022-11-03 06:52:20.810 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000080, Batch[34/130], Train loss:1.070, Train acc: 0.235\n2022-11-03 06:52:34.363 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000079, Batch[35/130], Train loss:1.088, Train acc: 0.220\n2022-11-03 06:52:47.911 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000034, Batch[36/130], Train loss:1.157, Train acc: 0.173\n2022-11-03 06:53:01.474 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000017, Batch[37/130], Train loss:1.148, Train acc: 0.177\n2022-11-03 06:53:15.019 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000004, Batch[38/130], Train loss:1.132, Train acc: 0.187\n2022-11-03 06:53:28.577 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000099, Batch[39/130], Train loss:1.141, Train acc: 0.180\n2022-11-03 06:53:42.162 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000133, Batch[40/130], Train loss:1.002, Train acc: 0.282\n2022-11-03 06:53:55.739 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000026, Batch[41/130], Train loss:1.091, Train acc: 0.213\n2022-11-03 06:54:09.302 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000102, Batch[42/130], Train loss:1.075, Train acc: 0.225\n2022-11-03 06:54:22.873 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000078, Batch[43/130], Train loss:1.115, Train acc: 0.194\n2022-11-03 06:54:36.421 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000098, Batch[44/130], Train loss:1.077, Train acc: 0.221\n2022-11-03 06:54:49.986 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000139, Batch[45/130], Train loss:1.015, Train acc: 0.265\n2022-11-03 06:55:03.531 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000087, Batch[46/130], Train loss:1.150, Train acc: 0.169\n2022-11-03 06:55:17.106 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000032, Batch[47/130], Train loss:1.024, Train acc: 0.256\n2022-11-03 06:55:30.659 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000126, Batch[48/130], Train loss:1.129, Train acc: 0.181\n2022-11-03 06:55:44.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000023, Batch[49/130], Train loss:1.137, Train acc: 0.174\n2022-11-03 06:55:57.800 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000037, Batch[50/130], Train loss:1.143, Train acc: 0.170\n2022-11-03 06:56:11.364 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000038, Batch[51/130], Train loss:1.143, Train acc: 0.169\n2022-11-03 06:56:24.936 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000018, Batch[52/130], Train loss:1.040, Train acc: 0.241\n2022-11-03 06:56:38.506 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000144, Batch[53/130], Train loss:1.046, Train acc: 0.236\n2022-11-03 06:56:52.089 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000020, Batch[54/130], Train loss:1.129, Train acc: 0.176\n2022-11-03 06:57:05.653 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000117, Batch[55/130], Train loss:1.093, Train acc: 0.201\n2022-11-03 06:57:19.220 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000136, Batch[56/130], Train loss:1.108, Train acc: 0.189\n2022-11-03 06:57:32.816 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000081, Batch[57/130], Train loss:1.006, Train acc: 0.264\n2022-11-03 06:57:46.369 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000057, Batch[58/130], Train loss:1.070, Train acc: 0.214\n2022-11-03 06:57:59.942 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000076, Batch[59/130], Train loss:0.985, Train acc: 0.277\n2022-11-03 06:58:13.503 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000036, Batch[60/130], Train loss:1.104, Train acc: 0.189\n2022-11-03 06:58:27.099 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000143, Batch[61/130], Train loss:1.077, Train acc: 0.208\n2022-11-03 06:58:40.653 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000030, Batch[62/130], Train loss:1.112, Train acc: 0.182\n2022-11-03 06:58:54.212 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000056, Batch[63/130], Train loss:0.992, Train acc: 0.267\n2022-11-03 06:59:07.765 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000074, Batch[64/130], Train loss:1.018, Train acc: 0.248\n2022-11-03 06:59:21.343 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000028, Batch[65/130], Train loss:0.946, Train acc: 0.302\n2022-11-03 06:59:34.897 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000149, Batch[66/130], Train loss:1.088, Train acc: 0.196\n2022-11-03 06:59:48.449 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000093, Batch[67/130], Train loss:1.013, Train acc: 0.249\n2022-11-03 07:00:02.027 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000092, Batch[68/130], Train loss:1.071, Train acc: 0.210\n2022-11-03 07:00:16.747 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000123, Batch[69/130], Train loss:1.002, Train acc: 0.256\n2022-11-03 07:00:30.297 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000128, Batch[70/130], Train loss:1.115, Train acc: 0.175\n2022-11-03 07:00:43.878 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000069, Batch[71/130], Train loss:1.021, Train acc: 0.241\n2022-11-03 07:00:57.424 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000052, Batch[72/130], Train loss:1.011, Train acc: 0.245\n2022-11-03 07:01:10.992 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000013, Batch[73/130], Train loss:1.063, Train acc: 0.207\n2022-11-03 07:01:24.550 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000041, Batch[74/130], Train loss:0.946, Train acc: 0.293\n2022-11-03 07:01:38.120 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000148, Batch[75/130], Train loss:0.968, Train acc: 0.275\n2022-11-03 07:01:51.677 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000116, Batch[76/130], Train loss:0.964, Train acc: 0.277\n2022-11-03 07:02:05.255 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000008, Batch[77/130], Train loss:0.892, Train acc: 0.334\n2022-11-03 07:02:18.815 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000145, Batch[78/130], Train loss:1.110, Train acc: 0.171\n2022-11-03 07:02:32.361 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000061, Batch[79/130], Train loss:1.027, Train acc: 0.228\n2022-11-03 07:02:45.937 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000118, Batch[80/130], Train loss:1.045, Train acc: 0.214\n2022-11-03 07:02:59.505 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000005, Batch[81/130], Train loss:1.029, Train acc: 0.225\n2022-11-03 07:03:13.060 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000051, Batch[82/130], Train loss:1.102, Train acc: 0.172\n2022-11-03 07:03:26.619 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000121, Batch[83/130], Train loss:0.939, Train acc: 0.291\n2022-11-03 07:03:40.178 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000124, Batch[84/130], Train loss:0.915, Train acc: 0.308\n2022-11-03 07:03:53.795 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000014, Batch[85/130], Train loss:1.000, Train acc: 0.242\n2022-11-03 07:04:07.366 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000055, Batch[86/130], Train loss:1.062, Train acc: 0.197\n2022-11-03 07:04:20.935 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000010, Batch[87/130], Train loss:1.020, Train acc: 0.228\n2022-11-03 07:04:34.507 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000146, Batch[88/130], Train loss:0.898, Train acc: 0.320\n2022-11-03 07:04:48.105 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000113, Batch[89/130], Train loss:0.951, Train acc: 0.275\n2022-11-03 07:05:01.667 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000024, Batch[90/130], Train loss:0.973, Train acc: 0.259\n2022-11-03 07:05:15.234 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000067, Batch[91/130], Train loss:0.912, Train acc: 0.304\n2022-11-03 07:05:28.804 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000003, Batch[92/130], Train loss:1.081, Train acc: 0.179\n2022-11-03 07:05:42.365 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000042, Batch[93/130], Train loss:0.897, Train acc: 0.314\n2022-11-03 07:05:55.914 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000119, Batch[94/130], Train loss:0.906, Train acc: 0.310\n2022-11-03 07:06:09.478 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000021, Batch[95/130], Train loss:1.006, Train acc: 0.232\n2022-11-03 07:06:23.033 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000142, Batch[96/130], Train loss:0.889, Train acc: 0.319\n2022-11-03 07:06:36.604 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000040, Batch[97/130], Train loss:0.863, Train acc: 0.342\n2022-11-03 07:06:50.156 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000141, Batch[98/130], Train loss:1.033, Train acc: 0.210\n2022-11-03 07:07:03.727 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000019, Batch[99/130], Train loss:0.917, Train acc: 0.296\n2022-11-03 07:07:17.275 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000025, Batch[100/130], Train loss:1.052, Train acc: 0.194\n2022-11-03 07:07:30.861 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000089, Batch[101/130], Train loss:0.921, Train acc: 0.291\n2022-11-03 07:07:44.419 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000097, Batch[102/130], Train loss:0.898, Train acc: 0.309\n2022-11-03 07:07:57.970 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000048, Batch[103/130], Train loss:0.911, Train acc: 0.296\n2022-11-03 07:08:11.527 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000127, Batch[104/130], Train loss:0.925, Train acc: 0.284\n2022-11-03 07:08:25.143 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000135, Batch[105/130], Train loss:0.982, Train acc: 0.240\n2022-11-03 07:08:38.718 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000082, Batch[106/130], Train loss:0.951, Train acc: 0.262\n2022-11-03 07:08:52.278 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000064, Batch[107/130], Train loss:0.964, Train acc: 0.252\n2022-11-03 07:09:05.838 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000006, Batch[108/130], Train loss:1.018, Train acc: 0.211\n2022-11-03 07:09:19.425 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000085, Batch[109/130], Train loss:1.005, Train acc: 0.220\n2022-11-03 07:09:33.000 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000058, Batch[110/130], Train loss:0.897, Train acc: 0.300\n2022-11-03 07:09:46.569 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000104, Batch[111/130], Train loss:0.872, Train acc: 0.318\n2022-11-03 07:10:00.125 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000138, Batch[112/130], Train loss:1.041, Train acc: 0.192\n2022-11-03 07:10:13.718 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000131, Batch[113/130], Train loss:1.064, Train acc: 0.175\n2022-11-03 07:10:27.276 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000031, Batch[114/130], Train loss:1.014, Train acc: 0.210\n2022-11-03 07:10:40.848 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000063, Batch[115/130], Train loss:1.029, Train acc: 0.199\n2022-11-03 07:10:54.398 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000111, Batch[116/130], Train loss:0.868, Train acc: 0.318\n2022-11-03 07:11:07.992 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000035, Batch[117/130], Train loss:0.919, Train acc: 0.276\n2022-11-03 07:11:21.542 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000094, Batch[118/130], Train loss:1.002, Train acc: 0.217\n2022-11-03 07:11:35.092 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000065, Batch[119/130], Train loss:0.872, Train acc: 0.313\n2022-11-03 07:11:48.666 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000109, Batch[120/130], Train loss:0.982, Train acc: 0.228\n2022-11-03 07:12:02.233 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000088, Batch[121/130], Train loss:0.784, Train acc: 0.382\n2022-11-03 07:12:15.784 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000070, Batch[122/130], Train loss:0.950, Train acc: 0.250\n2022-11-03 07:12:29.339 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000091, Batch[123/130], Train loss:0.859, Train acc: 0.320\n2022-11-03 07:12:42.895 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000105, Batch[124/130], Train loss:1.002, Train acc: 0.210\n2022-11-03 07:12:56.474 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000022, Batch[125/130], Train loss:0.953, Train acc: 0.244\n2022-11-03 07:13:10.020 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000049, Batch[126/130], Train loss:0.858, Train acc: 0.315\n2022-11-03 07:13:23.585 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000009, Batch[127/130], Train loss:0.807, Train acc: 0.356\n2022-11-03 07:13:37.150 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000072, Batch[128/130], Train loss:0.928, Train acc: 0.260\n2022-11-03 07:13:50.736 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000012, Batch[129/130], Train loss:0.921, Train acc: 0.267\n2022-11-03 07:14:04.283 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000046, Batch[130/130], Train loss:0.980, Train acc: 0.222\n2022-11-03 07:14:30.562 | INFO     | __main__:<cell line: 25>:70 - Epoch: 0, Train loss: 1.042, Epoch time=1790.634s, valid mean accuracy: 0.276, valid loss: 0.8815159797668457\n2022-11-03 07:14:44.062 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000036, Batch[1/130], Train loss:1.000, Train acc: 0.176\n2022-11-03 07:14:57.691 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000073, Batch[2/130], Train loss:1.001, Train acc: 0.258\n2022-11-03 07:15:11.307 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000026, Batch[3/130], Train loss:0.750, Train acc: 0.334\n2022-11-03 07:15:24.937 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000031, Batch[4/130], Train loss:0.556, Train acc: 0.425\n2022-11-03 07:15:38.590 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000009, Batch[5/130], Train loss:0.373, Train acc: 0.623\n2022-11-03 07:15:52.223 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000148, Batch[6/130], Train loss:0.389, Train acc: 0.583\n2022-11-03 07:16:05.846 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000137, Batch[7/130], Train loss:0.362, Train acc: 0.591\n2022-11-03 07:16:19.469 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000039, Batch[8/130], Train loss:0.383, Train acc: 0.639\n2022-11-03 07:16:33.147 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000027, Batch[9/130], Train loss:0.294, Train acc: 0.655\n2022-11-03 07:16:46.765 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000141, Batch[10/130], Train loss:0.265, Train acc: 0.681\n2022-11-03 07:17:00.389 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000066, Batch[11/130], Train loss:0.392, Train acc: 0.597\n2022-11-03 07:17:14.003 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000097, Batch[12/130], Train loss:0.305, Train acc: 0.672\n2022-11-03 07:17:27.641 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000098, Batch[13/130], Train loss:0.376, Train acc: 0.584\n2022-11-03 07:17:41.234 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000081, Batch[14/130], Train loss:0.356, Train acc: 0.616\n2022-11-03 07:17:54.840 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000042, Batch[15/130], Train loss:0.318, Train acc: 0.656\n2022-11-03 07:18:08.472 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000070, Batch[16/130], Train loss:0.334, Train acc: 0.649\n2022-11-03 07:18:22.112 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000025, Batch[17/130], Train loss:0.365, Train acc: 0.631\n2022-11-03 07:18:35.729 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000091, Batch[18/130], Train loss:0.266, Train acc: 0.706\n2022-11-03 07:18:49.356 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000053, Batch[19/130], Train loss:0.305, Train acc: 0.655\n2022-11-03 07:19:02.973 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000112, Batch[20/130], Train loss:0.332, Train acc: 0.616\n2022-11-03 07:19:16.630 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000001, Batch[21/130], Train loss:0.291, Train acc: 0.663\n2022-11-03 07:19:30.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000063, Batch[22/130], Train loss:0.289, Train acc: 0.650\n2022-11-03 07:19:43.886 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000149, Batch[23/130], Train loss:0.248, Train acc: 0.729\n2022-11-03 07:19:57.492 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000032, Batch[24/130], Train loss:0.234, Train acc: 0.727\n2022-11-03 07:20:11.137 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000119, Batch[25/130], Train loss:0.232, Train acc: 0.741\n2022-11-03 07:20:24.745 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000104, Batch[26/130], Train loss:0.263, Train acc: 0.707\n2022-11-03 07:20:38.373 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000124, Batch[27/130], Train loss:0.230, Train acc: 0.741\n2022-11-03 07:20:51.977 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000017, Batch[28/130], Train loss:0.530, Train acc: 0.536\n2022-11-03 07:21:05.605 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000083, Batch[29/130], Train loss:1.117, Train acc: 0.253\n2022-11-03 07:21:19.236 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000101, Batch[30/130], Train loss:0.377, Train acc: 0.560\n2022-11-03 07:21:32.871 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000085, Batch[31/130], Train loss:0.291, Train acc: 0.650\n2022-11-03 07:21:46.496 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000113, Batch[32/130], Train loss:0.355, Train acc: 0.608\n2022-11-03 07:22:00.151 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000006, Batch[33/130], Train loss:0.266, Train acc: 0.673\n2022-11-03 07:22:13.777 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000122, Batch[34/130], Train loss:0.441, Train acc: 0.526\n2022-11-03 07:22:27.398 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000037, Batch[35/130], Train loss:0.247, Train acc: 0.692\n2022-11-03 07:22:41.050 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000065, Batch[36/130], Train loss:0.287, Train acc: 0.675\n2022-11-03 07:22:54.720 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000058, Batch[37/130], Train loss:0.287, Train acc: 0.676\n2022-11-03 07:23:08.355 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000023, Batch[38/130], Train loss:0.240, Train acc: 0.702\n2022-11-03 07:23:21.974 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000012, Batch[39/130], Train loss:0.286, Train acc: 0.687\n2022-11-03 07:23:35.583 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000019, Batch[40/130], Train loss:0.317, Train acc: 0.648\n2022-11-03 07:23:49.216 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000047, Batch[41/130], Train loss:0.353, Train acc: 0.651\n2022-11-03 07:24:02.851 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000029, Batch[42/130], Train loss:0.267, Train acc: 0.695\n2022-11-03 07:24:16.466 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000015, Batch[43/130], Train loss:0.296, Train acc: 0.665\n2022-11-03 07:24:30.107 | INFO     | __main__:<cell line: 25>:60 - Epoc\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n| __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000143, Batch[90/130], Train loss:0.056, Train acc: 0.927\n2022-11-03 21:02:18.319 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000065, Batch[91/130], Train loss:0.032, Train acc: 0.965\n2022-11-03 21:02:31.895 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000079, Batch[92/130], Train loss:0.049, Train acc: 0.934\n2022-11-03 21:02:45.524 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000011, Batch[93/130], Train loss:0.037, Train acc: 0.964\n2022-11-03 21:02:59.085 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000098, Batch[94/130], Train loss:0.042, Train acc: 0.945\n2022-11-03 21:03:12.663 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000136, Batch[95/130], Train loss:0.044, Train acc: 0.942\n2022-11-03 21:03:26.236 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000127, Batch[96/130], Train loss:0.037, Train acc: 0.956\n2022-11-03 21:03:39.867 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000141, Batch[97/130], Train loss:0.045, Train acc: 0.957\n2022-11-03 21:03:53.433 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000040, Batch[98/130], Train loss:0.039, Train acc: 0.969\n2022-11-03 21:04:07.010 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000056, Batch[99/130], Train loss:0.033, Train acc: 0.971\n2022-11-03 21:04:20.599 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000061, Batch[100/130], Train loss:0.030, Train acc: 0.964\n2022-11-03 21:04:34.187 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000103, Batch[101/130], Train loss:0.042, Train acc: 0.944\n2022-11-03 21:04:47.769 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000008, Batch[102/130], Train loss:0.025, Train acc: 0.975\n2022-11-03 21:05:01.389 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000078, Batch[103/130], Train loss:0.049, Train acc: 0.933\n2022-11-03 21:05:14.989 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000005, Batch[104/130], Train loss:0.031, Train acc: 0.963\n2022-11-03 21:05:28.620 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000039, Batch[105/130], Train loss:0.036, Train acc: 0.952\n2022-11-03 21:05:42.219 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000083, Batch[106/130], Train loss:0.037, Train acc: 0.956\n2022-11-03 21:05:55.826 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000087, Batch[107/130], Train loss:0.036, Train acc: 0.951\n2022-11-03 21:06:09.414 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000132, Batch[108/130], Train loss:0.034, Train acc: 0.960\n2022-11-03 21:06:23.039 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000149, Batch[109/130], Train loss:0.044, Train acc: 0.943\n2022-11-03 21:06:36.633 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000112, Batch[110/130], Train loss:0.037, Train acc: 0.960\n2022-11-03 21:06:50.223 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000026, Batch[111/130], Train loss:0.047, Train acc: 0.964\n2022-11-03 21:07:03.837 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000052, Batch[112/130], Train loss:0.038, Train acc: 0.968\n2022-11-03 21:07:17.428 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000119, Batch[113/130], Train loss:0.034, Train acc: 0.968\n2022-11-03 21:07:31.020 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000148, Batch[114/130], Train loss:0.038, Train acc: 0.965\n2022-11-03 21:07:44.605 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000113, Batch[115/130], Train loss:0.029, Train acc: 0.967\n2022-11-03 21:07:58.208 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000013, Batch[116/130], Train loss:0.028, Train acc: 0.970\n2022-11-03 21:08:11.843 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000067, Batch[117/130], Train loss:0.029, Train acc: 0.964\n2022-11-03 21:08:25.445 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000138, Batch[118/130], Train loss:0.048, Train acc: 0.933\n2022-11-03 21:08:39.048 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000088, Batch[119/130], Train loss:0.031, Train acc: 0.963\n2022-11-03 21:08:52.637 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000017, Batch[120/130], Train loss:0.030, Train acc: 0.967\n2022-11-03 21:09:06.265 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000099, Batch[121/130], Train loss:0.039, Train acc: 0.953\n2022-11-03 21:09:19.878 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000046, Batch[122/130], Train loss:0.036, Train acc: 0.952\n2022-11-03 21:09:33.474 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000014, Batch[123/130], Train loss:0.054, Train acc: 0.946\n2022-11-03 21:09:47.066 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000104, Batch[124/130], Train loss:0.034, Train acc: 0.960\n2022-11-03 21:10:00.686 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000086, Batch[125/130], Train loss:0.035, Train acc: 0.955\n2022-11-03 21:10:14.287 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000025, Batch[126/130], Train loss:0.037, Train acc: 0.963\n2022-11-03 21:10:27.868 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000070, Batch[127/130], Train loss:0.036, Train acc: 0.959\n2022-11-03 21:10:41.457 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000066, Batch[128/130], Train loss:0.029, Train acc: 0.970\n2022-11-03 21:10:55.070 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000133, Batch[129/130], Train loss:0.044, Train acc: 0.946\n2022-11-03 21:11:08.654 | INFO     | __main__:<cell line: 25>:60 - Epoch: 28, idx:BJ00000102, Batch[130/130], Train loss:0.041, Train acc: 0.950\n2022-11-03 21:11:34.921 | INFO     | __main__:<cell line: 25>:70 - Epoch: 28, Train loss: 0.038, Epoch time=1793.936s, valid mean accuracy: 0.959, valid loss: 0.03815590217709541\n2022-11-03 21:11:48.391 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000121, Batch[1/130], Train loss:0.030, Train acc: 0.967\n2022-11-03 21:12:01.962 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000124, Batch[2/130], Train loss:0.038, Train acc: 0.952\n2022-11-03 21:12:15.548 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000018, Batch[3/130], Train loss:0.044, Train acc: 0.965\n2022-11-03 21:12:29.149 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000128, Batch[4/130], Train loss:0.043, Train acc: 0.949\n2022-11-03 21:12:42.734 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000067, Batch[5/130], Train loss:0.027, Train acc: 0.968\n2022-11-03 21:12:56.336 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000026, Batch[6/130], Train loss:0.025, Train acc: 0.975\n2022-11-03 21:13:09.917 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000080, Batch[7/130], Train loss:0.042, Train acc: 0.947\n2022-11-03 21:13:23.515 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000073, Batch[8/130], Train loss:0.035, Train acc: 0.957\n2022-11-03 21:13:37.128 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000135, Batch[9/130], Train loss:0.033, Train acc: 0.956\n2022-11-03 21:13:50.732 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000143, Batch[10/130], Train loss:0.050, Train acc: 0.938\n2022-11-03 21:14:04.330 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000118, Batch[11/130], Train loss:0.039, Train acc: 0.949\n2022-11-03 21:14:17.920 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000010, Batch[12/130], Train loss:0.052, Train acc: 0.959\n2022-11-03 21:14:31.535 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000095, Batch[13/130], Train loss:0.043, Train acc: 0.943\n2022-11-03 21:14:45.112 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000133, Batch[14/130], Train loss:0.041, Train acc: 0.952\n2022-11-03 21:14:58.690 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000089, Batch[15/130], Train loss:0.026, Train acc: 0.970\n2022-11-03 21:15:12.285 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000069, Batch[16/130], Train loss:0.042, Train acc: 0.960\n2022-11-03 21:15:25.919 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000078, Batch[17/130], Train loss:0.041, Train acc: 0.947\n2022-11-03 21:15:39.516 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000015, Batch[18/130], Train loss:0.045, Train acc: 0.960\n2022-11-03 21:15:53.108 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000003, Batch[19/130], Train loss:0.032, Train acc: 0.960\n2022-11-03 21:16:06.707 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000119, Batch[20/130], Train loss:0.029, Train acc: 0.968\n2022-11-03 21:16:20.327 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000006, Batch[21/130], Train loss:0.034, Train acc: 0.961\n2022-11-03 21:16:33.917 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000126, Batch[22/130], Train loss:0.043, Train acc: 0.943\n2022-11-03 21:16:47.525 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000094, Batch[23/130], Train loss:0.041, Train acc: 0.947\n2022-11-03 21:17:01.128 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000046, Batch[24/130], Train loss:0.034, Train acc: 0.955\n2022-11-03 21:17:14.759 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000036, Batch[25/130], Train loss:0.040, Train acc: 0.951\n2022-11-03 21:17:28.330 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000116, Batch[26/130], Train loss:0.037, Train acc: 0.961\n2022-11-03 21:17:41.944 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000063, Batch[27/130], Train loss:0.037, Train acc: 0.965\n2022-11-03 21:17:55.558 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000001, Batch[28/130], Train loss:0.031, Train acc: 0.968\n2022-11-03 21:18:09.173 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000020, Batch[29/130], Train loss:0.037, Train acc: 0.966\n2022-11-03 21:18:22.762 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000103, Batch[30/130], Train loss:0.035, Train acc: 0.956\n2022-11-03 21:18:36.356 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000061, Batch[31/130], Train loss:0.028, Train acc: 0.970\n2022-11-03 21:18:49.965 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000101, Batch[32/130], Train loss:0.042, Train acc: 0.946\n2022-11-03 21:19:03.578 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000070, Batch[33/130], Train loss:0.036, Train acc: 0.961\n2022-11-03 21:19:17.196 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000056, Batch[34/130], Train loss:0.031, Train acc: 0.971\n2022-11-03 21:19:30.779 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000122, Batch[35/130], Train loss:0.034, Train acc: 0.958\n2022-11-03 21:19:44.372 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000146, Batch[36/130], Train loss:0.031, Train acc: 0.965\n2022-11-03 21:19:57.966 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000019, Batch[37/130], Train loss:0.028, Train acc: 0.968\n2022-11-03 21:20:11.559 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000029, Batch[38/130], Train loss:0.028, Train acc: 0.967\n2022-11-03 21:20:25.170 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000035, Batch[39/130], Train loss:0.031, Train acc: 0.968\n2022-11-03 21:20:38.775 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000097, Batch[40/130], Train loss:0.040, Train acc: 0.950\n2022-11-03 21:20:52.405 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000092, Batch[41/130], Train loss:0.044, Train acc: 0.947\n2022-11-03 21:21:05.993 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000086, Batch[42/130], Train loss:0.033, Train acc: 0.957\n2022-11-03 21:21:19.560 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000107, Batch[43/130], Train loss:0.036, Train acc: 0.959\n2022-11-03 21:21:33.153 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000131, Batch[44/130], Train loss:0.035, Train acc: 0.953\n2022-11-03 21:21:46.764 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000083, Batch[45/130], Train loss:0.036, Train acc: 0.961\n2022-11-03 21:22:00.332 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000032, Batch[46/130], Train loss:0.026, Train acc: 0.975\n2022-11-03 21:22:13.932 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000027, Batch[47/130], Train loss:0.038, Train acc: 0.961\n2022-11-03 21:22:27.553 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000052, Batch[48/130], Train loss:0.029, Train acc: 0.972\n2022-11-03 21:22:41.154 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000049, Batch[49/130], Train loss:0.026, Train acc: 0.970\n2022-11-03 21:22:54.717 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000042, Batch[50/130], Train loss:0.030, Train acc: 0.969\n2022-11-03 21:23:08.305 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000023, Batch[51/130], Train loss:0.038, Train acc: 0.966\n2022-11-03 21:23:21.905 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000087, Batch[52/130], Train loss:0.038, Train acc: 0.949\n2022-11-03 21:23:35.547 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000136, Batch[53/130], Train loss:0.043, Train acc: 0.942\n2022-11-03 21:23:49.131 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000041, Batch[54/130], Train loss:0.027, Train acc: 0.969\n2022-11-03 21:24:02.760 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000030, Batch[55/130], Train loss:0.031, Train acc: 0.958\n2022-11-03 21:24:16.350 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000088, Batch[56/130], Train loss:0.029, Train acc: 0.967\n2022-11-03 21:24:29.975 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000085, Batch[57/130], Train loss:0.045, Train acc: 0.954\n2022-11-03 21:24:43.567 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000068, Batch[58/130], Train loss:0.044, Train acc: 0.950\n2022-11-03 21:24:57.160 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000022, Batch[59/130], Train loss:0.034, Train acc: 0.970\n2022-11-03 21:25:10.769 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000062, Batch[60/130], Train loss:0.035, Train acc: 0.965\n2022-11-03 21:25:24.401 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000140, Batch[61/130], Train loss:0.041, Train acc: 0.950\n2022-11-03 21:25:38.006 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000093, Batch[62/130], Train loss:0.035, Train acc: 0.956\n2022-11-03 21:25:51.581 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000007, Batch[63/130], Train loss:0.025, Train acc: 0.977\n2022-11-03 21:26:05.170 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000109, Batch[64/130], Train loss:0.040, Train acc: 0.945\n2022-11-03 21:26:18.780 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000134, Batch[65/130], Train loss:0.048, Train acc: 0.934\n2022-11-03 21:26:32.382 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000065, Batch[66/130], Train loss:0.033, Train acc: 0.963\n2022-11-03 21:26:45.978 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000037, Batch[67/130], Train loss:0.029, Train acc: 0.967\n2022-11-03 21:26:59.586 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000129, Batch[68/130], Train loss:0.040, Train acc: 0.952\n2022-11-03 21:27:13.229 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000076, Batch[69/130], Train loss:0.051, Train acc: 0.936\n2022-11-03 21:27:26.814 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000039, Batch[70/130], Train loss:0.034, Train acc: 0.956\n2022-11-03 21:27:40.387 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000139, Batch[71/130], Train loss:0.033, Train acc: 0.965\n2022-11-03 21:27:53.979 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000072, Batch[72/130], Train loss:0.034, Train acc: 0.966\n2022-11-03 21:28:07.618 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000025, Batch[73/130], Train loss:0.045, Train acc: 0.960\n2022-11-03 21:28:21.181 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000008, Batch[74/130], Train loss:0.031, Train acc: 0.973\n2022-11-03 21:28:34.760 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000066, Batch[75/130], Train loss:0.030, Train acc: 0.971\n2022-11-03 21:28:48.372 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000004, Batch[76/130], Train loss:0.053, Train acc: 0.930\n2022-11-03 21:29:02.003 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000005, Batch[77/130], Train loss:0.031, Train acc: 0.966\n2022-11-03 21:29:15.606 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000145, Batch[78/130], Train loss:0.045, Train acc: 0.951\n2022-11-03 21:29:29.215 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000149, Batch[79/130], Train loss:0.052, Train acc: 0.929\n2022-11-03 21:29:42.804 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000106, Batch[80/130], Train loss:0.030, Train acc: 0.960\n2022-11-03 21:29:56.424 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000045, Batch[81/130], Train loss:0.031, Train acc: 0.966\n2022-11-03 21:30:10.001 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000113, Batch[82/130], Train loss:0.030, Train acc: 0.962\n2022-11-03 21:30:23.630 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000013, Batch[83/130], Train loss:0.026, Train acc: 0.971\n2022-11-03 21:30:37.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000038, Batch[84/130], Train loss:0.032, Train acc: 0.963\n2022-11-03 21:30:50.863 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000137, Batch[85/130], Train loss:0.037, Train acc: 0.953\n2022-11-03 21:31:04.464 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000059, Batch[86/130], Train loss:0.054, Train acc: 0.942\n2022-11-03 21:31:18.058 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000024, Batch[87/130], Train loss:0.039, Train acc: 0.966\n2022-11-03 21:31:31.642 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000127, Batch[88/130], Train loss:0.036, Train acc: 0.955\n2022-11-03 21:31:45.273 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000012, Batch[89/130], Train loss:0.045, Train acc: 0.962\n2022-11-03 21:31:58.885 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000040, Batch[90/130], Train loss:0.030, Train acc: 0.973\n2022-11-03 21:32:12.468 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000011, Batch[91/130], Train loss:0.033, Train acc: 0.965\n2022-11-03 21:32:26.051 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000082, Batch[92/130], Train loss:0.044, Train acc: 0.941\n2022-11-03 21:32:39.658 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000148, Batch[93/130], Train loss:0.032, Train acc: 0.963\n2022-11-03 21:32:53.225 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000112, Batch[94/130], Train loss:0.047, Train acc: 0.940\n2022-11-03 21:33:06.816 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000047, Batch[95/130], Train loss:0.053, Train acc: 0.938\n2022-11-03 21:33:20.389 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000079, Batch[96/130], Train loss:0.056, Train acc: 0.925\n2022-11-03 21:33:34.030 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000048, Batch[97/130], Train loss:0.031, Train acc: 0.964\n2022-11-03 21:33:47.609 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000098, Batch[98/130], Train loss:0.048, Train acc: 0.935\n2022-11-03 21:34:01.198 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000064, Batch[99/130], Train loss:0.036, Train acc: 0.955\n2022-11-03 21:34:14.789 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000014, Batch[100/130], Train loss:0.052, Train acc: 0.948\n2022-11-03 21:34:28.406 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000102, Batch[101/130], Train loss:0.041, Train acc: 0.949\n2022-11-03 21:34:42.002 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000074, Batch[102/130], Train loss:0.037, Train acc: 0.961\n2022-11-03 21:34:55.593 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000117, Batch[103/130], Train loss:0.048, Train acc: 0.936\n2022-11-03 21:35:09.190 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000123, Batch[104/130], Train loss:0.035, Train acc: 0.960\n2022-11-03 21:35:22.804 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000099, Batch[105/130], Train loss:0.043, Train acc: 0.956\n2022-11-03 21:35:36.387 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000111, Batch[106/130], Train loss:0.034, Train acc: 0.960\n2022-11-03 21:35:49.983 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000051, Batch[107/130], Train loss:0.030, Train acc: 0.969\n2022-11-03 21:36:03.551 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000084, Batch[108/130], Train loss:0.035, Train acc: 0.955\n2022-11-03 21:36:17.157 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000058, Batch[109/130], Train loss:0.038, Train acc: 0.969\n2022-11-03 21:36:30.768 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000021, Batch[110/130], Train loss:0.031, Train acc: 0.972\n2022-11-03 21:36:44.392 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000141, Batch[111/130], Train loss:0.038, Train acc: 0.958\n2022-11-03 21:36:57.984 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000132, Batch[112/130], Train loss:0.036, Train acc: 0.957\n2022-11-03 21:37:11.604 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000043, Batch[113/130], Train loss:0.032, Train acc: 0.967\n2022-11-03 21:37:25.208 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000138, Batch[114/130], Train loss:0.050, Train acc: 0.929\n2022-11-03 21:37:38.820 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000091, Batch[115/130], Train loss:0.041, Train acc: 0.948\n2022-11-03 21:37:52.408 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000081, Batch[116/130], Train loss:0.046, Train acc: 0.940\n2022-11-03 21:38:06.050 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000057, Batch[117/130], Train loss:0.031, Train acc: 0.963\n2022-11-03 21:38:19.692 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000034, Batch[118/130], Train loss:0.034, Train acc: 0.958\n2022-11-03 21:38:33.261 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000130, Batch[119/130], Train loss:0.037, Train acc: 0.958\n2022-11-03 21:38:46.872 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000017, Batch[120/130], Train loss:0.033, Train acc: 0.969\n2022-11-03 21:39:00.492 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000105, Batch[121/130], Train loss:0.038, Train acc: 0.949\n2022-11-03 21:39:14.065 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000104, Batch[122/130], Train loss:0.038, Train acc: 0.963\n2022-11-03 21:39:27.661 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000031, Batch[123/130], Train loss:0.034, Train acc: 0.966\n2022-11-03 21:39:41.241 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000115, Batch[124/130], Train loss:0.035, Train acc: 0.958\n2022-11-03 21:39:54.879 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000144, Batch[125/130], Train loss:0.033, Train acc: 0.959\n2022-11-03 21:40:08.473 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000055, Batch[126/130], Train loss:0.028, Train acc: 0.972\n2022-11-03 21:40:22.050 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000028, Batch[127/130], Train loss:0.029, Train acc: 0.975\n2022-11-03 21:40:35.644 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000142, Batch[128/130], Train loss:0.034, Train acc: 0.959\n2022-11-03 21:40:49.269 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000053, Batch[129/130], Train loss:0.043, Train acc: 0.959\n2022-11-03 21:41:02.853 | INFO     | __main__:<cell line: 25>:60 - Epoch: 29, idx:BJ00000009, Batch[130/130], Train loss:0.044, Train acc: 0.965\n2022-11-03 21:41:29.124 | INFO     | __main__:<cell line: 25>:70 - Epoch: 29, Train loss: 0.037, Epoch time=1793.938s, valid mean accuracy: 0.954, valid loss: 0.03832871839404106\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    acc_list = []\n",
    "    losses = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, x, y in test_loader:\n",
    "            # x, y = x.to(device), y.to(device)\n",
    "            # with autocast():\n",
    "            logits = model(x.to(next(model.parameters()).device))\n",
    "            logits = F.softmax(logits, dim=1)\n",
    "            y = y.to(logits.device)\n",
    "            loss = evalmetric.tversky_loss(y, logits)\n",
    "            losses += loss\n",
    "            acc = evalmetric.dice_coef_multilabel(y, logits)\n",
    "            acc_list.append(acc)\n",
    "        val_loss = losses / len(test_loader)\n",
    "        return acc_list, sum(acc_list) / acc_list.__len__(), val_loss\n",
    "# 暂时使用交叉熵损失函数测试模型\n",
    "\n",
    "max_acc = 0\n",
    "# optimizer = torch.optim.ASGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = 0\n",
    "    start_time = time.time()\n",
    "    for i, (idx, x, y) in enumerate(train_loader):\n",
    "        # print(x.shape, y.shape)\n",
    "        # for s in range(0, 240, 40):\n",
    "        #     draw_multi(x[0][0][s], y[0][1][s], y[0][2][s])\n",
    "\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        logits = model(x.to(next(model.parameters()).device))\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        y = y.to(logits.device)\n",
    "        loss = evalmetric.tversky_loss(y, logits)\n",
    "        origin_loss = loss\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 模型并行必须讲处理之后的loss值重新赋值给原来的loss，使用原来的loss去backword\n",
    "        # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        #     # logger.info(f\"type {type(scaled_loss)}, {scaled_loss.shape}, {scaled_loss}, {scaled_loss.device}\")\n",
    "        #     loss = scaled_loss   \n",
    "\n",
    "            # scaled_loss.to(next(model.parameters()).device)\n",
    "            # scaled_loss.backward()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 1)\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "        acc = evalmetric.dice_coef_multilabel(y, logits)\n",
    "        \n",
    "        # if hasattr(torch.cuda, 'empty_cache'):\n",
    "        #     torch.cuda.empty_cache()\n",
    "        if i % 1 == 0:\n",
    "            logger.info(f\"Epoch: {epoch}, idx:{idx[0]}, Batch[{i+1}/{len(train_loader)}],\"\n",
    "                        f\" Train loss:{origin_loss:.3f}, Train acc:{acc: .3f}\")\n",
    "        \n",
    "    #     break\n",
    "    # break\n",
    "    acc_list, mean_acc, val_loss = evaluate(model, test_loader)\n",
    "    end_time = time.time()\n",
    "    train_loss = losses / len(train_loader)\n",
    "    if mean_acc > max_acc:\n",
    "        torch.save(model.state_dict(), f\"{model_path}epoch{epoch+1}_trainloss_{train_loss:.3f}_validacc_{mean_acc:.3f}.pth\")\n",
    "    logger.info(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Epoch time={(end_time - start_time):.3f}s, valid mean accuracy: {mean_acc:.3f}, valid loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9286d042-0c5b-49d5-b4d0-0c53125694fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022-11-08 06:36:13.012 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000001, Batch[1/135], Train loss:1.366, Train acc: 0.071\n",
       "2022-11-08 06:36:27.310 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000055, Batch[2/135], Train loss:1.266, Train acc: 0.107\n",
       "2022-11-08 06:36:41.587 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000006, Batch[3/135], Train loss:1.206, Train acc: 0.136\n",
       "2022-11-08 06:36:55.910 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000019, Batch[4/135], Train loss:1.127, Train acc: 0.195\n",
       "2022-11-08 06:37:10.230 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000065, Batch[5/135], Train loss:1.122, Train acc: 0.198\n",
       "2022-11-08 06:37:24.526 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000038, Batch[6/135], Train loss:1.214, Train acc: 0.128\n",
       "2022-11-08 06:37:38.816 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000141, Batch[7/135], Train loss:1.193, Train acc: 0.143\n",
       "2022-11-08 06:37:53.164 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000025, Batch[8/135], Train loss:1.210, Train acc: 0.130\n",
       "2022-11-08 06:38:07.468 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000041, Batch[9/135], Train loss:1.103, Train acc: 0.209\n",
       "2022-11-08 06:38:21.764 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000026, Batch[10/135], Train loss:1.159, Train acc: 0.165\n",
       "2022-11-08 06:38:36.076 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000027, Batch[11/135], Train loss:1.166, Train acc: 0.159\n",
       "2022-11-08 06:38:50.364 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000021, Batch[12/135], Train loss:1.161, Train acc: 0.162\n",
       "2022-11-08 06:39:04.663 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000082, Batch[13/135], Train loss:1.140, Train acc: 0.177\n",
       "2022-11-08 06:39:18.957 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000023, Batch[14/135], Train loss:1.195, Train acc: 0.136\n",
       "2022-11-08 06:39:33.288 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000049, Batch[15/135], Train loss:1.104, Train acc: 0.203\n",
       "2022-11-08 06:39:47.583 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000058, Batch[16/135], Train loss:1.103, Train acc: 0.203\n",
       "2022-11-08 06:40:01.897 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000146, Batch[17/135], Train loss:1.070, Train acc: 0.229\n",
       "2022-11-08 06:40:16.232 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000085, Batch[18/135], Train loss:1.170, Train acc: 0.151\n",
       "2022-11-08 06:40:30.529 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000102, Batch[19/135], Train loss:1.130, Train acc: 0.180\n",
       "2022-11-08 06:40:44.831 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000108, Batch[20/135], Train loss:1.184, Train acc: 0.140\n",
       "2022-11-08 06:40:59.140 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000093, Batch[21/135], Train loss:1.115, Train acc: 0.190\n",
       "2022-11-08 06:41:13.475 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000004, Batch[22/135], Train loss:1.163, Train acc: 0.154\n",
       "2022-11-08 06:41:27.774 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000014, Batch[23/135], Train loss:1.127, Train acc: 0.179\n",
       "2022-11-08 06:41:42.084 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000045, Batch[24/135], Train loss:1.170, Train acc: 0.146\n",
       "2022-11-08 06:41:56.404 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000059, Batch[25/135], Train loss:1.192, Train acc: 0.131\n",
       "2022-11-08 06:42:10.707 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000043, Batch[26/135], Train loss:1.194, Train acc: 0.129\n",
       "2022-11-08 06:42:25.001 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000090, Batch[27/135], Train loss:1.142, Train acc: 0.165\n",
       "2022-11-08 06:42:39.322 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000031, Batch[28/135], Train loss:1.163, Train acc: 0.149\n",
       "2022-11-08 06:42:53.650 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000128, Batch[29/135], Train loss:1.176, Train acc: 0.139\n",
       "2022-11-08 06:43:07.950 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000064, Batch[30/135], Train loss:1.118, Train acc: 0.180\n",
       "2022-11-08 06:43:22.243 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000079, Batch[31/135], Train loss:1.107, Train acc: 0.187\n",
       "2022-11-08 06:43:36.539 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000022, Batch[32/135], Train loss:1.130, Train acc: 0.169\n",
       "2022-11-08 06:43:50.872 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000018, Batch[33/135], Train loss:1.088, Train acc: 0.199\n",
       "2022-11-08 06:44:05.158 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000127, Batch[34/135], Train loss:1.078, Train acc: 0.208\n",
       "2022-11-08 06:44:19.476 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000005, Batch[35/135], Train loss:1.118, Train acc: 0.176\n",
       "2022-11-08 06:44:33.788 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000034, Batch[36/135], Train loss:1.151, Train acc: 0.152\n",
       "2022-11-08 06:44:48.094 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000125, Batch[37/135], Train loss:1.042, Train acc: 0.234\n",
       "2022-11-08 06:45:02.401 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000123, Batch[38/135], Train loss:1.073, Train acc: 0.207\n",
       "2022-11-08 06:45:16.719 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000047, Batch[39/135], Train loss:1.140, Train acc: 0.157\n",
       "2022-11-08 06:45:31.017 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000111, Batch[40/135], Train loss:1.043, Train acc: 0.230\n",
       "2022-11-08 06:45:45.318 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000039, Batch[41/135], Train loss:1.158, Train acc: 0.144\n",
       "2022-11-08 06:45:59.615 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000009, Batch[42/135], Train loss:1.015, Train acc: 0.252\n",
       "2022-11-08 06:46:13.925 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000089, Batch[43/135], Train loss:1.049, Train acc: 0.222\n",
       "2022-11-08 06:46:28.229 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000124, Batch[44/135], Train loss:1.024, Train acc: 0.242\n",
       "2022-11-08 06:46:42.538 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000122, Batch[45/135], Train loss:1.097, Train acc: 0.185\n",
       "2022-11-08 06:46:56.871 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000115, Batch[46/135], Train loss:1.083, Train acc: 0.193\n",
       "2022-11-08 06:47:11.159 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000030, Batch[47/135], Train loss:1.134, Train acc: 0.156\n",
       "2022-11-08 06:47:25.478 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000121, Batch[48/135], Train loss:1.031, Train acc: 0.233\n",
       "2022-11-08 06:47:39.790 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000035, Batch[49/135], Train loss:1.064, Train acc: 0.206\n",
       "2022-11-08 06:47:54.114 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000069, Batch[50/135], Train loss:1.068, Train acc: 0.202\n",
       "2022-11-08 06:48:08.425 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000109, Batch[51/135], Train loss:1.108, Train acc: 0.171\n",
       "2022-11-08 06:48:22.719 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000095, Batch[52/135], Train loss:1.126, Train acc: 0.158\n",
       "2022-11-08 06:48:37.067 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000061, Batch[53/135], Train loss:1.081, Train acc: 0.191\n",
       "2022-11-08 06:48:51.363 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000148, Batch[54/135], Train loss:1.027, Train acc: 0.230\n",
       "2022-11-08 06:49:05.679 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000076, Batch[55/135], Train loss:1.015, Train acc: 0.240\n",
       "2022-11-08 06:49:19.977 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000138, Batch[56/135], Train loss:1.134, Train acc: 0.150\n",
       "2022-11-08 06:49:34.310 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000057, Batch[57/135], Train loss:1.079, Train acc: 0.189\n",
       "2022-11-08 06:49:48.600 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000070, Batch[58/135], Train loss:1.073, Train acc: 0.192\n",
       "2022-11-08 06:50:02.910 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000083, Batch[59/135], Train loss:0.997, Train acc: 0.251\n",
       "2022-11-08 06:50:17.216 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000131, Batch[60/135], Train loss:1.143, Train acc: 0.140\n",
       "2022-11-08 06:50:31.533 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000081, Batch[61/135], Train loss:1.015, Train acc: 0.234\n",
       "2022-11-08 06:50:45.844 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000118, Batch[62/135], Train loss:1.081, Train acc: 0.183\n",
       "2022-11-08 06:51:00.155 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000105, Batch[63/135], Train loss:1.107, Train acc: 0.163\n",
       "2022-11-08 06:51:14.455 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000104, Batch[64/135], Train loss:0.993, Train acc: 0.250\n",
       "2022-11-08 06:51:28.792 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000107, Batch[65/135], Train loss:0.976, Train acc: 0.263\n",
       "2022-11-08 06:51:43.089 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000098, Batch[66/135], Train loss:1.044, Train acc: 0.207\n",
       "2022-11-08 06:51:57.408 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000139, Batch[67/135], Train loss:0.995, Train acc: 0.244\n",
       "2022-11-08 06:52:11.695 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000130, Batch[68/135], Train loss:0.980, Train acc: 0.256\n",
       "2022-11-08 06:52:25.995 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000129, Batch[69/135], Train loss:1.136, Train acc: 0.140\n",
       "2022-11-08 06:52:40.300 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000078, Batch[70/135], Train loss:1.074, Train acc: 0.184\n",
       "2022-11-08 06:52:54.604 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000052, Batch[71/135], Train loss:1.028, Train acc: 0.216\n",
       "2022-11-08 06:53:08.934 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000051, Batch[72/135], Train loss:1.115, Train acc: 0.153\n",
       "2022-11-08 06:53:23.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000113, Batch[73/135], Train loss:1.004, Train acc: 0.234\n",
       "2022-11-08 06:53:37.569 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000119, Batch[74/135], Train loss:0.967, Train acc: 0.263\n",
       "2022-11-08 06:53:51.875 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000110, Batch[75/135], Train loss:1.123, Train acc: 0.144\n",
       "2022-11-08 06:54:06.168 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000101, Batch[76/135], Train loss:1.093, Train acc: 0.163\n",
       "2022-11-08 06:54:20.484 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000063, Batch[77/135], Train loss:1.090, Train acc: 0.166\n",
       "2022-11-08 06:54:34.799 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000145, Batch[78/135], Train loss:1.102, Train acc: 0.157\n",
       "2022-11-08 06:54:49.097 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000099, Batch[79/135], Train loss:1.067, Train acc: 0.181\n",
       "2022-11-08 06:55:03.390 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000072, Batch[80/135], Train loss:1.028, Train acc: 0.208\n",
       "2022-11-08 06:55:17.723 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000080, Batch[81/135], Train loss:0.993, Train acc: 0.234\n",
       "2022-11-08 06:55:32.041 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000112, Batch[82/135], Train loss:1.028, Train acc: 0.208\n",
       "2022-11-08 06:55:46.332 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000074, Batch[83/135], Train loss:0.993, Train acc: 0.233\n",
       "2022-11-08 06:56:00.626 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000042, Batch[84/135], Train loss:0.944, Train acc: 0.273\n",
       "2022-11-08 06:56:14.996 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000066, Batch[85/135], Train loss:0.926, Train acc: 0.288\n",
       "2022-11-08 06:56:29.287 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000056, Batch[86/135], Train loss:0.968, Train acc: 0.252\n",
       "2022-11-08 06:56:43.589 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000013, Batch[87/135], Train loss:1.039, Train acc: 0.195\n",
       "2022-11-08 06:56:57.893 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000091, Batch[88/135], Train loss:0.950, Train acc: 0.264\n",
       "2022-11-08 06:57:12.229 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000046, Batch[89/135], Train loss:1.053, Train acc: 0.184\n",
       "2022-11-08 06:57:26.512 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000106, Batch[90/135], Train loss:0.938, Train acc: 0.272\n",
       "2022-11-08 06:57:40.803 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000132, Batch[91/135], Train loss:0.892, Train acc: 0.310\n",
       "2022-11-08 06:57:55.140 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000092, Batch[92/135], Train loss:1.027, Train acc: 0.203\n",
       "2022-11-08 06:58:09.438 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000024, Batch[93/135], Train loss:0.978, Train acc: 0.237\n",
       "2022-11-08 06:58:23.746 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000142, Batch[94/135], Train loss:0.917, Train acc: 0.285\n",
       "2022-11-08 06:58:38.087 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000008, Batch[95/135], Train loss:0.886, Train acc: 0.311\n",
       "2022-11-08 06:58:52.388 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000012, Batch[96/135], Train loss:0.993, Train acc: 0.224\n",
       "2022-11-08 06:59:06.700 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000010, Batch[97/135], Train loss:1.005, Train acc: 0.214\n",
       "2022-11-08 06:59:20.997 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000028, Batch[98/135], Train loss:0.902, Train acc: 0.294\n",
       "2022-11-08 06:59:35.305 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000133, Batch[99/135], Train loss:0.918, Train acc: 0.285\n",
       "2022-11-08 06:59:49.603 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000084, Batch[100/135], Train loss:1.029, Train acc: 0.193\n",
       "2022-11-08 07:00:03.902 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000097, Batch[101/135], Train loss:0.917, Train acc: 0.280\n",
       "2022-11-08 07:00:18.239 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000086, Batch[102/135], Train loss:0.962, Train acc: 0.242\n",
       "2022-11-08 07:00:32.537 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000137, Batch[103/135], Train loss:0.984, Train acc: 0.226\n",
       "2022-11-08 07:00:46.831 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000135, Batch[104/135], Train loss:0.989, Train acc: 0.220\n",
       "2022-11-08 07:01:01.151 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000003, Batch[105/135], Train loss:1.051, Train acc: 0.173\n",
       "2022-11-08 07:01:15.477 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000073, Batch[106/135], Train loss:0.947, Train acc: 0.252\n",
       "2022-11-08 07:01:29.760 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000007, Batch[107/135], Train loss:0.893, Train acc: 0.295\n",
       "2022-11-08 07:01:44.054 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000088, Batch[108/135], Train loss:0.854, Train acc: 0.329\n",
       "2022-11-08 07:01:58.402 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000147, Batch[109/135], Train loss:0.904, Train acc: 0.285\n",
       "2022-11-08 07:02:12.691 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000037, Batch[110/135], Train loss:1.033, Train acc: 0.184\n",
       "2022-11-08 07:02:27.005 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000032, Batch[111/135], Train loss:0.922, Train acc: 0.268\n",
       "2022-11-08 07:02:41.310 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000103, Batch[112/135], Train loss:0.933, Train acc: 0.258\n",
       "2022-11-08 07:02:55.643 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000149, Batch[113/135], Train loss:1.002, Train acc: 0.204\n",
       "2022-11-08 07:03:09.930 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000143, Batch[114/135], Train loss:0.982, Train acc: 0.219\n",
       "2022-11-08 07:03:24.264 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000136, Batch[115/135], Train loss:1.000, Train acc: 0.204\n",
       "2022-11-08 07:03:38.566 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000134, Batch[116/135], Train loss:1.002, Train acc: 0.202\n",
       "2022-11-08 07:03:52.870 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000036, Batch[117/135], Train loss:1.003, Train acc: 0.201\n",
       "2022-11-08 07:04:07.170 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000015, Batch[118/135], Train loss:0.981, Train acc: 0.215\n",
       "2022-11-08 07:04:21.465 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000144, Batch[119/135], Train loss:0.931, Train acc: 0.253\n",
       "2022-11-08 07:04:35.789 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000094, Batch[120/135], Train loss:0.993, Train acc: 0.205\n",
       "2022-11-08 07:04:50.102 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000011, Batch[121/135], Train loss:0.987, Train acc: 0.209\n",
       "2022-11-08 07:05:04.404 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000140, Batch[122/135], Train loss:0.963, Train acc: 0.226\n",
       "2022-11-08 07:05:18.705 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000087, Batch[123/135], Train loss:1.007, Train acc: 0.192\n",
       "2022-11-08 07:05:33.008 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000053, Batch[124/135], Train loss:0.950, Train acc: 0.239\n",
       "2022-11-08 07:05:47.330 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000048, Batch[125/135], Train loss:0.883, Train acc: 0.287\n",
       "2022-11-08 07:06:01.630 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000040, Batch[126/135], Train loss:0.832, Train acc: 0.332\n",
       "2022-11-08 07:06:15.943 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000062, Batch[127/135], Train loss:0.949, Train acc: 0.234\n",
       "2022-11-08 07:06:30.232 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000126, Batch[128/135], Train loss:0.982, Train acc: 0.207\n",
       "2022-11-08 07:06:44.542 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000117, Batch[129/135], Train loss:0.959, Train acc: 0.225\n",
       "2022-11-08 07:06:58.842 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000067, Batch[130/135], Train loss:0.861, Train acc: 0.303\n",
       "2022-11-08 07:07:13.150 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000068, Batch[131/135], Train loss:1.015, Train acc: 0.182\n",
       "2022-11-08 07:07:27.452 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000116, Batch[132/135], Train loss:0.871, Train acc: 0.291\n",
       "2022-11-08 07:07:41.748 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000020, Batch[133/135], Train loss:0.983, Train acc: 0.203\n",
       "2022-11-08 07:07:56.064 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000029, Batch[134/135], Train loss:0.898, Train acc: 0.267\n",
       "2022-11-08 07:08:10.393 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000017, Batch[135/135], Train loss:0.969, Train acc: 0.212\n",
       "2022-11-08 07:08:37.792 | INFO     | __main__:<cell line: 25>:70 - Epoch: 0, Train loss: 1.046, Epoch time=1958.828s, valid mean accuracy: 0.243, valid loss: 0.9425207376480103\n",
       "2022-11-08 07:08:52.047 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000010, Batch[1/135], Train loss:0.978, Train acc: 0.208\n",
       "2022-11-08 07:09:06.429 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000094, Batch[2/135], Train loss:1.048, Train acc: 0.198\n",
       "2022-11-08 07:09:20.809 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000063, Batch[3/135], Train loss:0.962, Train acc: 0.185\n",
       "2022-11-08 07:09:35.192 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000046, Batch[4/135], Train loss:0.884, Train acc: 0.228\n",
       "2022-11-08 07:09:49.587 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000022, Batch[5/135], Train loss:0.818, Train acc: 0.288\n",
       "2022-11-08 07:10:03.945 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000041, Batch[6/135], Train loss:0.596, Train acc: 0.437\n",
       "2022-11-08 07:10:18.318 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000098, Batch[7/135], Train loss:0.580, Train acc: 0.423\n",
       "2022-11-08 07:10:32.723 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000134, Batch[8/135], Train loss:0.451, Train acc: 0.503\n",
       "2022-11-08 07:10:47.110 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000065, Batch[9/135], Train loss:0.280, Train acc: 0.692\n",
       "2022-11-08 07:11:01.474 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000102, Batch[10/135], Train loss:0.715, Train acc: 0.367\n",
       "2022-11-08 07:11:15.869 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000026, Batch[11/135], Train loss:0.359, Train acc: 0.590\n",
       "2022-11-08 07:11:30.248 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000043, Batch[12/135], Train loss:0.475, Train acc: 0.469\n",
       "2022-11-08 07:11:44.643 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000027, Batch[13/135], Train loss:0.551, Train acc: 0.437\n",
       "2022-11-08 07:11:59.032 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000092, Batch[14/135], Train loss:0.548, Train acc: 0.437\n",
       "2022-11-08 07:12:13.437 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000067, Batch[15/135], Train loss:0.554, Train acc: 0.467\n",
       "2022-11-08 07:12:27.816 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000111, Batch[16/135], Train loss:0.494, Train acc: 0.505\n",
       "2022-11-08 07:12:42.215 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000117, Batch[17/135], Train loss:0.501, Train acc: 0.468\n",
       "2022-11-08 07:12:56.632 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000123, Batch[18/135], Train loss:0.519, Train acc: 0.476\n",
       "2022-11-08 07:13:11.005 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000132, Batch[19/135], Train loss:0.462, Train acc: 0.548\n",
       "2022-11-08 07:13:25.393 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000053, Batch[20/135], Train loss:0.408, Train acc: 0.552\n",
       "2022-11-08 07:13:39.795 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000084, Batch[21/135], Train loss:0.365, Train acc: 0.575\n",
       "2022-11-08 07:13:54.190 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000105, Batch[22/135], Train loss:0.305, Train acc: 0.630\n",
       "2022-11-08 07:14:08.557 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000068, Batch[23/135], Train loss:0.634, Train acc: 0.434\n",
       "2022-11-08 07:14:22.926 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000091, Batch[24/135], Train loss:0.287, Train acc: 0.682\n",
       "2022-11-08 07:14:37.341 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000093, Batch[25/135], Train loss:0.373, Train acc: 0.585\n",
       "2022-11-08 07:14:51.703 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000110, Batch[26/135], Train loss:0.315, Train acc: 0.610\n",
       "2022-11-08 07:15:06.076 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000039, Batch[27/135], Train loss:0.301, Train acc: 0.631\n",
       "2022-11-08 07:15:20.476 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000019, Batch[28/135], Train loss:0.445, Train acc: 0.537\n",
       "2022-11-08 07:15:34.857 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000074, Batch[29/135], Train loss:0.386, Train acc: 0.575\n",
       "2022-11-08 07:15:49.234 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000021, Batch[30/135], Train loss:0.323, Train acc: 0.620\n",
       "2022-11-08 07:16:03.603 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000003, Batch[31/135], Train loss:0.374, Train acc: 0.557\n",
       "2022-11-08 07:16:17.990 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000146, Batch[32/135], Train loss:0.374, Train acc: 0.604\n",
       "2022-11-08 07:16:32.382 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000028, Batch[33/135], Train loss:0.349, Train acc: 0.624\n",
       "2022-11-08 07:16:46.741 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000136, Batch[34/135], Train loss:0.343, Train acc: 0.594\n",
       "2022-11-08 07:17:01.115 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000141, Batch[35/135], Train loss:0.277, Train acc: 0.688\n",
       "2022-11-08 07:17:15.484 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000149, Batch[36/135], Train loss:0.273, Train acc: 0.669\n",
       "2022-11-08 07:17:29.862 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000106, Batch[37/135], Train loss:0.310, Train acc: 0.652\n",
       "2022-11-08 07:17:44.236 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000042, Batch[38/135], Train loss:0.360, Train acc: 0.611\n",
       "2022-11-08 07:17:58.614 | INFO     | __main__:<cell line: 25>:60 -\n",
       "\n",
       "*** WARNING: max output size exceeded, skipping output. ***\n",
       "\n",
       "ss:0.031, Train acc: 0.967\n",
       "2022-11-09 01:44:07.757 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000001, Batch[19/135], Train loss:0.031, Train acc: 0.962\n",
       "2022-11-09 01:44:22.059 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000035, Batch[20/135], Train loss:0.031, Train acc: 0.968\n",
       "2022-11-09 01:44:36.385 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000101, Batch[21/135], Train loss:0.043, Train acc: 0.942\n",
       "2022-11-09 01:44:50.714 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000052, Batch[22/135], Train loss:0.027, Train acc: 0.970\n",
       "2022-11-09 01:45:04.998 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000057, Batch[23/135], Train loss:0.032, Train acc: 0.968\n",
       "2022-11-09 01:45:19.295 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000149, Batch[24/135], Train loss:0.049, Train acc: 0.933\n",
       "2022-11-09 01:45:33.652 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000086, Batch[25/135], Train loss:0.032, Train acc: 0.957\n",
       "2022-11-09 01:45:47.945 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000127, Batch[26/135], Train loss:0.035, Train acc: 0.956\n",
       "2022-11-09 01:46:02.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000137, Batch[27/135], Train loss:0.035, Train acc: 0.956\n",
       "2022-11-09 01:46:16.542 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000095, Batch[28/135], Train loss:0.044, Train acc: 0.942\n",
       "2022-11-09 01:46:30.892 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000047, Batch[29/135], Train loss:0.051, Train acc: 0.951\n",
       "2022-11-09 01:46:45.179 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000106, Batch[30/135], Train loss:0.028, Train acc: 0.968\n",
       "2022-11-09 01:46:59.472 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000064, Batch[31/135], Train loss:0.032, Train acc: 0.967\n",
       "2022-11-09 01:47:13.784 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000143, Batch[32/135], Train loss:0.049, Train acc: 0.945\n",
       "2022-11-09 01:47:28.129 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000040, Batch[33/135], Train loss:0.042, Train acc: 0.967\n",
       "2022-11-09 01:47:42.434 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000088, Batch[34/135], Train loss:0.030, Train acc: 0.968\n",
       "2022-11-09 01:47:56.753 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000030, Batch[35/135], Train loss:0.030, Train acc: 0.960\n",
       "2022-11-09 01:48:11.062 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000082, Batch[36/135], Train loss:0.035, Train acc: 0.953\n",
       "2022-11-09 01:48:25.392 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000063, Batch[37/135], Train loss:0.032, Train acc: 0.964\n",
       "2022-11-09 01:48:39.699 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000103, Batch[38/135], Train loss:0.037, Train acc: 0.950\n",
       "2022-11-09 01:48:54.025 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000139, Batch[39/135], Train loss:0.029, Train acc: 0.967\n",
       "2022-11-09 01:49:08.317 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000043, Batch[40/135], Train loss:0.035, Train acc: 0.966\n",
       "2022-11-09 01:49:22.644 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000067, Batch[41/135], Train loss:0.025, Train acc: 0.971\n",
       "2022-11-09 01:49:36.934 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000068, Batch[42/135], Train loss:0.042, Train acc: 0.949\n",
       "2022-11-09 01:49:51.261 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000110, Batch[43/135], Train loss:0.041, Train acc: 0.943\n",
       "2022-11-09 01:50:05.552 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000108, Batch[44/135], Train loss:0.041, Train acc: 0.950\n",
       "2022-11-09 01:50:19.887 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000039, Batch[45/135], Train loss:0.033, Train acc: 0.956\n",
       "2022-11-09 01:50:34.206 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000141, Batch[46/135], Train loss:0.037, Train acc: 0.960\n",
       "2022-11-09 01:50:48.502 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000083, Batch[47/135], Train loss:0.033, Train acc: 0.964\n",
       "2022-11-09 01:51:02.806 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000116, Batch[48/135], Train loss:0.037, Train acc: 0.960\n",
       "2022-11-09 01:51:17.123 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000062, Batch[49/135], Train loss:0.037, Train acc: 0.965\n",
       "2022-11-09 01:51:31.443 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000059, Batch[50/135], Train loss:0.052, Train acc: 0.933\n",
       "2022-11-09 01:51:45.744 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000074, Batch[51/135], Train loss:0.037, Train acc: 0.962\n",
       "2022-11-09 01:52:00.039 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000084, Batch[52/135], Train loss:0.034, Train acc: 0.954\n",
       "2022-11-09 01:52:14.373 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000022, Batch[53/135], Train loss:0.034, Train acc: 0.970\n",
       "2022-11-09 01:52:28.668 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000015, Batch[54/135], Train loss:0.041, Train acc: 0.962\n",
       "2022-11-09 01:52:42.980 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000031, Batch[55/135], Train loss:0.030, Train acc: 0.964\n",
       "2022-11-09 01:52:57.274 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000122, Batch[56/135], Train loss:0.032, Train acc: 0.960\n",
       "2022-11-09 01:53:11.615 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000090, Batch[57/135], Train loss:0.046, Train acc: 0.940\n",
       "2022-11-09 01:53:25.933 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000027, Batch[58/135], Train loss:0.039, Train acc: 0.952\n",
       "2022-11-09 01:53:40.223 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000128, Batch[59/135], Train loss:0.044, Train acc: 0.942\n",
       "2022-11-09 01:53:54.548 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000079, Batch[60/135], Train loss:0.048, Train acc: 0.935\n",
       "2022-11-09 01:54:08.850 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000019, Batch[61/135], Train loss:0.027, Train acc: 0.969\n",
       "2022-11-09 01:54:23.149 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000125, Batch[62/135], Train loss:0.034, Train acc: 0.964\n",
       "2022-11-09 01:54:37.435 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000051, Batch[63/135], Train loss:0.028, Train acc: 0.966\n",
       "2022-11-09 01:54:51.735 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000072, Batch[64/135], Train loss:0.038, Train acc: 0.961\n",
       "2022-11-09 01:55:06.077 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000085, Batch[65/135], Train loss:0.044, Train acc: 0.956\n",
       "2022-11-09 01:55:20.362 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000126, Batch[66/135], Train loss:0.041, Train acc: 0.946\n",
       "2022-11-09 01:55:34.687 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000146, Batch[67/135], Train loss:0.028, Train acc: 0.970\n",
       "2022-11-09 01:55:48.992 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000029, Batch[68/135], Train loss:0.027, Train acc: 0.971\n",
       "2022-11-09 01:56:03.328 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000123, Batch[69/135], Train loss:0.034, Train acc: 0.959\n",
       "2022-11-09 01:56:17.622 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000023, Batch[70/135], Train loss:0.035, Train acc: 0.967\n",
       "2022-11-09 01:56:31.940 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000129, Batch[71/135], Train loss:0.043, Train acc: 0.954\n",
       "2022-11-09 01:56:46.247 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000118, Batch[72/135], Train loss:0.042, Train acc: 0.943\n",
       "2022-11-09 01:57:00.581 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000008, Batch[73/135], Train loss:0.026, Train acc: 0.976\n",
       "2022-11-09 01:57:14.905 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000041, Batch[74/135], Train loss:0.025, Train acc: 0.972\n",
       "2022-11-09 01:57:29.209 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000117, Batch[75/135], Train loss:0.056, Train acc: 0.923\n",
       "2022-11-09 01:57:43.502 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000132, Batch[76/135], Train loss:0.038, Train acc: 0.954\n",
       "2022-11-09 01:57:57.791 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000105, Batch[77/135], Train loss:0.045, Train acc: 0.937\n",
       "2022-11-09 01:58:12.118 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000124, Batch[78/135], Train loss:0.035, Train acc: 0.955\n",
       "2022-11-09 01:58:26.403 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000138, Batch[79/135], Train loss:0.042, Train acc: 0.943\n",
       "2022-11-09 01:58:40.691 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000053, Batch[80/135], Train loss:0.043, Train acc: 0.962\n",
       "2022-11-09 01:58:55.030 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000130, Batch[81/135], Train loss:0.035, Train acc: 0.962\n",
       "2022-11-09 01:59:09.315 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000026, Batch[82/135], Train loss:0.035, Train acc: 0.972\n",
       "2022-11-09 01:59:23.608 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000020, Batch[83/135], Train loss:0.037, Train acc: 0.968\n",
       "2022-11-09 01:59:37.885 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000093, Batch[84/135], Train loss:0.032, Train acc: 0.964\n",
       "2022-11-09 01:59:52.225 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000098, Batch[85/135], Train loss:0.037, Train acc: 0.952\n",
       "2022-11-09 02:00:06.524 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000080, Batch[86/135], Train loss:0.038, Train acc: 0.952\n",
       "2022-11-09 02:00:20.810 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000135, Batch[87/135], Train loss:0.032, Train acc: 0.958\n",
       "2022-11-09 02:00:35.156 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000081, Batch[88/135], Train loss:0.043, Train acc: 0.943\n",
       "2022-11-09 02:00:49.476 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000004, Batch[89/135], Train loss:0.052, Train acc: 0.931\n",
       "2022-11-09 02:01:03.769 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000024, Batch[90/135], Train loss:0.047, Train acc: 0.961\n",
       "2022-11-09 02:01:18.057 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000042, Batch[91/135], Train loss:0.030, Train acc: 0.971\n",
       "2022-11-09 02:01:32.383 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000104, Batch[92/135], Train loss:0.035, Train acc: 0.962\n",
       "2022-11-09 02:01:46.703 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000070, Batch[93/135], Train loss:0.034, Train acc: 0.962\n",
       "2022-11-09 02:02:00.982 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000112, Batch[94/135], Train loss:0.038, Train acc: 0.954\n",
       "2022-11-09 02:02:15.335 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000102, Batch[95/135], Train loss:0.039, Train acc: 0.951\n",
       "2022-11-09 02:02:29.632 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000055, Batch[96/135], Train loss:0.025, Train acc: 0.969\n",
       "2022-11-09 02:02:43.963 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000133, Batch[97/135], Train loss:0.042, Train acc: 0.949\n",
       "2022-11-09 02:02:58.286 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000009, Batch[98/135], Train loss:0.038, Train acc: 0.969\n",
       "2022-11-09 02:03:12.605 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000121, Batch[99/135], Train loss:0.029, Train acc: 0.968\n",
       "2022-11-09 02:03:26.895 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000007, Batch[100/135], Train loss:0.025, Train acc: 0.978\n",
       "2022-11-09 02:03:41.188 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000013, Batch[101/135], Train loss:0.026, Train acc: 0.969\n",
       "2022-11-09 02:03:55.502 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000028, Batch[102/135], Train loss:0.023, Train acc: 0.975\n",
       "2022-11-09 02:04:09.797 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000131, Batch[103/135], Train loss:0.038, Train acc: 0.946\n",
       "2022-11-09 02:04:24.091 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000111, Batch[104/135], Train loss:0.039, Train acc: 0.949\n",
       "2022-11-09 02:04:38.430 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000005, Batch[105/135], Train loss:0.031, Train acc: 0.964\n",
       "2022-11-09 02:04:52.741 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000089, Batch[106/135], Train loss:0.028, Train acc: 0.964\n",
       "2022-11-09 02:05:07.038 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000078, Batch[107/135], Train loss:0.045, Train acc: 0.939\n",
       "2022-11-09 02:05:21.358 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000145, Batch[108/135], Train loss:0.042, Train acc: 0.956\n",
       "2022-11-09 02:05:35.712 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000048, Batch[109/135], Train loss:0.026, Train acc: 0.973\n",
       "2022-11-09 02:05:50.000 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000087, Batch[110/135], Train loss:0.037, Train acc: 0.950\n",
       "2022-11-09 02:06:04.300 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000148, Batch[111/135], Train loss:0.034, Train acc: 0.966\n",
       "2022-11-09 02:06:18.584 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000003, Batch[112/135], Train loss:0.031, Train acc: 0.963\n",
       "2022-11-09 02:06:32.944 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000034, Batch[113/135], Train loss:0.032, Train acc: 0.969\n",
       "2022-11-09 02:06:47.229 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000109, Batch[114/135], Train loss:0.036, Train acc: 0.955\n",
       "2022-11-09 02:07:01.565 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000091, Batch[115/135], Train loss:0.037, Train acc: 0.957\n",
       "2022-11-09 02:07:15.865 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000014, Batch[116/135], Train loss:0.058, Train acc: 0.945\n",
       "2022-11-09 02:07:30.194 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000113, Batch[117/135], Train loss:0.027, Train acc: 0.968\n",
       "2022-11-09 02:07:44.507 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000142, Batch[118/135], Train loss:0.033, Train acc: 0.960\n",
       "2022-11-09 02:07:58.811 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000012, Batch[119/135], Train loss:0.050, Train acc: 0.959\n",
       "2022-11-09 02:08:13.126 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000066, Batch[120/135], Train loss:0.028, Train acc: 0.970\n",
       "2022-11-09 02:08:27.430 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000073, Batch[121/135], Train loss:0.032, Train acc: 0.959\n",
       "2022-11-09 02:08:41.731 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000038, Batch[122/135], Train loss:0.035, Train acc: 0.954\n",
       "2022-11-09 02:08:56.054 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000147, Batch[123/135], Train loss:0.029, Train acc: 0.963\n",
       "2022-11-09 02:09:10.352 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000006, Batch[124/135], Train loss:0.035, Train acc: 0.955\n",
       "2022-11-09 02:09:24.702 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000065, Batch[125/135], Train loss:0.034, Train acc: 0.963\n",
       "2022-11-09 02:09:38.995 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000119, Batch[126/135], Train loss:0.027, Train acc: 0.967\n",
       "2022-11-09 02:09:53.315 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000134, Batch[127/135], Train loss:0.043, Train acc: 0.940\n",
       "2022-11-09 02:10:07.628 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000037, Batch[128/135], Train loss:0.029, Train acc: 0.969\n",
       "2022-11-09 02:10:21.957 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000092, Batch[129/135], Train loss:0.043, Train acc: 0.948\n",
       "2022-11-09 02:10:36.250 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000025, Batch[130/135], Train loss:0.040, Train acc: 0.963\n",
       "2022-11-09 02:10:50.553 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000058, Batch[131/135], Train loss:0.030, Train acc: 0.972\n",
       "2022-11-09 02:11:04.845 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000136, Batch[132/135], Train loss:0.043, Train acc: 0.944\n",
       "2022-11-09 02:11:19.178 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000010, Batch[133/135], Train loss:0.036, Train acc: 0.968\n",
       "2022-11-09 02:11:33.492 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000107, Batch[134/135], Train loss:0.036, Train acc: 0.957\n",
       "2022-11-09 02:11:47.803 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000011, Batch[135/135], Train loss:0.034, Train acc: 0.965\n",
       "2022-11-09 02:12:15.162 | INFO     | __main__:<cell line: 25>:70 - Epoch: 35, Train loss: 0.036, Epoch time=1958.896s, valid mean accuracy: 0.955, valid loss: 0.0372004508972168\n",
       "2022-11-09 02:12:29.362 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000073, Batch[1/135], Train loss:0.034, Train acc: 0.957\n",
       "2022-11-09 02:12:43.652 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000127, Batch[2/135], Train loss:0.037, Train acc: 0.952\n",
       "2022-11-09 02:12:57.958 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000086, Batch[3/135], Train loss:0.036, Train acc: 0.951\n",
       "2022-11-09 02:13:12.267 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000122, Batch[4/135], Train loss:0.033, Train acc: 0.959\n",
       "2022-11-09 02:13:26.618 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000144, Batch[5/135], Train loss:0.039, Train acc: 0.949\n",
       "2022-11-09 02:13:40.900 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000068, Batch[6/135], Train loss:0.043, Train acc: 0.946\n",
       "2022-11-09 02:13:55.196 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000028, Batch[7/135], Train loss:0.027, Train acc: 0.976\n",
       "2022-11-09 02:14:09.526 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000006, Batch[8/135], Train loss:0.033, Train acc: 0.961\n",
       "2022-11-09 02:14:23.848 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000005, Batch[9/135], Train loss:0.031, Train acc: 0.969\n",
       "2022-11-09 02:14:38.131 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000113, Batch[10/135], Train loss:0.029, Train acc: 0.969\n",
       "2022-11-09 02:14:52.435 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000026, Batch[11/135], Train loss:0.026, Train acc: 0.976\n",
       "2022-11-09 02:15:06.730 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000123, Batch[12/135], Train loss:0.034, Train acc: 0.959\n",
       "2022-11-09 02:15:21.049 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000009, Batch[13/135], Train loss:0.041, Train acc: 0.968\n",
       "2022-11-09 02:15:35.350 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000030, Batch[14/135], Train loss:0.034, Train acc: 0.952\n",
       "2022-11-09 02:15:49.687 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000084, Batch[15/135], Train loss:0.039, Train acc: 0.945\n",
       "2022-11-09 02:16:03.977 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000105, Batch[16/135], Train loss:0.045, Train acc: 0.936\n",
       "2022-11-09 02:16:18.303 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000087, Batch[17/135], Train loss:0.039, Train acc: 0.945\n",
       "2022-11-09 02:16:32.646 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000129, Batch[18/135], Train loss:0.042, Train acc: 0.952\n",
       "2022-11-09 02:16:46.938 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000094, Batch[19/135], Train loss:0.039, Train acc: 0.950\n",
       "2022-11-09 02:17:01.243 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000082, Batch[20/135], Train loss:0.035, Train acc: 0.956\n",
       "2022-11-09 02:17:15.569 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000146, Batch[21/135], Train loss:0.033, Train acc: 0.969\n",
       "2022-11-09 02:17:29.888 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000149, Batch[22/135], Train loss:0.045, Train acc: 0.943\n",
       "2022-11-09 02:17:44.183 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000065, Batch[23/135], Train loss:0.040, Train acc: 0.965\n",
       "2022-11-09 02:17:58.478 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000101, Batch[24/135], Train loss:0.041, Train acc: 0.952\n",
       "2022-11-09 02:18:12.814 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000115, Batch[25/135], Train loss:0.034, Train acc: 0.961\n",
       "2022-11-09 02:18:27.106 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000139, Batch[26/135], Train loss:0.031, Train acc: 0.969\n",
       "2022-11-09 02:18:41.397 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000032, Batch[27/135], Train loss:0.024, Train acc: 0.978\n",
       "2022-11-09 02:18:55.703 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000074, Batch[28/135], Train loss:0.040, Train acc: 0.962\n",
       "2022-11-09 02:19:10.041 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000112, Batch[29/135], Train loss:0.036, Train acc: 0.957\n",
       "2022-11-09 02:19:24.339 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000003, Batch[30/135], Train loss:0.031, Train acc: 0.961\n",
       "2022-11-09 02:19:38.630 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000053, Batch[31/135], Train loss:0.036, Train acc: 0.962\n",
       "2022-11-09 02:19:52.933 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000089, Batch[32/135], Train loss:0.027, Train acc: 0.966\n",
       "2022-11-09 02:20:07.257 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000042, Batch[33/135], Train loss:0.028, Train acc: 0.969\n",
       "2022-11-09 02:20:21.541 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000017, Batch[34/135], Train loss:0.030, Train acc: 0.967\n",
       "2022-11-09 02:20:35.841 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000041, Batch[35/135], Train loss:0.026, Train acc: 0.971\n",
       "2022-11-09 02:20:50.157 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000104, Batch[36/135], Train loss:0.033, Train acc: 0.961\n",
       "2022-11-09 02:21:04.479 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000021, Batch[37/135], Train loss:0.027, Train acc: 0.969\n",
       "2022-11-09 02:21:18.783 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000024, Batch[38/135], Train loss:0.034, Train acc: 0.968\n",
       "2022-11-09 02:21:33.079 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000079, Batch[39/135], Train loss:0.050, Train acc: 0.932\n",
       "2022-11-09 02:21:47.378 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000107, Batch[40/135], Train loss:0.038, Train acc: 0.953\n",
       "2022-11-09 02:22:01.673 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000049, Batch[41/135], Train loss:0.029, Train acc: 0.965\n",
       "2022-11-09 02:22:15.968 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000116, Batch[42/135], Train loss:0.040, Train acc: 0.953\n",
       "2022-11-09 02:22:30.284 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000119, Batch[43/135], Train loss:0.030, Train acc: 0.965\n",
       "2022-11-09 02:22:44.585 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000036, Batch[44/135], Train loss:0.042, Train acc: 0.947\n",
       "2022-11-09 02:22:58.933 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000052, Batch[45/135], Train loss:0.027, Train acc: 0.971\n",
       "2022-11-09 02:23:13.237 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000043, Batch[46/135], Train loss:0.033, Train acc: 0.969\n",
       "2022-11-09 02:23:27.522 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000111, Batch[47/135], Train loss:0.034, Train acc: 0.958\n",
       "2022-11-09 02:23:41.815 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000106, Batch[48/135], Train loss:0.025, Train acc: 0.970\n",
       "2022-11-09 02:23:56.126 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000055, Batch[49/135], Train loss:0.025, Train acc: 0.974\n",
       "2022-11-09 02:24:10.435 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000121, Batch[50/135], Train loss:0.031, Train acc: 0.969\n",
       "2022-11-09 02:24:24.727 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000010, Batch[51/135], Train loss:0.045, Train acc: 0.964\n",
       "2022-11-09 02:24:39.014 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000098, Batch[52/135], Train loss:0.037, Train acc: 0.952\n",
       "2022-11-09 02:24:53.344 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000142, Batch[53/135], Train loss:0.034, Train acc: 0.959\n",
       "2022-11-09 02:25:07.634 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000117, Batch[54/135], Train loss:0.050, Train acc: 0.932\n",
       "2022-11-09 02:25:21.931 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000148, Batch[55/135], Train loss:0.031, Train acc: 0.968\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "2022-11-08 06:36:13.012 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000001, Batch[1/135], Train loss:1.366, Train acc: 0.071\n2022-11-08 06:36:27.310 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000055, Batch[2/135], Train loss:1.266, Train acc: 0.107\n2022-11-08 06:36:41.587 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000006, Batch[3/135], Train loss:1.206, Train acc: 0.136\n2022-11-08 06:36:55.910 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000019, Batch[4/135], Train loss:1.127, Train acc: 0.195\n2022-11-08 06:37:10.230 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000065, Batch[5/135], Train loss:1.122, Train acc: 0.198\n2022-11-08 06:37:24.526 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000038, Batch[6/135], Train loss:1.214, Train acc: 0.128\n2022-11-08 06:37:38.816 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000141, Batch[7/135], Train loss:1.193, Train acc: 0.143\n2022-11-08 06:37:53.164 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000025, Batch[8/135], Train loss:1.210, Train acc: 0.130\n2022-11-08 06:38:07.468 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000041, Batch[9/135], Train loss:1.103, Train acc: 0.209\n2022-11-08 06:38:21.764 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000026, Batch[10/135], Train loss:1.159, Train acc: 0.165\n2022-11-08 06:38:36.076 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000027, Batch[11/135], Train loss:1.166, Train acc: 0.159\n2022-11-08 06:38:50.364 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000021, Batch[12/135], Train loss:1.161, Train acc: 0.162\n2022-11-08 06:39:04.663 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000082, Batch[13/135], Train loss:1.140, Train acc: 0.177\n2022-11-08 06:39:18.957 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000023, Batch[14/135], Train loss:1.195, Train acc: 0.136\n2022-11-08 06:39:33.288 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000049, Batch[15/135], Train loss:1.104, Train acc: 0.203\n2022-11-08 06:39:47.583 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000058, Batch[16/135], Train loss:1.103, Train acc: 0.203\n2022-11-08 06:40:01.897 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000146, Batch[17/135], Train loss:1.070, Train acc: 0.229\n2022-11-08 06:40:16.232 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000085, Batch[18/135], Train loss:1.170, Train acc: 0.151\n2022-11-08 06:40:30.529 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000102, Batch[19/135], Train loss:1.130, Train acc: 0.180\n2022-11-08 06:40:44.831 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000108, Batch[20/135], Train loss:1.184, Train acc: 0.140\n2022-11-08 06:40:59.140 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000093, Batch[21/135], Train loss:1.115, Train acc: 0.190\n2022-11-08 06:41:13.475 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000004, Batch[22/135], Train loss:1.163, Train acc: 0.154\n2022-11-08 06:41:27.774 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000014, Batch[23/135], Train loss:1.127, Train acc: 0.179\n2022-11-08 06:41:42.084 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000045, Batch[24/135], Train loss:1.170, Train acc: 0.146\n2022-11-08 06:41:56.404 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000059, Batch[25/135], Train loss:1.192, Train acc: 0.131\n2022-11-08 06:42:10.707 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000043, Batch[26/135], Train loss:1.194, Train acc: 0.129\n2022-11-08 06:42:25.001 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000090, Batch[27/135], Train loss:1.142, Train acc: 0.165\n2022-11-08 06:42:39.322 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000031, Batch[28/135], Train loss:1.163, Train acc: 0.149\n2022-11-08 06:42:53.650 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000128, Batch[29/135], Train loss:1.176, Train acc: 0.139\n2022-11-08 06:43:07.950 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000064, Batch[30/135], Train loss:1.118, Train acc: 0.180\n2022-11-08 06:43:22.243 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000079, Batch[31/135], Train loss:1.107, Train acc: 0.187\n2022-11-08 06:43:36.539 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000022, Batch[32/135], Train loss:1.130, Train acc: 0.169\n2022-11-08 06:43:50.872 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000018, Batch[33/135], Train loss:1.088, Train acc: 0.199\n2022-11-08 06:44:05.158 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000127, Batch[34/135], Train loss:1.078, Train acc: 0.208\n2022-11-08 06:44:19.476 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000005, Batch[35/135], Train loss:1.118, Train acc: 0.176\n2022-11-08 06:44:33.788 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000034, Batch[36/135], Train loss:1.151, Train acc: 0.152\n2022-11-08 06:44:48.094 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000125, Batch[37/135], Train loss:1.042, Train acc: 0.234\n2022-11-08 06:45:02.401 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000123, Batch[38/135], Train loss:1.073, Train acc: 0.207\n2022-11-08 06:45:16.719 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000047, Batch[39/135], Train loss:1.140, Train acc: 0.157\n2022-11-08 06:45:31.017 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000111, Batch[40/135], Train loss:1.043, Train acc: 0.230\n2022-11-08 06:45:45.318 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000039, Batch[41/135], Train loss:1.158, Train acc: 0.144\n2022-11-08 06:45:59.615 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000009, Batch[42/135], Train loss:1.015, Train acc: 0.252\n2022-11-08 06:46:13.925 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000089, Batch[43/135], Train loss:1.049, Train acc: 0.222\n2022-11-08 06:46:28.229 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000124, Batch[44/135], Train loss:1.024, Train acc: 0.242\n2022-11-08 06:46:42.538 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000122, Batch[45/135], Train loss:1.097, Train acc: 0.185\n2022-11-08 06:46:56.871 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000115, Batch[46/135], Train loss:1.083, Train acc: 0.193\n2022-11-08 06:47:11.159 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000030, Batch[47/135], Train loss:1.134, Train acc: 0.156\n2022-11-08 06:47:25.478 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000121, Batch[48/135], Train loss:1.031, Train acc: 0.233\n2022-11-08 06:47:39.790 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000035, Batch[49/135], Train loss:1.064, Train acc: 0.206\n2022-11-08 06:47:54.114 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000069, Batch[50/135], Train loss:1.068, Train acc: 0.202\n2022-11-08 06:48:08.425 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000109, Batch[51/135], Train loss:1.108, Train acc: 0.171\n2022-11-08 06:48:22.719 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000095, Batch[52/135], Train loss:1.126, Train acc: 0.158\n2022-11-08 06:48:37.067 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000061, Batch[53/135], Train loss:1.081, Train acc: 0.191\n2022-11-08 06:48:51.363 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000148, Batch[54/135], Train loss:1.027, Train acc: 0.230\n2022-11-08 06:49:05.679 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000076, Batch[55/135], Train loss:1.015, Train acc: 0.240\n2022-11-08 06:49:19.977 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000138, Batch[56/135], Train loss:1.134, Train acc: 0.150\n2022-11-08 06:49:34.310 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000057, Batch[57/135], Train loss:1.079, Train acc: 0.189\n2022-11-08 06:49:48.600 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000070, Batch[58/135], Train loss:1.073, Train acc: 0.192\n2022-11-08 06:50:02.910 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000083, Batch[59/135], Train loss:0.997, Train acc: 0.251\n2022-11-08 06:50:17.216 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000131, Batch[60/135], Train loss:1.143, Train acc: 0.140\n2022-11-08 06:50:31.533 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000081, Batch[61/135], Train loss:1.015, Train acc: 0.234\n2022-11-08 06:50:45.844 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000118, Batch[62/135], Train loss:1.081, Train acc: 0.183\n2022-11-08 06:51:00.155 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000105, Batch[63/135], Train loss:1.107, Train acc: 0.163\n2022-11-08 06:51:14.455 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000104, Batch[64/135], Train loss:0.993, Train acc: 0.250\n2022-11-08 06:51:28.792 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000107, Batch[65/135], Train loss:0.976, Train acc: 0.263\n2022-11-08 06:51:43.089 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000098, Batch[66/135], Train loss:1.044, Train acc: 0.207\n2022-11-08 06:51:57.408 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000139, Batch[67/135], Train loss:0.995, Train acc: 0.244\n2022-11-08 06:52:11.695 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000130, Batch[68/135], Train loss:0.980, Train acc: 0.256\n2022-11-08 06:52:25.995 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000129, Batch[69/135], Train loss:1.136, Train acc: 0.140\n2022-11-08 06:52:40.300 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000078, Batch[70/135], Train loss:1.074, Train acc: 0.184\n2022-11-08 06:52:54.604 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000052, Batch[71/135], Train loss:1.028, Train acc: 0.216\n2022-11-08 06:53:08.934 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000051, Batch[72/135], Train loss:1.115, Train acc: 0.153\n2022-11-08 06:53:23.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000113, Batch[73/135], Train loss:1.004, Train acc: 0.234\n2022-11-08 06:53:37.569 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000119, Batch[74/135], Train loss:0.967, Train acc: 0.263\n2022-11-08 06:53:51.875 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000110, Batch[75/135], Train loss:1.123, Train acc: 0.144\n2022-11-08 06:54:06.168 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000101, Batch[76/135], Train loss:1.093, Train acc: 0.163\n2022-11-08 06:54:20.484 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000063, Batch[77/135], Train loss:1.090, Train acc: 0.166\n2022-11-08 06:54:34.799 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000145, Batch[78/135], Train loss:1.102, Train acc: 0.157\n2022-11-08 06:54:49.097 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000099, Batch[79/135], Train loss:1.067, Train acc: 0.181\n2022-11-08 06:55:03.390 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000072, Batch[80/135], Train loss:1.028, Train acc: 0.208\n2022-11-08 06:55:17.723 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000080, Batch[81/135], Train loss:0.993, Train acc: 0.234\n2022-11-08 06:55:32.041 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000112, Batch[82/135], Train loss:1.028, Train acc: 0.208\n2022-11-08 06:55:46.332 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000074, Batch[83/135], Train loss:0.993, Train acc: 0.233\n2022-11-08 06:56:00.626 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000042, Batch[84/135], Train loss:0.944, Train acc: 0.273\n2022-11-08 06:56:14.996 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000066, Batch[85/135], Train loss:0.926, Train acc: 0.288\n2022-11-08 06:56:29.287 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000056, Batch[86/135], Train loss:0.968, Train acc: 0.252\n2022-11-08 06:56:43.589 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000013, Batch[87/135], Train loss:1.039, Train acc: 0.195\n2022-11-08 06:56:57.893 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000091, Batch[88/135], Train loss:0.950, Train acc: 0.264\n2022-11-08 06:57:12.229 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000046, Batch[89/135], Train loss:1.053, Train acc: 0.184\n2022-11-08 06:57:26.512 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000106, Batch[90/135], Train loss:0.938, Train acc: 0.272\n2022-11-08 06:57:40.803 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000132, Batch[91/135], Train loss:0.892, Train acc: 0.310\n2022-11-08 06:57:55.140 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000092, Batch[92/135], Train loss:1.027, Train acc: 0.203\n2022-11-08 06:58:09.438 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000024, Batch[93/135], Train loss:0.978, Train acc: 0.237\n2022-11-08 06:58:23.746 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000142, Batch[94/135], Train loss:0.917, Train acc: 0.285\n2022-11-08 06:58:38.087 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000008, Batch[95/135], Train loss:0.886, Train acc: 0.311\n2022-11-08 06:58:52.388 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000012, Batch[96/135], Train loss:0.993, Train acc: 0.224\n2022-11-08 06:59:06.700 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000010, Batch[97/135], Train loss:1.005, Train acc: 0.214\n2022-11-08 06:59:20.997 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000028, Batch[98/135], Train loss:0.902, Train acc: 0.294\n2022-11-08 06:59:35.305 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000133, Batch[99/135], Train loss:0.918, Train acc: 0.285\n2022-11-08 06:59:49.603 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000084, Batch[100/135], Train loss:1.029, Train acc: 0.193\n2022-11-08 07:00:03.902 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000097, Batch[101/135], Train loss:0.917, Train acc: 0.280\n2022-11-08 07:00:18.239 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000086, Batch[102/135], Train loss:0.962, Train acc: 0.242\n2022-11-08 07:00:32.537 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000137, Batch[103/135], Train loss:0.984, Train acc: 0.226\n2022-11-08 07:00:46.831 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000135, Batch[104/135], Train loss:0.989, Train acc: 0.220\n2022-11-08 07:01:01.151 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000003, Batch[105/135], Train loss:1.051, Train acc: 0.173\n2022-11-08 07:01:15.477 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000073, Batch[106/135], Train loss:0.947, Train acc: 0.252\n2022-11-08 07:01:29.760 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000007, Batch[107/135], Train loss:0.893, Train acc: 0.295\n2022-11-08 07:01:44.054 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000088, Batch[108/135], Train loss:0.854, Train acc: 0.329\n2022-11-08 07:01:58.402 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000147, Batch[109/135], Train loss:0.904, Train acc: 0.285\n2022-11-08 07:02:12.691 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000037, Batch[110/135], Train loss:1.033, Train acc: 0.184\n2022-11-08 07:02:27.005 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000032, Batch[111/135], Train loss:0.922, Train acc: 0.268\n2022-11-08 07:02:41.310 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000103, Batch[112/135], Train loss:0.933, Train acc: 0.258\n2022-11-08 07:02:55.643 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000149, Batch[113/135], Train loss:1.002, Train acc: 0.204\n2022-11-08 07:03:09.930 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000143, Batch[114/135], Train loss:0.982, Train acc: 0.219\n2022-11-08 07:03:24.264 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000136, Batch[115/135], Train loss:1.000, Train acc: 0.204\n2022-11-08 07:03:38.566 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000134, Batch[116/135], Train loss:1.002, Train acc: 0.202\n2022-11-08 07:03:52.870 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000036, Batch[117/135], Train loss:1.003, Train acc: 0.201\n2022-11-08 07:04:07.170 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000015, Batch[118/135], Train loss:0.981, Train acc: 0.215\n2022-11-08 07:04:21.465 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000144, Batch[119/135], Train loss:0.931, Train acc: 0.253\n2022-11-08 07:04:35.789 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000094, Batch[120/135], Train loss:0.993, Train acc: 0.205\n2022-11-08 07:04:50.102 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000011, Batch[121/135], Train loss:0.987, Train acc: 0.209\n2022-11-08 07:05:04.404 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000140, Batch[122/135], Train loss:0.963, Train acc: 0.226\n2022-11-08 07:05:18.705 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000087, Batch[123/135], Train loss:1.007, Train acc: 0.192\n2022-11-08 07:05:33.008 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000053, Batch[124/135], Train loss:0.950, Train acc: 0.239\n2022-11-08 07:05:47.330 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000048, Batch[125/135], Train loss:0.883, Train acc: 0.287\n2022-11-08 07:06:01.630 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000040, Batch[126/135], Train loss:0.832, Train acc: 0.332\n2022-11-08 07:06:15.943 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000062, Batch[127/135], Train loss:0.949, Train acc: 0.234\n2022-11-08 07:06:30.232 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000126, Batch[128/135], Train loss:0.982, Train acc: 0.207\n2022-11-08 07:06:44.542 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000117, Batch[129/135], Train loss:0.959, Train acc: 0.225\n2022-11-08 07:06:58.842 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000067, Batch[130/135], Train loss:0.861, Train acc: 0.303\n2022-11-08 07:07:13.150 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000068, Batch[131/135], Train loss:1.015, Train acc: 0.182\n2022-11-08 07:07:27.452 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000116, Batch[132/135], Train loss:0.871, Train acc: 0.291\n2022-11-08 07:07:41.748 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000020, Batch[133/135], Train loss:0.983, Train acc: 0.203\n2022-11-08 07:07:56.064 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000029, Batch[134/135], Train loss:0.898, Train acc: 0.267\n2022-11-08 07:08:10.393 | INFO     | __main__:<cell line: 25>:60 - Epoch: 0, idx:BJ00000017, Batch[135/135], Train loss:0.969, Train acc: 0.212\n2022-11-08 07:08:37.792 | INFO     | __main__:<cell line: 25>:70 - Epoch: 0, Train loss: 1.046, Epoch time=1958.828s, valid mean accuracy: 0.243, valid loss: 0.9425207376480103\n2022-11-08 07:08:52.047 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000010, Batch[1/135], Train loss:0.978, Train acc: 0.208\n2022-11-08 07:09:06.429 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000094, Batch[2/135], Train loss:1.048, Train acc: 0.198\n2022-11-08 07:09:20.809 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000063, Batch[3/135], Train loss:0.962, Train acc: 0.185\n2022-11-08 07:09:35.192 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000046, Batch[4/135], Train loss:0.884, Train acc: 0.228\n2022-11-08 07:09:49.587 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000022, Batch[5/135], Train loss:0.818, Train acc: 0.288\n2022-11-08 07:10:03.945 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000041, Batch[6/135], Train loss:0.596, Train acc: 0.437\n2022-11-08 07:10:18.318 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000098, Batch[7/135], Train loss:0.580, Train acc: 0.423\n2022-11-08 07:10:32.723 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000134, Batch[8/135], Train loss:0.451, Train acc: 0.503\n2022-11-08 07:10:47.110 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000065, Batch[9/135], Train loss:0.280, Train acc: 0.692\n2022-11-08 07:11:01.474 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000102, Batch[10/135], Train loss:0.715, Train acc: 0.367\n2022-11-08 07:11:15.869 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000026, Batch[11/135], Train loss:0.359, Train acc: 0.590\n2022-11-08 07:11:30.248 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000043, Batch[12/135], Train loss:0.475, Train acc: 0.469\n2022-11-08 07:11:44.643 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000027, Batch[13/135], Train loss:0.551, Train acc: 0.437\n2022-11-08 07:11:59.032 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000092, Batch[14/135], Train loss:0.548, Train acc: 0.437\n2022-11-08 07:12:13.437 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000067, Batch[15/135], Train loss:0.554, Train acc: 0.467\n2022-11-08 07:12:27.816 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000111, Batch[16/135], Train loss:0.494, Train acc: 0.505\n2022-11-08 07:12:42.215 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000117, Batch[17/135], Train loss:0.501, Train acc: 0.468\n2022-11-08 07:12:56.632 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000123, Batch[18/135], Train loss:0.519, Train acc: 0.476\n2022-11-08 07:13:11.005 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000132, Batch[19/135], Train loss:0.462, Train acc: 0.548\n2022-11-08 07:13:25.393 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000053, Batch[20/135], Train loss:0.408, Train acc: 0.552\n2022-11-08 07:13:39.795 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000084, Batch[21/135], Train loss:0.365, Train acc: 0.575\n2022-11-08 07:13:54.190 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000105, Batch[22/135], Train loss:0.305, Train acc: 0.630\n2022-11-08 07:14:08.557 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000068, Batch[23/135], Train loss:0.634, Train acc: 0.434\n2022-11-08 07:14:22.926 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000091, Batch[24/135], Train loss:0.287, Train acc: 0.682\n2022-11-08 07:14:37.341 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000093, Batch[25/135], Train loss:0.373, Train acc: 0.585\n2022-11-08 07:14:51.703 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000110, Batch[26/135], Train loss:0.315, Train acc: 0.610\n2022-11-08 07:15:06.076 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000039, Batch[27/135], Train loss:0.301, Train acc: 0.631\n2022-11-08 07:15:20.476 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000019, Batch[28/135], Train loss:0.445, Train acc: 0.537\n2022-11-08 07:15:34.857 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000074, Batch[29/135], Train loss:0.386, Train acc: 0.575\n2022-11-08 07:15:49.234 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000021, Batch[30/135], Train loss:0.323, Train acc: 0.620\n2022-11-08 07:16:03.603 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000003, Batch[31/135], Train loss:0.374, Train acc: 0.557\n2022-11-08 07:16:17.990 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000146, Batch[32/135], Train loss:0.374, Train acc: 0.604\n2022-11-08 07:16:32.382 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000028, Batch[33/135], Train loss:0.349, Train acc: 0.624\n2022-11-08 07:16:46.741 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000136, Batch[34/135], Train loss:0.343, Train acc: 0.594\n2022-11-08 07:17:01.115 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000141, Batch[35/135], Train loss:0.277, Train acc: 0.688\n2022-11-08 07:17:15.484 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000149, Batch[36/135], Train loss:0.273, Train acc: 0.669\n2022-11-08 07:17:29.862 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000106, Batch[37/135], Train loss:0.310, Train acc: 0.652\n2022-11-08 07:17:44.236 | INFO     | __main__:<cell line: 25>:60 - Epoch: 1, idx:BJ00000042, Batch[38/135], Train loss:0.360, Train acc: 0.611\n2022-11-08 07:17:58.614 | INFO     | __main__:<cell line: 25>:60 -\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nss:0.031, Train acc: 0.967\n2022-11-09 01:44:07.757 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000001, Batch[19/135], Train loss:0.031, Train acc: 0.962\n2022-11-09 01:44:22.059 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000035, Batch[20/135], Train loss:0.031, Train acc: 0.968\n2022-11-09 01:44:36.385 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000101, Batch[21/135], Train loss:0.043, Train acc: 0.942\n2022-11-09 01:44:50.714 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000052, Batch[22/135], Train loss:0.027, Train acc: 0.970\n2022-11-09 01:45:04.998 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000057, Batch[23/135], Train loss:0.032, Train acc: 0.968\n2022-11-09 01:45:19.295 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000149, Batch[24/135], Train loss:0.049, Train acc: 0.933\n2022-11-09 01:45:33.652 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000086, Batch[25/135], Train loss:0.032, Train acc: 0.957\n2022-11-09 01:45:47.945 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000127, Batch[26/135], Train loss:0.035, Train acc: 0.956\n2022-11-09 01:46:02.242 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000137, Batch[27/135], Train loss:0.035, Train acc: 0.956\n2022-11-09 01:46:16.542 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000095, Batch[28/135], Train loss:0.044, Train acc: 0.942\n2022-11-09 01:46:30.892 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000047, Batch[29/135], Train loss:0.051, Train acc: 0.951\n2022-11-09 01:46:45.179 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000106, Batch[30/135], Train loss:0.028, Train acc: 0.968\n2022-11-09 01:46:59.472 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000064, Batch[31/135], Train loss:0.032, Train acc: 0.967\n2022-11-09 01:47:13.784 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000143, Batch[32/135], Train loss:0.049, Train acc: 0.945\n2022-11-09 01:47:28.129 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000040, Batch[33/135], Train loss:0.042, Train acc: 0.967\n2022-11-09 01:47:42.434 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000088, Batch[34/135], Train loss:0.030, Train acc: 0.968\n2022-11-09 01:47:56.753 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000030, Batch[35/135], Train loss:0.030, Train acc: 0.960\n2022-11-09 01:48:11.062 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000082, Batch[36/135], Train loss:0.035, Train acc: 0.953\n2022-11-09 01:48:25.392 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000063, Batch[37/135], Train loss:0.032, Train acc: 0.964\n2022-11-09 01:48:39.699 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000103, Batch[38/135], Train loss:0.037, Train acc: 0.950\n2022-11-09 01:48:54.025 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000139, Batch[39/135], Train loss:0.029, Train acc: 0.967\n2022-11-09 01:49:08.317 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000043, Batch[40/135], Train loss:0.035, Train acc: 0.966\n2022-11-09 01:49:22.644 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000067, Batch[41/135], Train loss:0.025, Train acc: 0.971\n2022-11-09 01:49:36.934 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000068, Batch[42/135], Train loss:0.042, Train acc: 0.949\n2022-11-09 01:49:51.261 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000110, Batch[43/135], Train loss:0.041, Train acc: 0.943\n2022-11-09 01:50:05.552 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000108, Batch[44/135], Train loss:0.041, Train acc: 0.950\n2022-11-09 01:50:19.887 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000039, Batch[45/135], Train loss:0.033, Train acc: 0.956\n2022-11-09 01:50:34.206 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000141, Batch[46/135], Train loss:0.037, Train acc: 0.960\n2022-11-09 01:50:48.502 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000083, Batch[47/135], Train loss:0.033, Train acc: 0.964\n2022-11-09 01:51:02.806 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000116, Batch[48/135], Train loss:0.037, Train acc: 0.960\n2022-11-09 01:51:17.123 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000062, Batch[49/135], Train loss:0.037, Train acc: 0.965\n2022-11-09 01:51:31.443 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000059, Batch[50/135], Train loss:0.052, Train acc: 0.933\n2022-11-09 01:51:45.744 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000074, Batch[51/135], Train loss:0.037, Train acc: 0.962\n2022-11-09 01:52:00.039 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000084, Batch[52/135], Train loss:0.034, Train acc: 0.954\n2022-11-09 01:52:14.373 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000022, Batch[53/135], Train loss:0.034, Train acc: 0.970\n2022-11-09 01:52:28.668 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000015, Batch[54/135], Train loss:0.041, Train acc: 0.962\n2022-11-09 01:52:42.980 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000031, Batch[55/135], Train loss:0.030, Train acc: 0.964\n2022-11-09 01:52:57.274 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000122, Batch[56/135], Train loss:0.032, Train acc: 0.960\n2022-11-09 01:53:11.615 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000090, Batch[57/135], Train loss:0.046, Train acc: 0.940\n2022-11-09 01:53:25.933 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000027, Batch[58/135], Train loss:0.039, Train acc: 0.952\n2022-11-09 01:53:40.223 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000128, Batch[59/135], Train loss:0.044, Train acc: 0.942\n2022-11-09 01:53:54.548 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000079, Batch[60/135], Train loss:0.048, Train acc: 0.935\n2022-11-09 01:54:08.850 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000019, Batch[61/135], Train loss:0.027, Train acc: 0.969\n2022-11-09 01:54:23.149 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000125, Batch[62/135], Train loss:0.034, Train acc: 0.964\n2022-11-09 01:54:37.435 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000051, Batch[63/135], Train loss:0.028, Train acc: 0.966\n2022-11-09 01:54:51.735 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000072, Batch[64/135], Train loss:0.038, Train acc: 0.961\n2022-11-09 01:55:06.077 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000085, Batch[65/135], Train loss:0.044, Train acc: 0.956\n2022-11-09 01:55:20.362 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000126, Batch[66/135], Train loss:0.041, Train acc: 0.946\n2022-11-09 01:55:34.687 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000146, Batch[67/135], Train loss:0.028, Train acc: 0.970\n2022-11-09 01:55:48.992 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000029, Batch[68/135], Train loss:0.027, Train acc: 0.971\n2022-11-09 01:56:03.328 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000123, Batch[69/135], Train loss:0.034, Train acc: 0.959\n2022-11-09 01:56:17.622 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000023, Batch[70/135], Train loss:0.035, Train acc: 0.967\n2022-11-09 01:56:31.940 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000129, Batch[71/135], Train loss:0.043, Train acc: 0.954\n2022-11-09 01:56:46.247 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000118, Batch[72/135], Train loss:0.042, Train acc: 0.943\n2022-11-09 01:57:00.581 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000008, Batch[73/135], Train loss:0.026, Train acc: 0.976\n2022-11-09 01:57:14.905 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000041, Batch[74/135], Train loss:0.025, Train acc: 0.972\n2022-11-09 01:57:29.209 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000117, Batch[75/135], Train loss:0.056, Train acc: 0.923\n2022-11-09 01:57:43.502 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000132, Batch[76/135], Train loss:0.038, Train acc: 0.954\n2022-11-09 01:57:57.791 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000105, Batch[77/135], Train loss:0.045, Train acc: 0.937\n2022-11-09 01:58:12.118 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000124, Batch[78/135], Train loss:0.035, Train acc: 0.955\n2022-11-09 01:58:26.403 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000138, Batch[79/135], Train loss:0.042, Train acc: 0.943\n2022-11-09 01:58:40.691 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000053, Batch[80/135], Train loss:0.043, Train acc: 0.962\n2022-11-09 01:58:55.030 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000130, Batch[81/135], Train loss:0.035, Train acc: 0.962\n2022-11-09 01:59:09.315 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000026, Batch[82/135], Train loss:0.035, Train acc: 0.972\n2022-11-09 01:59:23.608 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000020, Batch[83/135], Train loss:0.037, Train acc: 0.968\n2022-11-09 01:59:37.885 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000093, Batch[84/135], Train loss:0.032, Train acc: 0.964\n2022-11-09 01:59:52.225 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000098, Batch[85/135], Train loss:0.037, Train acc: 0.952\n2022-11-09 02:00:06.524 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000080, Batch[86/135], Train loss:0.038, Train acc: 0.952\n2022-11-09 02:00:20.810 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000135, Batch[87/135], Train loss:0.032, Train acc: 0.958\n2022-11-09 02:00:35.156 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000081, Batch[88/135], Train loss:0.043, Train acc: 0.943\n2022-11-09 02:00:49.476 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000004, Batch[89/135], Train loss:0.052, Train acc: 0.931\n2022-11-09 02:01:03.769 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000024, Batch[90/135], Train loss:0.047, Train acc: 0.961\n2022-11-09 02:01:18.057 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000042, Batch[91/135], Train loss:0.030, Train acc: 0.971\n2022-11-09 02:01:32.383 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000104, Batch[92/135], Train loss:0.035, Train acc: 0.962\n2022-11-09 02:01:46.703 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000070, Batch[93/135], Train loss:0.034, Train acc: 0.962\n2022-11-09 02:02:00.982 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000112, Batch[94/135], Train loss:0.038, Train acc: 0.954\n2022-11-09 02:02:15.335 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000102, Batch[95/135], Train loss:0.039, Train acc: 0.951\n2022-11-09 02:02:29.632 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000055, Batch[96/135], Train loss:0.025, Train acc: 0.969\n2022-11-09 02:02:43.963 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000133, Batch[97/135], Train loss:0.042, Train acc: 0.949\n2022-11-09 02:02:58.286 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000009, Batch[98/135], Train loss:0.038, Train acc: 0.969\n2022-11-09 02:03:12.605 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000121, Batch[99/135], Train loss:0.029, Train acc: 0.968\n2022-11-09 02:03:26.895 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000007, Batch[100/135], Train loss:0.025, Train acc: 0.978\n2022-11-09 02:03:41.188 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000013, Batch[101/135], Train loss:0.026, Train acc: 0.969\n2022-11-09 02:03:55.502 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000028, Batch[102/135], Train loss:0.023, Train acc: 0.975\n2022-11-09 02:04:09.797 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000131, Batch[103/135], Train loss:0.038, Train acc: 0.946\n2022-11-09 02:04:24.091 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000111, Batch[104/135], Train loss:0.039, Train acc: 0.949\n2022-11-09 02:04:38.430 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000005, Batch[105/135], Train loss:0.031, Train acc: 0.964\n2022-11-09 02:04:52.741 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000089, Batch[106/135], Train loss:0.028, Train acc: 0.964\n2022-11-09 02:05:07.038 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000078, Batch[107/135], Train loss:0.045, Train acc: 0.939\n2022-11-09 02:05:21.358 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000145, Batch[108/135], Train loss:0.042, Train acc: 0.956\n2022-11-09 02:05:35.712 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000048, Batch[109/135], Train loss:0.026, Train acc: 0.973\n2022-11-09 02:05:50.000 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000087, Batch[110/135], Train loss:0.037, Train acc: 0.950\n2022-11-09 02:06:04.300 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000148, Batch[111/135], Train loss:0.034, Train acc: 0.966\n2022-11-09 02:06:18.584 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000003, Batch[112/135], Train loss:0.031, Train acc: 0.963\n2022-11-09 02:06:32.944 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000034, Batch[113/135], Train loss:0.032, Train acc: 0.969\n2022-11-09 02:06:47.229 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000109, Batch[114/135], Train loss:0.036, Train acc: 0.955\n2022-11-09 02:07:01.565 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000091, Batch[115/135], Train loss:0.037, Train acc: 0.957\n2022-11-09 02:07:15.865 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000014, Batch[116/135], Train loss:0.058, Train acc: 0.945\n2022-11-09 02:07:30.194 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000113, Batch[117/135], Train loss:0.027, Train acc: 0.968\n2022-11-09 02:07:44.507 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000142, Batch[118/135], Train loss:0.033, Train acc: 0.960\n2022-11-09 02:07:58.811 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000012, Batch[119/135], Train loss:0.050, Train acc: 0.959\n2022-11-09 02:08:13.126 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000066, Batch[120/135], Train loss:0.028, Train acc: 0.970\n2022-11-09 02:08:27.430 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000073, Batch[121/135], Train loss:0.032, Train acc: 0.959\n2022-11-09 02:08:41.731 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000038, Batch[122/135], Train loss:0.035, Train acc: 0.954\n2022-11-09 02:08:56.054 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000147, Batch[123/135], Train loss:0.029, Train acc: 0.963\n2022-11-09 02:09:10.352 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000006, Batch[124/135], Train loss:0.035, Train acc: 0.955\n2022-11-09 02:09:24.702 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000065, Batch[125/135], Train loss:0.034, Train acc: 0.963\n2022-11-09 02:09:38.995 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000119, Batch[126/135], Train loss:0.027, Train acc: 0.967\n2022-11-09 02:09:53.315 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000134, Batch[127/135], Train loss:0.043, Train acc: 0.940\n2022-11-09 02:10:07.628 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000037, Batch[128/135], Train loss:0.029, Train acc: 0.969\n2022-11-09 02:10:21.957 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000092, Batch[129/135], Train loss:0.043, Train acc: 0.948\n2022-11-09 02:10:36.250 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000025, Batch[130/135], Train loss:0.040, Train acc: 0.963\n2022-11-09 02:10:50.553 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000058, Batch[131/135], Train loss:0.030, Train acc: 0.972\n2022-11-09 02:11:04.845 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000136, Batch[132/135], Train loss:0.043, Train acc: 0.944\n2022-11-09 02:11:19.178 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000010, Batch[133/135], Train loss:0.036, Train acc: 0.968\n2022-11-09 02:11:33.492 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000107, Batch[134/135], Train loss:0.036, Train acc: 0.957\n2022-11-09 02:11:47.803 | INFO     | __main__:<cell line: 25>:60 - Epoch: 35, idx:BJ00000011, Batch[135/135], Train loss:0.034, Train acc: 0.965\n2022-11-09 02:12:15.162 | INFO     | __main__:<cell line: 25>:70 - Epoch: 35, Train loss: 0.036, Epoch time=1958.896s, valid mean accuracy: 0.955, valid loss: 0.0372004508972168\n2022-11-09 02:12:29.362 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000073, Batch[1/135], Train loss:0.034, Train acc: 0.957\n2022-11-09 02:12:43.652 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000127, Batch[2/135], Train loss:0.037, Train acc: 0.952\n2022-11-09 02:12:57.958 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000086, Batch[3/135], Train loss:0.036, Train acc: 0.951\n2022-11-09 02:13:12.267 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000122, Batch[4/135], Train loss:0.033, Train acc: 0.959\n2022-11-09 02:13:26.618 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000144, Batch[5/135], Train loss:0.039, Train acc: 0.949\n2022-11-09 02:13:40.900 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000068, Batch[6/135], Train loss:0.043, Train acc: 0.946\n2022-11-09 02:13:55.196 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000028, Batch[7/135], Train loss:0.027, Train acc: 0.976\n2022-11-09 02:14:09.526 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000006, Batch[8/135], Train loss:0.033, Train acc: 0.961\n2022-11-09 02:14:23.848 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000005, Batch[9/135], Train loss:0.031, Train acc: 0.969\n2022-11-09 02:14:38.131 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000113, Batch[10/135], Train loss:0.029, Train acc: 0.969\n2022-11-09 02:14:52.435 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000026, Batch[11/135], Train loss:0.026, Train acc: 0.976\n2022-11-09 02:15:06.730 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000123, Batch[12/135], Train loss:0.034, Train acc: 0.959\n2022-11-09 02:15:21.049 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000009, Batch[13/135], Train loss:0.041, Train acc: 0.968\n2022-11-09 02:15:35.350 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000030, Batch[14/135], Train loss:0.034, Train acc: 0.952\n2022-11-09 02:15:49.687 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000084, Batch[15/135], Train loss:0.039, Train acc: 0.945\n2022-11-09 02:16:03.977 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000105, Batch[16/135], Train loss:0.045, Train acc: 0.936\n2022-11-09 02:16:18.303 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000087, Batch[17/135], Train loss:0.039, Train acc: 0.945\n2022-11-09 02:16:32.646 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000129, Batch[18/135], Train loss:0.042, Train acc: 0.952\n2022-11-09 02:16:46.938 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000094, Batch[19/135], Train loss:0.039, Train acc: 0.950\n2022-11-09 02:17:01.243 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000082, Batch[20/135], Train loss:0.035, Train acc: 0.956\n2022-11-09 02:17:15.569 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000146, Batch[21/135], Train loss:0.033, Train acc: 0.969\n2022-11-09 02:17:29.888 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000149, Batch[22/135], Train loss:0.045, Train acc: 0.943\n2022-11-09 02:17:44.183 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000065, Batch[23/135], Train loss:0.040, Train acc: 0.965\n2022-11-09 02:17:58.478 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000101, Batch[24/135], Train loss:0.041, Train acc: 0.952\n2022-11-09 02:18:12.814 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000115, Batch[25/135], Train loss:0.034, Train acc: 0.961\n2022-11-09 02:18:27.106 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000139, Batch[26/135], Train loss:0.031, Train acc: 0.969\n2022-11-09 02:18:41.397 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000032, Batch[27/135], Train loss:0.024, Train acc: 0.978\n2022-11-09 02:18:55.703 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000074, Batch[28/135], Train loss:0.040, Train acc: 0.962\n2022-11-09 02:19:10.041 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000112, Batch[29/135], Train loss:0.036, Train acc: 0.957\n2022-11-09 02:19:24.339 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000003, Batch[30/135], Train loss:0.031, Train acc: 0.961\n2022-11-09 02:19:38.630 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000053, Batch[31/135], Train loss:0.036, Train acc: 0.962\n2022-11-09 02:19:52.933 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000089, Batch[32/135], Train loss:0.027, Train acc: 0.966\n2022-11-09 02:20:07.257 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000042, Batch[33/135], Train loss:0.028, Train acc: 0.969\n2022-11-09 02:20:21.541 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000017, Batch[34/135], Train loss:0.030, Train acc: 0.967\n2022-11-09 02:20:35.841 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000041, Batch[35/135], Train loss:0.026, Train acc: 0.971\n2022-11-09 02:20:50.157 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000104, Batch[36/135], Train loss:0.033, Train acc: 0.961\n2022-11-09 02:21:04.479 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000021, Batch[37/135], Train loss:0.027, Train acc: 0.969\n2022-11-09 02:21:18.783 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000024, Batch[38/135], Train loss:0.034, Train acc: 0.968\n2022-11-09 02:21:33.079 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000079, Batch[39/135], Train loss:0.050, Train acc: 0.932\n2022-11-09 02:21:47.378 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000107, Batch[40/135], Train loss:0.038, Train acc: 0.953\n2022-11-09 02:22:01.673 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000049, Batch[41/135], Train loss:0.029, Train acc: 0.965\n2022-11-09 02:22:15.968 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000116, Batch[42/135], Train loss:0.040, Train acc: 0.953\n2022-11-09 02:22:30.284 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000119, Batch[43/135], Train loss:0.030, Train acc: 0.965\n2022-11-09 02:22:44.585 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000036, Batch[44/135], Train loss:0.042, Train acc: 0.947\n2022-11-09 02:22:58.933 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000052, Batch[45/135], Train loss:0.027, Train acc: 0.971\n2022-11-09 02:23:13.237 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000043, Batch[46/135], Train loss:0.033, Train acc: 0.969\n2022-11-09 02:23:27.522 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000111, Batch[47/135], Train loss:0.034, Train acc: 0.958\n2022-11-09 02:23:41.815 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000106, Batch[48/135], Train loss:0.025, Train acc: 0.970\n2022-11-09 02:23:56.126 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000055, Batch[49/135], Train loss:0.025, Train acc: 0.974\n2022-11-09 02:24:10.435 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000121, Batch[50/135], Train loss:0.031, Train acc: 0.969\n2022-11-09 02:24:24.727 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000010, Batch[51/135], Train loss:0.045, Train acc: 0.964\n2022-11-09 02:24:39.014 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000098, Batch[52/135], Train loss:0.037, Train acc: 0.952\n2022-11-09 02:24:53.344 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000142, Batch[53/135], Train loss:0.034, Train acc: 0.959\n2022-11-09 02:25:07.634 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000117, Batch[54/135], Train loss:0.050, Train acc: 0.932\n2022-11-09 02:25:21.931 | INFO     | __main__:<cell line: 25>:60 - Epoch: 36, idx:BJ00000148, Batch[55/135], Train loss:0.031, Train acc: 0.968\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    acc_list = []\n",
    "    losses = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, x, y in test_loader:\n",
    "            # x, y = x.to(device), y.to(device)\n",
    "            # with autocast():\n",
    "            logits = model(x.to(next(model.parameters()).device))\n",
    "            logits = F.softmax(logits, dim=1)\n",
    "            y = y.to(logits.device)\n",
    "            loss = evalmetric.tversky_loss(y, logits)\n",
    "            losses += loss\n",
    "            acc = evalmetric.dice_coef_multilabel(y, logits)\n",
    "            acc_list.append(acc)\n",
    "        val_loss = losses / len(test_loader)\n",
    "        return acc_list, sum(acc_list) / acc_list.__len__(), val_loss\n",
    "# 暂时使用交叉熵损失函数测试模型\n",
    "\n",
    "max_acc = 0\n",
    "# optimizer = torch.optim.ASGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = 0\n",
    "    start_time = time.time()\n",
    "    for i, (idx, x, y) in enumerate(train_loader):\n",
    "        # print(x.shape, y.shape)\n",
    "        # for s in range(0, 240, 40):\n",
    "        #     draw_multi(x[0][0][s], y[0][1][s], y[0][2][s])\n",
    "\n",
    "        # x = x.to(device)\n",
    "        # y = y.to(device)\n",
    "        logits = model(x.to(next(model.parameters()).device))\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        y = y.to(logits.device)\n",
    "        loss = evalmetric.tversky_loss(y, logits)\n",
    "        origin_loss = loss\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 模型并行必须讲处理之后的loss值重新赋值给原来的loss，使用原来的loss去backword\n",
    "        # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        #     # logger.info(f\"type {type(scaled_loss)}, {scaled_loss.shape}, {scaled_loss}, {scaled_loss.device}\")\n",
    "        #     loss = scaled_loss   \n",
    "\n",
    "            # scaled_loss.to(next(model.parameters()).device)\n",
    "            # scaled_loss.backward()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 1)\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "        acc = evalmetric.dice_coef_multilabel(y, logits)\n",
    "        \n",
    "        # if hasattr(torch.cuda, 'empty_cache'):\n",
    "        #     torch.cuda.empty_cache()\n",
    "        if i % 1 == 0:\n",
    "            logger.info(f\"Epoch: {epoch}, idx:{idx[0]}, Batch[{i+1}/{len(train_loader)}],\"\n",
    "                        f\" Train loss:{origin_loss:.3f}, Train acc:{acc: .3f}\")\n",
    "        \n",
    "    #     break\n",
    "    # break\n",
    "    acc_list, mean_acc, val_loss = evaluate(model, test_loader)\n",
    "    end_time = time.time()\n",
    "    train_loss = losses / len(train_loader)\n",
    "    if mean_acc > max_acc:\n",
    "        torch.save(model.state_dict(), f\"{model_path}epoch{epoch+1}_trainloss_{train_loss:.3f}_validacc_{mean_acc:.3f}.pth\")\n",
    "    logger.info(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Epoch time={(end_time - start_time):.3f}s, valid mean accuracy: {mean_acc:.3f}, valid loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8763d42-62b6-4415-8be7-b637949ddfc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch10_trainloss_0.051_validacc_0.946.pth\n",
       "epoch11_trainloss_0.052_validacc_0.944.pth\n",
       "epoch12_trainloss_0.049_validacc_0.949.pth\n",
       "epoch13_trainloss_0.048_validacc_0.944.pth\n",
       "epoch14_trainloss_0.046_validacc_0.951.pth\n",
       "epoch15_trainloss_0.047_validacc_0.941.pth\n",
       "epoch16_trainloss_0.046_validacc_0.953.pth\n",
       "epoch17_trainloss_0.056_validacc_0.940.pth\n",
       "epoch18_trainloss_0.045_validacc_0.947.pth\n",
       "epoch19_trainloss_0.044_validacc_0.954.pth\n",
       "epoch1_trainloss_1.046_validacc_0.243.pth\n",
       "epoch20_trainloss_0.043_validacc_0.951.pth\n",
       "epoch21_trainloss_0.041_validacc_0.955.pth\n",
       "epoch22_trainloss_0.041_validacc_0.956.pth\n",
       "epoch23_trainloss_0.040_validacc_0.956.pth\n",
       "epoch24_trainloss_0.040_validacc_0.951.pth\n",
       "epoch25_trainloss_0.041_validacc_0.951.pth\n",
       "epoch26_trainloss_0.040_validacc_0.951.pth\n",
       "epoch27_trainloss_0.038_validacc_0.954.pth\n",
       "epoch28_trainloss_0.038_validacc_0.958.pth\n",
       "epoch29_trainloss_0.038_validacc_0.959.pth\n",
       "epoch2_trainloss_0.382_validacc_0.668.pth\n",
       "epoch30_trainloss_0.037_validacc_0.958.pth\n",
       "epoch31_trainloss_0.039_validacc_0.948.pth\n",
       "epoch32_trainloss_0.040_validacc_0.957.pth\n",
       "epoch33_trainloss_0.038_validacc_0.956.pth\n",
       "epoch34_trainloss_0.037_validacc_0.959.pth\n",
       "epoch35_trainloss_0.037_validacc_0.961.pth\n",
       "epoch36_trainloss_0.036_validacc_0.955.pth\n",
       "epoch3_trainloss_0.295_validacc_0.793.pth\n",
       "epoch4_trainloss_0.158_validacc_0.835.pth\n",
       "epoch5_trainloss_0.138_validacc_0.832.pth\n",
       "epoch6_trainloss_0.115_validacc_0.811.pth\n",
       "epoch7_trainloss_0.084_validacc_0.935.pth\n",
       "epoch8_trainloss_0.055_validacc_0.935.pth\n",
       "epoch9_trainloss_0.055_validacc_0.945.pth\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "epoch10_trainloss_0.051_validacc_0.946.pth\nepoch11_trainloss_0.052_validacc_0.944.pth\nepoch12_trainloss_0.049_validacc_0.949.pth\nepoch13_trainloss_0.048_validacc_0.944.pth\nepoch14_trainloss_0.046_validacc_0.951.pth\nepoch15_trainloss_0.047_validacc_0.941.pth\nepoch16_trainloss_0.046_validacc_0.953.pth\nepoch17_trainloss_0.056_validacc_0.940.pth\nepoch18_trainloss_0.045_validacc_0.947.pth\nepoch19_trainloss_0.044_validacc_0.954.pth\nepoch1_trainloss_1.046_validacc_0.243.pth\nepoch20_trainloss_0.043_validacc_0.951.pth\nepoch21_trainloss_0.041_validacc_0.955.pth\nepoch22_trainloss_0.041_validacc_0.956.pth\nepoch23_trainloss_0.040_validacc_0.956.pth\nepoch24_trainloss_0.040_validacc_0.951.pth\nepoch25_trainloss_0.041_validacc_0.951.pth\nepoch26_trainloss_0.040_validacc_0.951.pth\nepoch27_trainloss_0.038_validacc_0.954.pth\nepoch28_trainloss_0.038_validacc_0.958.pth\nepoch29_trainloss_0.038_validacc_0.959.pth\nepoch2_trainloss_0.382_validacc_0.668.pth\nepoch30_trainloss_0.037_validacc_0.958.pth\nepoch31_trainloss_0.039_validacc_0.948.pth\nepoch32_trainloss_0.040_validacc_0.957.pth\nepoch33_trainloss_0.038_validacc_0.956.pth\nepoch34_trainloss_0.037_validacc_0.959.pth\nepoch35_trainloss_0.037_validacc_0.961.pth\nepoch36_trainloss_0.036_validacc_0.955.pth\nepoch3_trainloss_0.295_validacc_0.793.pth\nepoch4_trainloss_0.158_validacc_0.835.pth\nepoch5_trainloss_0.138_validacc_0.832.pth\nepoch6_trainloss_0.115_validacc_0.811.pth\nepoch7_trainloss_0.084_validacc_0.935.pth\nepoch8_trainloss_0.055_validacc_0.935.pth\nepoch9_trainloss_0.055_validacc_0.945.pth\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "ls ./TEST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "033fb244-ab1e-4aa9-ae87-a37eff784263",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f3de284-9d72-4e13-a1bf-d26e5450341c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# (6) save weights to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2d0eb8d-5c90-4e6c-a7ea-9880a7c0a493",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "rm ./TEST/cpu_epoch29_trainloss_0.038_validacc_0.959.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b365b7df-b6b3-4d9f-934a-715f0196f8f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu_epoch34_trainloss_0.037_validacc_0.959.pth\n",
       "cpu_epoch35_trainloss_0.037_validacc_0.961.pth\n",
       "epoch10_trainloss_0.051_validacc_0.946.pth\n",
       "epoch11_trainloss_0.052_validacc_0.944.pth\n",
       "epoch12_trainloss_0.049_validacc_0.949.pth\n",
       "epoch13_trainloss_0.048_validacc_0.944.pth\n",
       "epoch14_trainloss_0.046_validacc_0.951.pth\n",
       "epoch15_trainloss_0.047_validacc_0.941.pth\n",
       "epoch16_trainloss_0.046_validacc_0.953.pth\n",
       "epoch17_trainloss_0.056_validacc_0.940.pth\n",
       "epoch18_trainloss_0.045_validacc_0.947.pth\n",
       "epoch19_trainloss_0.044_validacc_0.954.pth\n",
       "epoch1_trainloss_1.046_validacc_0.243.pth\n",
       "epoch20_trainloss_0.043_validacc_0.951.pth\n",
       "epoch21_trainloss_0.041_validacc_0.955.pth\n",
       "epoch22_trainloss_0.041_validacc_0.956.pth\n",
       "epoch23_trainloss_0.040_validacc_0.956.pth\n",
       "epoch24_trainloss_0.040_validacc_0.951.pth\n",
       "epoch25_trainloss_0.041_validacc_0.951.pth\n",
       "epoch26_trainloss_0.040_validacc_0.951.pth\n",
       "epoch27_trainloss_0.038_validacc_0.954.pth\n",
       "epoch28_trainloss_0.038_validacc_0.958.pth\n",
       "epoch29_trainloss_0.038_validacc_0.959.pth\n",
       "epoch2_trainloss_0.382_validacc_0.668.pth\n",
       "epoch30_trainloss_0.037_validacc_0.958.pth\n",
       "epoch31_trainloss_0.039_validacc_0.948.pth\n",
       "epoch32_trainloss_0.040_validacc_0.957.pth\n",
       "epoch33_trainloss_0.038_validacc_0.956.pth\n",
       "epoch34_trainloss_0.037_validacc_0.959.pth\n",
       "epoch35_trainloss_0.037_validacc_0.961.pth\n",
       "epoch36_trainloss_0.036_validacc_0.955.pth\n",
       "epoch3_trainloss_0.295_validacc_0.793.pth\n",
       "epoch4_trainloss_0.158_validacc_0.835.pth\n",
       "epoch5_trainloss_0.138_validacc_0.832.pth\n",
       "epoch6_trainloss_0.115_validacc_0.811.pth\n",
       "epoch7_trainloss_0.084_validacc_0.935.pth\n",
       "epoch8_trainloss_0.055_validacc_0.935.pth\n",
       "epoch9_trainloss_0.055_validacc_0.945.pth\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "cpu_epoch34_trainloss_0.037_validacc_0.959.pth\ncpu_epoch35_trainloss_0.037_validacc_0.961.pth\nepoch10_trainloss_0.051_validacc_0.946.pth\nepoch11_trainloss_0.052_validacc_0.944.pth\nepoch12_trainloss_0.049_validacc_0.949.pth\nepoch13_trainloss_0.048_validacc_0.944.pth\nepoch14_trainloss_0.046_validacc_0.951.pth\nepoch15_trainloss_0.047_validacc_0.941.pth\nepoch16_trainloss_0.046_validacc_0.953.pth\nepoch17_trainloss_0.056_validacc_0.940.pth\nepoch18_trainloss_0.045_validacc_0.947.pth\nepoch19_trainloss_0.044_validacc_0.954.pth\nepoch1_trainloss_1.046_validacc_0.243.pth\nepoch20_trainloss_0.043_validacc_0.951.pth\nepoch21_trainloss_0.041_validacc_0.955.pth\nepoch22_trainloss_0.041_validacc_0.956.pth\nepoch23_trainloss_0.040_validacc_0.956.pth\nepoch24_trainloss_0.040_validacc_0.951.pth\nepoch25_trainloss_0.041_validacc_0.951.pth\nepoch26_trainloss_0.040_validacc_0.951.pth\nepoch27_trainloss_0.038_validacc_0.954.pth\nepoch28_trainloss_0.038_validacc_0.958.pth\nepoch29_trainloss_0.038_validacc_0.959.pth\nepoch2_trainloss_0.382_validacc_0.668.pth\nepoch30_trainloss_0.037_validacc_0.958.pth\nepoch31_trainloss_0.039_validacc_0.948.pth\nepoch32_trainloss_0.040_validacc_0.957.pth\nepoch33_trainloss_0.038_validacc_0.956.pth\nepoch34_trainloss_0.037_validacc_0.959.pth\nepoch35_trainloss_0.037_validacc_0.961.pth\nepoch36_trainloss_0.036_validacc_0.955.pth\nepoch3_trainloss_0.295_validacc_0.793.pth\nepoch4_trainloss_0.158_validacc_0.835.pth\nepoch5_trainloss_0.138_validacc_0.832.pth\nepoch6_trainloss_0.115_validacc_0.811.pth\nepoch7_trainloss_0.084_validacc_0.935.pth\nepoch8_trainloss_0.055_validacc_0.935.pth\nepoch9_trainloss_0.055_validacc_0.945.pth\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "ls ./TEST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab24582-e5c3-403b-8332-c2696b60d641",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_dict = model.load_state_dict(torch.load('./TEST/cpu_epoch35_trainloss_0.037_validacc_0.961.pth'))\n",
    "# torch.save(model.cpu().state_dict(), './TEST/cpu_epoch34_trainloss_0.037_validacc_0.959.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14e25a82-3770-4eb9-840a-170a46dd3fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completed 256.0 KiB/6.5 GiB (1.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 512.0 KiB/6.5 GiB (3.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 768.0 KiB/6.5 GiB (5.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 1.0 MiB/6.5 GiB (6.6 MiB/s) with 38 file(s) remaining  \n",
       "Completed 1.2 MiB/6.5 GiB (8.2 MiB/s) with 38 file(s) remaining  \n",
       "Completed 1.5 MiB/6.5 GiB (9.8 MiB/s) with 38 file(s) remaining  \n",
       "Completed 1.8 MiB/6.5 GiB (11.5 MiB/s) with 38 file(s) remaining \n",
       "Completed 2.0 MiB/6.5 GiB (13.0 MiB/s) with 38 file(s) remaining \n",
       "Completed 2.2 MiB/6.5 GiB (14.4 MiB/s) with 38 file(s) remaining \n",
       "Completed 2.5 MiB/6.5 GiB (15.9 MiB/s) with 38 file(s) remaining \n",
       "Completed 2.8 MiB/6.5 GiB (17.4 MiB/s) with 38 file(s) remaining \n",
       "Completed 3.0 MiB/6.5 GiB (18.9 MiB/s) with 38 file(s) remaining \n",
       "Completed 3.2 MiB/6.5 GiB (20.4 MiB/s) with 38 file(s) remaining \n",
       "Completed 3.5 MiB/6.5 GiB (21.7 MiB/s) with 38 file(s) remaining \n",
       "Completed 3.8 MiB/6.5 GiB (23.2 MiB/s) with 38 file(s) remaining \n",
       "Completed 4.0 MiB/6.5 GiB (24.6 MiB/s) with 38 file(s) remaining \n",
       "Completed 4.2 MiB/6.5 GiB (26.0 MiB/s) with 38 file(s) remaining \n",
       "Completed 4.5 MiB/6.5 GiB (27.4 MiB/s) with 38 file(s) remaining \n",
       "Completed 4.8 MiB/6.5 GiB (28.7 MiB/s) with 38 file(s) remaining \n",
       "Completed 5.0 MiB/6.5 GiB (30.1 MiB/s) with 38 file(s) remaining \n",
       "Completed 5.2 MiB/6.5 GiB (31.4 MiB/s) with 38 file(s) remaining \n",
       "Completed 5.5 MiB/6.5 GiB (32.7 MiB/s) with 38 file(s) remaining \n",
       "Completed 5.8 MiB/6.5 GiB (34.0 MiB/s) with 38 file(s) remaining \n",
       "Completed 6.0 MiB/6.5 GiB (35.3 MiB/s) with 38 file(s) remaining \n",
       "Completed 6.2 MiB/6.5 GiB (36.7 MiB/s) with 38 file(s) remaining \n",
       "Completed 6.5 MiB/6.5 GiB (37.9 MiB/s) with 38 file(s) remaining \n",
       "Completed 6.8 MiB/6.5 GiB (39.0 MiB/s) with 38 file(s) remaining \n",
       "Completed 7.0 MiB/6.5 GiB (40.2 MiB/s) with 38 file(s) remaining \n",
       "Completed 7.2 MiB/6.5 GiB (41.6 MiB/s) with 38 file(s) remaining \n",
       "Completed 7.5 MiB/6.5 GiB (42.9 MiB/s) with 38 file(s) remaining \n",
       "Completed 7.8 MiB/6.5 GiB (43.9 MiB/s) with 38 file(s) remaining \n",
       "Completed 8.0 MiB/6.5 GiB (45.0 MiB/s) with 38 file(s) remaining \n",
       "Completed 8.2 MiB/6.5 GiB (46.2 MiB/s) with 38 file(s) remaining \n",
       "Completed 8.5 MiB/6.5 GiB (47.4 MiB/s) with 38 file(s) remaining \n",
       "Completed 8.8 MiB/6.5 GiB (48.7 MiB/s) with 38 file(s) remaining \n",
       "Completed 9.0 MiB/6.5 GiB (49.8 MiB/s) with 38 file(s) remaining \n",
       "Completed 9.2 MiB/6.5 GiB (50.7 MiB/s) with 38 file(s) remaining \n",
       "Completed 9.5 MiB/6.5 GiB (52.0 MiB/s) with 38 file(s) remaining \n",
       "Completed 9.8 MiB/6.5 GiB (53.2 MiB/s) with 38 file(s) remaining \n",
       "Completed 10.0 MiB/6.5 GiB (54.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 10.2 MiB/6.5 GiB (55.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 10.5 MiB/6.5 GiB (56.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 10.8 MiB/6.5 GiB (57.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 11.0 MiB/6.5 GiB (58.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 11.2 MiB/6.5 GiB (59.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 11.5 MiB/6.5 GiB (60.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 11.8 MiB/6.5 GiB (60.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 12.0 MiB/6.5 GiB (62.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 12.2 MiB/6.5 GiB (63.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 12.5 MiB/6.5 GiB (64.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 12.8 MiB/6.5 GiB (64.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 13.0 MiB/6.5 GiB (65.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 13.2 MiB/6.5 GiB (65.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 13.5 MiB/6.5 GiB (66.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 13.8 MiB/6.5 GiB (67.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 14.0 MiB/6.5 GiB (68.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 14.2 MiB/6.5 GiB (69.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 14.5 MiB/6.5 GiB (70.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 14.8 MiB/6.5 GiB (71.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 15.0 MiB/6.5 GiB (71.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 15.2 MiB/6.5 GiB (72.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 15.5 MiB/6.5 GiB (72.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 15.8 MiB/6.5 GiB (73.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 16.0 MiB/6.5 GiB (75.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 16.2 MiB/6.5 GiB (76.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 16.5 MiB/6.5 GiB (76.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 16.8 MiB/6.5 GiB (77.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 17.0 MiB/6.5 GiB (78.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 17.2 MiB/6.5 GiB (79.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 17.5 MiB/6.5 GiB (79.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 17.8 MiB/6.5 GiB (80.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 18.0 MiB/6.5 GiB (81.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 18.2 MiB/6.5 GiB (82.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 18.5 MiB/6.5 GiB (82.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 18.8 MiB/6.5 GiB (83.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 19.0 MiB/6.5 GiB (84.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 19.2 MiB/6.5 GiB (85.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 19.5 MiB/6.5 GiB (84.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 19.8 MiB/6.5 GiB (85.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 20.0 MiB/6.5 GiB (86.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 20.2 MiB/6.5 GiB (87.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 20.5 MiB/6.5 GiB (88.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 20.8 MiB/6.5 GiB (89.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 21.0 MiB/6.5 GiB (89.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 21.2 MiB/6.5 GiB (90.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 21.5 MiB/6.5 GiB (90.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 21.8 MiB/6.5 GiB (91.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 22.0 MiB/6.5 GiB (91.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 22.2 MiB/6.5 GiB (92.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 22.5 MiB/6.5 GiB (93.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 22.8 MiB/6.5 GiB (94.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 23.0 MiB/6.5 GiB (95.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 23.2 MiB/6.5 GiB (96.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 23.5 MiB/6.5 GiB (96.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 23.8 MiB/6.5 GiB (96.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 24.0 MiB/6.5 GiB (96.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 24.2 MiB/6.5 GiB (97.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 24.5 MiB/6.5 GiB (98.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 24.8 MiB/6.5 GiB (99.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 25.0 MiB/6.5 GiB (99.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 25.2 MiB/6.5 GiB (100.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 25.5 MiB/6.5 GiB (100.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 25.8 MiB/6.5 GiB (100.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 26.0 MiB/6.5 GiB (101.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 26.2 MiB/6.5 GiB (102.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 26.5 MiB/6.5 GiB (103.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 26.8 MiB/6.5 GiB (103.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 27.0 MiB/6.5 GiB (104.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 27.2 MiB/6.5 GiB (105.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 27.5 MiB/6.5 GiB (105.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 27.8 MiB/6.5 GiB (105.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 28.0 MiB/6.5 GiB (106.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 28.2 MiB/6.5 GiB (106.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 28.5 MiB/6.5 GiB (107.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 28.8 MiB/6.5 GiB (108.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 29.0 MiB/6.5 GiB (108.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 29.2 MiB/6.5 GiB (108.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 29.5 MiB/6.5 GiB (108.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 29.8 MiB/6.5 GiB (109.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 30.0 MiB/6.5 GiB (110.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 30.2 MiB/6.5 GiB (111.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 30.5 MiB/6.5 GiB (111.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 30.8 MiB/6.5 GiB (112.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 31.0 MiB/6.5 GiB (112.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 31.2 MiB/6.5 GiB (113.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 31.5 MiB/6.5 GiB (113.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 31.8 MiB/6.5 GiB (113.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 32.0 MiB/6.5 GiB (114.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 32.2 MiB/6.5 GiB (114.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 32.5 MiB/6.5 GiB (115.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 32.8 MiB/6.5 GiB (116.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 33.0 MiB/6.5 GiB (116.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 33.2 MiB/6.5 GiB (116.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 33.5 MiB/6.5 GiB (117.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 33.8 MiB/6.5 GiB (117.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 34.0 MiB/6.5 GiB (117.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 34.2 MiB/6.5 GiB (118.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 34.5 MiB/6.5 GiB (118.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 34.8 MiB/6.5 GiB (119.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 35.0 MiB/6.5 GiB (119.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 35.2 MiB/6.5 GiB (120.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 35.5 MiB/6.5 GiB (120.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 35.8 MiB/6.5 GiB (121.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 36.0 MiB/6.5 GiB (120.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 36.2 MiB/6.5 GiB (120.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 36.5 MiB/6.5 GiB (121.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 36.8 MiB/6.5 GiB (122.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 37.0 MiB/6.5 GiB (123.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 37.2 MiB/6.5 GiB (123.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 37.5 MiB/6.5 GiB (123.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 37.8 MiB/6.5 GiB (124.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 38.0 MiB/6.5 GiB (125.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 38.2 MiB/6.5 GiB (123.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 38.5 MiB/6.5 GiB (124.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 38.8 MiB/6.5 GiB (125.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 39.0 MiB/6.5 GiB (125.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 39.2 MiB/6.5 GiB (126.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 39.5 MiB/6.5 GiB (126.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 39.8 MiB/6.5 GiB (127.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 40.0 MiB/6.5 GiB (127.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 40.2 MiB/6.5 GiB (128.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 40.5 MiB/6.5 GiB (127.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 40.8 MiB/6.5 GiB (127.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 41.0 MiB/6.5 GiB (128.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 41.2 MiB/6.5 GiB (129.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 41.5 MiB/6.5 GiB (129.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 41.8 MiB/6.5 GiB (130.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 42.0 MiB/6.5 GiB (130.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 42.2 MiB/6.5 GiB (131.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 42.5 MiB/6.5 GiB (131.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 42.8 MiB/6.5 GiB (130.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 43.0 MiB/6.5 GiB (130.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 43.2 MiB/6.5 GiB (131.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 43.5 MiB/6.5 GiB (131.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 43.8 MiB/6.5 GiB (132.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 44.0 MiB/6.5 GiB (133.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 44.2 MiB/6.5 GiB (133.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 44.5 MiB/6.5 GiB (133.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 44.8 MiB/6.5 GiB (134.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 45.0 MiB/6.5 GiB (134.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 45.2 MiB/6.5 GiB (134.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 45.5 MiB/6.5 GiB (134.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 45.8 MiB/6.5 GiB (135.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 46.0 MiB/6.5 GiB (135.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 46.2 MiB/6.5 GiB (136.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 46.5 MiB/6.5 GiB (136.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 46.8 MiB/6.5 GiB (136.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 47.0 MiB/6.5 GiB (137.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 47.2 MiB/6.5 GiB (136.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 47.5 MiB/6.5 GiB (137.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 47.8 MiB/6.5 GiB (137.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 48.0 MiB/6.5 GiB (137.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 48.2 MiB/6.5 GiB (138.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 48.5 MiB/6.5 GiB (138.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 48.8 MiB/6.5 GiB (138.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 49.0 MiB/6.5 GiB (139.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 49.2 MiB/6.5 GiB (139.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 49.5 MiB/6.5 GiB (140.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 49.8 MiB/6.5 GiB (140.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 50.0 MiB/6.5 GiB (140.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 50.2 MiB/6.5 GiB (140.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 50.5 MiB/6.5 GiB (141.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 50.8 MiB/6.5 GiB (140.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 51.0 MiB/6.5 GiB (141.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 51.2 MiB/6.5 GiB (141.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 51.5 MiB/6.5 GiB (142.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 51.8 MiB/6.5 GiB (142.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 52.0 MiB/6.5 GiB (142.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 52.2 MiB/6.5 GiB (142.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 52.5 MiB/6.5 GiB (142.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 52.8 MiB/6.5 GiB (143.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 53.0 MiB/6.5 GiB (143.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 53.2 MiB/6.5 GiB (143.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 53.5 MiB/6.5 GiB (144.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 53.8 MiB/6.5 GiB (144.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 54.0 MiB/6.5 GiB (144.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 54.2 MiB/6.5 GiB (145.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 54.5 MiB/6.5 GiB (145.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 54.8 MiB/6.5 GiB (145.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 55.0 MiB/6.5 GiB (145.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 55.2 MiB/6.5 GiB (145.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 55.5 MiB/6.5 GiB (146.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 55.8 MiB/6.5 GiB (146.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 56.0 MiB/6.5 GiB (147.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 56.2 MiB/6.5 GiB (146.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 56.5 MiB/6.5 GiB (147.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 56.8 MiB/6.5 GiB (147.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 57.0 MiB/6.5 GiB (148.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 57.2 MiB/6.5 GiB (147.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 57.5 MiB/6.5 GiB (147.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 57.8 MiB/6.5 GiB (148.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 58.0 MiB/6.5 GiB (148.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 58.2 MiB/6.5 GiB (149.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 58.5 MiB/6.5 GiB (149.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 58.8 MiB/6.5 GiB (150.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 59.0 MiB/6.5 GiB (150.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 59.2 MiB/6.5 GiB (150.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 59.5 MiB/6.5 GiB (149.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 59.8 MiB/6.5 GiB (149.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 60.0 MiB/6.5 GiB (150.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 60.2 MiB/6.5 GiB (150.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 60.5 MiB/6.5 GiB (151.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 60.8 MiB/6.5 GiB (151.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 61.0 MiB/6.5 GiB (151.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 61.2 MiB/6.5 GiB (152.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 61.5 MiB/6.5 GiB (151.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 61.8 MiB/6.5 GiB (151.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 62.0 MiB/6.5 GiB (151.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 62.2 MiB/6.5 GiB (152.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 62.5 MiB/6.5 GiB (152.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 62.8 MiB/6.5 GiB (152.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 63.0 MiB/6.5 GiB (152.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 63.2 MiB/6.5 GiB (153.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 63.5 MiB/6.5 GiB (153.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 63.8 MiB/6.5 GiB (153.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 64.0 MiB/6.5 GiB (154.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 64.2 MiB/6.5 GiB (153.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 64.5 MiB/6.5 GiB (154.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 64.8 MiB/6.5 GiB (154.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 65.0 MiB/6.5 GiB (154.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 65.2 MiB/6.5 GiB (153.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 65.5 MiB/6.5 GiB (153.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 65.8 MiB/6.5 GiB (154.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 66.0 MiB/6.5 GiB (153.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 66.2 MiB/6.5 GiB (154.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 66.5 MiB/6.5 GiB (154.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 66.8 MiB/6.5 GiB (154.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 67.0 MiB/6.5 GiB (154.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 67.2 MiB/6.5 GiB (154.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 67.5 MiB/6.5 GiB (155.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 67.8 MiB/6.5 GiB (155.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 68.0 MiB/6.5 GiB (155.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 68.2 MiB/6.5 GiB (155.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 68.5 MiB/6.5 GiB (155.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 68.8 MiB/6.5 GiB (155.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 69.0 MiB/6.5 GiB (155.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 69.2 MiB/6.5 GiB (155.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 69.5 MiB/6.5 GiB (155.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 69.8 MiB/6.5 GiB (155.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 70.0 MiB/6.5 GiB (155.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 70.2 MiB/6.5 GiB (156.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 70.5 MiB/6.5 GiB (156.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 70.8 MiB/6.5 GiB (156.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 71.0 MiB/6.5 GiB (156.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 71.2 MiB/6.5 GiB (156.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 71.5 MiB/6.5 GiB (156.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 71.8 MiB/6.5 GiB (157.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 72.0 MiB/6.5 GiB (157.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 72.2 MiB/6.5 GiB (157.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 72.5 MiB/6.5 GiB (157.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 72.8 MiB/6.5 GiB (157.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 73.0 MiB/6.5 GiB (158.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 73.2 MiB/6.5 GiB (157.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 73.5 MiB/6.5 GiB (157.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 73.8 MiB/6.5 GiB (157.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 74.0 MiB/6.5 GiB (158.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 74.2 MiB/6.5 GiB (158.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 74.5 MiB/6.5 GiB (158.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 74.8 MiB/6.5 GiB (158.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 75.0 MiB/6.5 GiB (158.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 75.2 MiB/6.5 GiB (158.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 75.5 MiB/6.5 GiB (158.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 75.8 MiB/6.5 GiB (158.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 76.0 MiB/6.5 GiB (158.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 76.2 MiB/6.5 GiB (159.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 76.5 MiB/6.5 GiB (159.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 76.8 MiB/6.5 GiB (158.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 77.0 MiB/6.5 GiB (158.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 77.2 MiB/6.5 GiB (159.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 77.5 MiB/6.5 GiB (159.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 77.8 MiB/6.5 GiB (158.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 78.0 MiB/6.5 GiB (159.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 78.2 MiB/6.5 GiB (159.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 78.5 MiB/6.5 GiB (159.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 78.8 MiB/6.5 GiB (159.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 79.0 MiB/6.5 GiB (159.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 79.2 MiB/6.5 GiB (159.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 79.5 MiB/6.5 GiB (159.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 79.8 MiB/6.5 GiB (160.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 80.0 MiB/6.5 GiB (159.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 80.2 MiB/6.5 GiB (159.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 80.5 MiB/6.5 GiB (159.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 80.8 MiB/6.5 GiB (160.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 81.0 MiB/6.5 GiB (160.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 81.2 MiB/6.5 GiB (160.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 81.5 MiB/6.5 GiB (160.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 81.8 MiB/6.5 GiB (160.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 82.0 MiB/6.5 GiB (160.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 82.2 MiB/6.5 GiB (160.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 82.5 MiB/6.5 GiB (161.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 82.8 MiB/6.5 GiB (161.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 83.0 MiB/6.5 GiB (161.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 83.2 MiB/6.5 GiB (161.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 83.5 MiB/6.5 GiB (161.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 83.8 MiB/6.5 GiB (161.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 84.0 MiB/6.5 GiB (161.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 84.2 MiB/6.5 GiB (161.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 84.5 MiB/6.5 GiB (162.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 84.8 MiB/6.5 GiB (162.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 85.0 MiB/6.5 GiB (161.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 85.2 MiB/6.5 GiB (162.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 85.5 MiB/6.5 GiB (162.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 85.8 MiB/6.5 GiB (162.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 86.0 MiB/6.5 GiB (163.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 86.2 MiB/6.5 GiB (162.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 86.5 MiB/6.5 GiB (162.7 MiB/s) with 38 file(s) remaining\n",
       "Completed 86.8 MiB/6.5 GiB (163.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 87.0 MiB/6.5 GiB (163.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 87.2 MiB/6.5 GiB (163.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 87.5 MiB/6.5 GiB (163.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 87.8 MiB/6.5 GiB (163.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 88.0 MiB/6.5 GiB (163.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 88.2 MiB/6.5 GiB (163.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 88.5 MiB/6.5 GiB (163.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 88.8 MiB/6.5 GiB (163.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 89.0 MiB/6.5 GiB (163.4 MiB/s) with 38 file(s) remaining\n",
       "Completed 89.2 MiB/6.5 GiB (163.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 89.5 MiB/6.5 GiB (164.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 89.8 MiB/6.5 GiB (163.9 MiB/s) with 38 file(s) remaining\n",
       "Completed 90.0 MiB/6.5 GiB (163.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 90.2 MiB/6.5 GiB (164.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 90.5 MiB/6.5 GiB (164.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 90.8 MiB/6.5 GiB (164.5 MiB/s) with 38 file(s) remaining\n",
       "Completed 91.0 MiB/6.5 GiB (164.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 91.2 MiB/6.5 GiB (164.0 MiB/s) with 38 file(s) remaining\n",
       "Completed 91.5 MiB/6.5 GiB (163.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 91.8 MiB/6.5 GiB (163.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 92.0 MiB/6.5 GiB (163.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 92.2 MiB/6.5 GiB (163.8 MiB/s) with 38 file(s) remaining\n",
       "Completed 92.5 MiB/6.5 GiB (164.2 MiB/s) with 38 file(s) remaining\n",
       "Completed 92.8 MiB/6.5 GiB (164.1 MiB/s) with 38 file(s) remaining\n",
       "Completed 93.0 MiB/6.5 GiB (164.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 93.2 MiB/6.5 GiB (164.3 MiB/s) with 38 file(s) remaining\n",
       "Completed 93.5 MiB/6.5 GiB (164.6 MiB/s) with 38 file(s) remaining\n",
       "Completed 93.8 MiB/6.5 GiB (164.8 MiB/s) w\n",
       "\n",
       "*** WARNING: max output size exceeded, skipping output. ***\n",
       "\n",
       "mpleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\n",
       "upload: TEST/epoch7_trainloss_0.084_validacc_0.935.pth to s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11072022_vnet_135_cases_304_256_320/epoch7_trainloss_0.084_validacc_0.935.pth\n",
       "Completed 6.5 GiB/6.5 GiB (215.1 MiB/s) with 2 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (214.7 MiB/s) with 2 file(s) remaining\n",
       "Completed 6.5 GiB/6.5 GiB (214.6 MiB/s) with 2 file(s) remaining\n",
       "upload: TEST/epoch9_trainloss_0.055_validacc_0.945.pth to s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11072022_vnet_135_cases_304_256_320/epoch9_trainloss_0.055_validacc_0.945.pth\n",
       "Completed 6.5 GiB/6.5 GiB (214.6 MiB/s) with 1 file(s) remaining\n",
       "upload: TEST/epoch8_trainloss_0.055_validacc_0.935.pth to s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11072022_vnet_135_cases_304_256_320/epoch8_trainloss_0.055_validacc_0.935.pth\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Completed 256.0 KiB/6.5 GiB (1.8 MiB/s) with 38 file(s) remaining\nCompleted 512.0 KiB/6.5 GiB (3.5 MiB/s) with 38 file(s) remaining\nCompleted 768.0 KiB/6.5 GiB (5.1 MiB/s) with 38 file(s) remaining\nCompleted 1.0 MiB/6.5 GiB (6.6 MiB/s) with 38 file(s) remaining  \nCompleted 1.2 MiB/6.5 GiB (8.2 MiB/s) with 38 file(s) remaining  \nCompleted 1.5 MiB/6.5 GiB (9.8 MiB/s) with 38 file(s) remaining  \nCompleted 1.8 MiB/6.5 GiB (11.5 MiB/s) with 38 file(s) remaining \nCompleted 2.0 MiB/6.5 GiB (13.0 MiB/s) with 38 file(s) remaining \nCompleted 2.2 MiB/6.5 GiB (14.4 MiB/s) with 38 file(s) remaining \nCompleted 2.5 MiB/6.5 GiB (15.9 MiB/s) with 38 file(s) remaining \nCompleted 2.8 MiB/6.5 GiB (17.4 MiB/s) with 38 file(s) remaining \nCompleted 3.0 MiB/6.5 GiB (18.9 MiB/s) with 38 file(s) remaining \nCompleted 3.2 MiB/6.5 GiB (20.4 MiB/s) with 38 file(s) remaining \nCompleted 3.5 MiB/6.5 GiB (21.7 MiB/s) with 38 file(s) remaining \nCompleted 3.8 MiB/6.5 GiB (23.2 MiB/s) with 38 file(s) remaining \nCompleted 4.0 MiB/6.5 GiB (24.6 MiB/s) with 38 file(s) remaining \nCompleted 4.2 MiB/6.5 GiB (26.0 MiB/s) with 38 file(s) remaining \nCompleted 4.5 MiB/6.5 GiB (27.4 MiB/s) with 38 file(s) remaining \nCompleted 4.8 MiB/6.5 GiB (28.7 MiB/s) with 38 file(s) remaining \nCompleted 5.0 MiB/6.5 GiB (30.1 MiB/s) with 38 file(s) remaining \nCompleted 5.2 MiB/6.5 GiB (31.4 MiB/s) with 38 file(s) remaining \nCompleted 5.5 MiB/6.5 GiB (32.7 MiB/s) with 38 file(s) remaining \nCompleted 5.8 MiB/6.5 GiB (34.0 MiB/s) with 38 file(s) remaining \nCompleted 6.0 MiB/6.5 GiB (35.3 MiB/s) with 38 file(s) remaining \nCompleted 6.2 MiB/6.5 GiB (36.7 MiB/s) with 38 file(s) remaining \nCompleted 6.5 MiB/6.5 GiB (37.9 MiB/s) with 38 file(s) remaining \nCompleted 6.8 MiB/6.5 GiB (39.0 MiB/s) with 38 file(s) remaining \nCompleted 7.0 MiB/6.5 GiB (40.2 MiB/s) with 38 file(s) remaining \nCompleted 7.2 MiB/6.5 GiB (41.6 MiB/s) with 38 file(s) remaining \nCompleted 7.5 MiB/6.5 GiB (42.9 MiB/s) with 38 file(s) remaining \nCompleted 7.8 MiB/6.5 GiB (43.9 MiB/s) with 38 file(s) remaining \nCompleted 8.0 MiB/6.5 GiB (45.0 MiB/s) with 38 file(s) remaining \nCompleted 8.2 MiB/6.5 GiB (46.2 MiB/s) with 38 file(s) remaining \nCompleted 8.5 MiB/6.5 GiB (47.4 MiB/s) with 38 file(s) remaining \nCompleted 8.8 MiB/6.5 GiB (48.7 MiB/s) with 38 file(s) remaining \nCompleted 9.0 MiB/6.5 GiB (49.8 MiB/s) with 38 file(s) remaining \nCompleted 9.2 MiB/6.5 GiB (50.7 MiB/s) with 38 file(s) remaining \nCompleted 9.5 MiB/6.5 GiB (52.0 MiB/s) with 38 file(s) remaining \nCompleted 9.8 MiB/6.5 GiB (53.2 MiB/s) with 38 file(s) remaining \nCompleted 10.0 MiB/6.5 GiB (54.1 MiB/s) with 38 file(s) remaining\nCompleted 10.2 MiB/6.5 GiB (55.3 MiB/s) with 38 file(s) remaining\nCompleted 10.5 MiB/6.5 GiB (56.2 MiB/s) with 38 file(s) remaining\nCompleted 10.8 MiB/6.5 GiB (57.3 MiB/s) with 38 file(s) remaining\nCompleted 11.0 MiB/6.5 GiB (58.5 MiB/s) with 38 file(s) remaining\nCompleted 11.2 MiB/6.5 GiB (59.1 MiB/s) with 38 file(s) remaining\nCompleted 11.5 MiB/6.5 GiB (60.1 MiB/s) with 38 file(s) remaining\nCompleted 11.8 MiB/6.5 GiB (60.7 MiB/s) with 38 file(s) remaining\nCompleted 12.0 MiB/6.5 GiB (62.0 MiB/s) with 38 file(s) remaining\nCompleted 12.2 MiB/6.5 GiB (63.0 MiB/s) with 38 file(s) remaining\nCompleted 12.5 MiB/6.5 GiB (64.1 MiB/s) with 38 file(s) remaining\nCompleted 12.8 MiB/6.5 GiB (64.7 MiB/s) with 38 file(s) remaining\nCompleted 13.0 MiB/6.5 GiB (65.3 MiB/s) with 38 file(s) remaining\nCompleted 13.2 MiB/6.5 GiB (65.7 MiB/s) with 38 file(s) remaining\nCompleted 13.5 MiB/6.5 GiB (66.4 MiB/s) with 38 file(s) remaining\nCompleted 13.8 MiB/6.5 GiB (67.5 MiB/s) with 38 file(s) remaining\nCompleted 14.0 MiB/6.5 GiB (68.5 MiB/s) with 38 file(s) remaining\nCompleted 14.2 MiB/6.5 GiB (69.1 MiB/s) with 38 file(s) remaining\nCompleted 14.5 MiB/6.5 GiB (70.1 MiB/s) with 38 file(s) remaining\nCompleted 14.8 MiB/6.5 GiB (71.3 MiB/s) with 38 file(s) remaining\nCompleted 15.0 MiB/6.5 GiB (71.8 MiB/s) with 38 file(s) remaining\nCompleted 15.2 MiB/6.5 GiB (72.2 MiB/s) with 38 file(s) remaining\nCompleted 15.5 MiB/6.5 GiB (72.9 MiB/s) with 38 file(s) remaining\nCompleted 15.8 MiB/6.5 GiB (73.9 MiB/s) with 38 file(s) remaining\nCompleted 16.0 MiB/6.5 GiB (75.0 MiB/s) with 38 file(s) remaining\nCompleted 16.2 MiB/6.5 GiB (76.0 MiB/s) with 38 file(s) remaining\nCompleted 16.5 MiB/6.5 GiB (76.7 MiB/s) with 38 file(s) remaining\nCompleted 16.8 MiB/6.5 GiB (77.7 MiB/s) with 38 file(s) remaining\nCompleted 17.0 MiB/6.5 GiB (78.8 MiB/s) with 38 file(s) remaining\nCompleted 17.2 MiB/6.5 GiB (79.1 MiB/s) with 38 file(s) remaining\nCompleted 17.5 MiB/6.5 GiB (79.6 MiB/s) with 38 file(s) remaining\nCompleted 17.8 MiB/6.5 GiB (80.7 MiB/s) with 38 file(s) remaining\nCompleted 18.0 MiB/6.5 GiB (81.2 MiB/s) with 38 file(s) remaining\nCompleted 18.2 MiB/6.5 GiB (82.3 MiB/s) with 38 file(s) remaining\nCompleted 18.5 MiB/6.5 GiB (82.9 MiB/s) with 38 file(s) remaining\nCompleted 18.8 MiB/6.5 GiB (83.9 MiB/s) with 38 file(s) remaining\nCompleted 19.0 MiB/6.5 GiB (84.2 MiB/s) with 38 file(s) remaining\nCompleted 19.2 MiB/6.5 GiB (85.2 MiB/s) with 38 file(s) remaining\nCompleted 19.5 MiB/6.5 GiB (84.7 MiB/s) with 38 file(s) remaining\nCompleted 19.8 MiB/6.5 GiB (85.7 MiB/s) with 38 file(s) remaining\nCompleted 20.0 MiB/6.5 GiB (86.7 MiB/s) with 38 file(s) remaining\nCompleted 20.2 MiB/6.5 GiB (87.6 MiB/s) with 38 file(s) remaining\nCompleted 20.5 MiB/6.5 GiB (88.6 MiB/s) with 38 file(s) remaining\nCompleted 20.8 MiB/6.5 GiB (89.5 MiB/s) with 38 file(s) remaining\nCompleted 21.0 MiB/6.5 GiB (89.5 MiB/s) with 38 file(s) remaining\nCompleted 21.2 MiB/6.5 GiB (90.4 MiB/s) with 38 file(s) remaining\nCompleted 21.5 MiB/6.5 GiB (90.8 MiB/s) with 38 file(s) remaining\nCompleted 21.8 MiB/6.5 GiB (91.7 MiB/s) with 38 file(s) remaining\nCompleted 22.0 MiB/6.5 GiB (91.8 MiB/s) with 38 file(s) remaining\nCompleted 22.2 MiB/6.5 GiB (92.7 MiB/s) with 38 file(s) remaining\nCompleted 22.5 MiB/6.5 GiB (93.7 MiB/s) with 38 file(s) remaining\nCompleted 22.8 MiB/6.5 GiB (94.4 MiB/s) with 38 file(s) remaining\nCompleted 23.0 MiB/6.5 GiB (95.3 MiB/s) with 38 file(s) remaining\nCompleted 23.2 MiB/6.5 GiB (96.2 MiB/s) with 38 file(s) remaining\nCompleted 23.5 MiB/6.5 GiB (96.1 MiB/s) with 38 file(s) remaining\nCompleted 23.8 MiB/6.5 GiB (96.3 MiB/s) with 38 file(s) remaining\nCompleted 24.0 MiB/6.5 GiB (96.9 MiB/s) with 38 file(s) remaining\nCompleted 24.2 MiB/6.5 GiB (97.7 MiB/s) with 38 file(s) remaining\nCompleted 24.5 MiB/6.5 GiB (98.3 MiB/s) with 38 file(s) remaining\nCompleted 24.8 MiB/6.5 GiB (99.1 MiB/s) with 38 file(s) remaining\nCompleted 25.0 MiB/6.5 GiB (99.5 MiB/s) with 38 file(s) remaining\nCompleted 25.2 MiB/6.5 GiB (100.1 MiB/s) with 38 file(s) remaining\nCompleted 25.5 MiB/6.5 GiB (100.4 MiB/s) with 38 file(s) remaining\nCompleted 25.8 MiB/6.5 GiB (100.9 MiB/s) with 38 file(s) remaining\nCompleted 26.0 MiB/6.5 GiB (101.5 MiB/s) with 38 file(s) remaining\nCompleted 26.2 MiB/6.5 GiB (102.2 MiB/s) with 38 file(s) remaining\nCompleted 26.5 MiB/6.5 GiB (103.1 MiB/s) with 38 file(s) remaining\nCompleted 26.8 MiB/6.5 GiB (103.6 MiB/s) with 38 file(s) remaining\nCompleted 27.0 MiB/6.5 GiB (104.3 MiB/s) with 38 file(s) remaining\nCompleted 27.2 MiB/6.5 GiB (105.1 MiB/s) with 38 file(s) remaining\nCompleted 27.5 MiB/6.5 GiB (105.4 MiB/s) with 38 file(s) remaining\nCompleted 27.8 MiB/6.5 GiB (105.8 MiB/s) with 38 file(s) remaining\nCompleted 28.0 MiB/6.5 GiB (106.6 MiB/s) with 38 file(s) remaining\nCompleted 28.2 MiB/6.5 GiB (106.7 MiB/s) with 38 file(s) remaining\nCompleted 28.5 MiB/6.5 GiB (107.6 MiB/s) with 38 file(s) remaining\nCompleted 28.8 MiB/6.5 GiB (108.2 MiB/s) with 38 file(s) remaining\nCompleted 29.0 MiB/6.5 GiB (108.4 MiB/s) with 38 file(s) remaining\nCompleted 29.2 MiB/6.5 GiB (108.9 MiB/s) with 38 file(s) remaining\nCompleted 29.5 MiB/6.5 GiB (108.8 MiB/s) with 38 file(s) remaining\nCompleted 29.8 MiB/6.5 GiB (109.7 MiB/s) with 38 file(s) remaining\nCompleted 30.0 MiB/6.5 GiB (110.3 MiB/s) with 38 file(s) remaining\nCompleted 30.2 MiB/6.5 GiB (111.2 MiB/s) with 38 file(s) remaining\nCompleted 30.5 MiB/6.5 GiB (111.7 MiB/s) with 38 file(s) remaining\nCompleted 30.8 MiB/6.5 GiB (112.1 MiB/s) with 38 file(s) remaining\nCompleted 31.0 MiB/6.5 GiB (112.9 MiB/s) with 38 file(s) remaining\nCompleted 31.2 MiB/6.5 GiB (113.4 MiB/s) with 38 file(s) remaining\nCompleted 31.5 MiB/6.5 GiB (113.1 MiB/s) with 38 file(s) remaining\nCompleted 31.8 MiB/6.5 GiB (113.9 MiB/s) with 38 file(s) remaining\nCompleted 32.0 MiB/6.5 GiB (114.2 MiB/s) with 38 file(s) remaining\nCompleted 32.2 MiB/6.5 GiB (114.5 MiB/s) with 38 file(s) remaining\nCompleted 32.5 MiB/6.5 GiB (115.2 MiB/s) with 38 file(s) remaining\nCompleted 32.8 MiB/6.5 GiB (116.0 MiB/s) with 38 file(s) remaining\nCompleted 33.0 MiB/6.5 GiB (116.1 MiB/s) with 38 file(s) remaining\nCompleted 33.2 MiB/6.5 GiB (116.9 MiB/s) with 38 file(s) remaining\nCompleted 33.5 MiB/6.5 GiB (117.3 MiB/s) with 38 file(s) remaining\nCompleted 33.8 MiB/6.5 GiB (117.1 MiB/s) with 38 file(s) remaining\nCompleted 34.0 MiB/6.5 GiB (117.2 MiB/s) with 38 file(s) remaining\nCompleted 34.2 MiB/6.5 GiB (118.0 MiB/s) with 38 file(s) remaining\nCompleted 34.5 MiB/6.5 GiB (118.6 MiB/s) with 38 file(s) remaining\nCompleted 34.8 MiB/6.5 GiB (119.3 MiB/s) with 38 file(s) remaining\nCompleted 35.0 MiB/6.5 GiB (119.9 MiB/s) with 38 file(s) remaining\nCompleted 35.2 MiB/6.5 GiB (120.7 MiB/s) with 38 file(s) remaining\nCompleted 35.5 MiB/6.5 GiB (120.6 MiB/s) with 38 file(s) remaining\nCompleted 35.8 MiB/6.5 GiB (121.0 MiB/s) with 38 file(s) remaining\nCompleted 36.0 MiB/6.5 GiB (120.3 MiB/s) with 38 file(s) remaining\nCompleted 36.2 MiB/6.5 GiB (120.8 MiB/s) with 38 file(s) remaining\nCompleted 36.5 MiB/6.5 GiB (121.5 MiB/s) with 38 file(s) remaining\nCompleted 36.8 MiB/6.5 GiB (122.3 MiB/s) with 38 file(s) remaining\nCompleted 37.0 MiB/6.5 GiB (123.0 MiB/s) with 38 file(s) remaining\nCompleted 37.2 MiB/6.5 GiB (123.5 MiB/s) with 38 file(s) remaining\nCompleted 37.5 MiB/6.5 GiB (123.9 MiB/s) with 38 file(s) remaining\nCompleted 37.8 MiB/6.5 GiB (124.3 MiB/s) with 38 file(s) remaining\nCompleted 38.0 MiB/6.5 GiB (125.1 MiB/s) with 38 file(s) remaining\nCompleted 38.2 MiB/6.5 GiB (123.9 MiB/s) with 38 file(s) remaining\nCompleted 38.5 MiB/6.5 GiB (124.4 MiB/s) with 38 file(s) remaining\nCompleted 38.8 MiB/6.5 GiB (125.1 MiB/s) with 38 file(s) remaining\nCompleted 39.0 MiB/6.5 GiB (125.9 MiB/s) with 38 file(s) remaining\nCompleted 39.2 MiB/6.5 GiB (126.4 MiB/s) with 38 file(s) remaining\nCompleted 39.5 MiB/6.5 GiB (126.9 MiB/s) with 38 file(s) remaining\nCompleted 39.8 MiB/6.5 GiB (127.6 MiB/s) with 38 file(s) remaining\nCompleted 40.0 MiB/6.5 GiB (127.9 MiB/s) with 38 file(s) remaining\nCompleted 40.2 MiB/6.5 GiB (128.6 MiB/s) with 38 file(s) remaining\nCompleted 40.5 MiB/6.5 GiB (127.8 MiB/s) with 38 file(s) remaining\nCompleted 40.8 MiB/6.5 GiB (127.7 MiB/s) with 38 file(s) remaining\nCompleted 41.0 MiB/6.5 GiB (128.4 MiB/s) with 38 file(s) remaining\nCompleted 41.2 MiB/6.5 GiB (129.2 MiB/s) with 38 file(s) remaining\nCompleted 41.5 MiB/6.5 GiB (129.5 MiB/s) with 38 file(s) remaining\nCompleted 41.8 MiB/6.5 GiB (130.0 MiB/s) with 38 file(s) remaining\nCompleted 42.0 MiB/6.5 GiB (130.4 MiB/s) with 38 file(s) remaining\nCompleted 42.2 MiB/6.5 GiB (131.2 MiB/s) with 38 file(s) remaining\nCompleted 42.5 MiB/6.5 GiB (131.2 MiB/s) with 38 file(s) remaining\nCompleted 42.8 MiB/6.5 GiB (130.4 MiB/s) with 38 file(s) remaining\nCompleted 43.0 MiB/6.5 GiB (130.7 MiB/s) with 38 file(s) remaining\nCompleted 43.2 MiB/6.5 GiB (131.4 MiB/s) with 38 file(s) remaining\nCompleted 43.5 MiB/6.5 GiB (131.9 MiB/s) with 38 file(s) remaining\nCompleted 43.8 MiB/6.5 GiB (132.5 MiB/s) with 38 file(s) remaining\nCompleted 44.0 MiB/6.5 GiB (133.1 MiB/s) with 38 file(s) remaining\nCompleted 44.2 MiB/6.5 GiB (133.6 MiB/s) with 38 file(s) remaining\nCompleted 44.5 MiB/6.5 GiB (133.9 MiB/s) with 38 file(s) remaining\nCompleted 44.8 MiB/6.5 GiB (134.0 MiB/s) with 38 file(s) remaining\nCompleted 45.0 MiB/6.5 GiB (134.5 MiB/s) with 38 file(s) remaining\nCompleted 45.2 MiB/6.5 GiB (134.1 MiB/s) with 38 file(s) remaining\nCompleted 45.5 MiB/6.5 GiB (134.5 MiB/s) with 38 file(s) remaining\nCompleted 45.8 MiB/6.5 GiB (135.1 MiB/s) with 38 file(s) remaining\nCompleted 46.0 MiB/6.5 GiB (135.7 MiB/s) with 38 file(s) remaining\nCompleted 46.2 MiB/6.5 GiB (136.1 MiB/s) with 38 file(s) remaining\nCompleted 46.5 MiB/6.5 GiB (136.2 MiB/s) with 38 file(s) remaining\nCompleted 46.8 MiB/6.5 GiB (136.8 MiB/s) with 38 file(s) remaining\nCompleted 47.0 MiB/6.5 GiB (137.0 MiB/s) with 38 file(s) remaining\nCompleted 47.2 MiB/6.5 GiB (136.9 MiB/s) with 38 file(s) remaining\nCompleted 47.5 MiB/6.5 GiB (137.0 MiB/s) with 38 file(s) remaining\nCompleted 47.8 MiB/6.5 GiB (137.3 MiB/s) with 38 file(s) remaining\nCompleted 48.0 MiB/6.5 GiB (137.8 MiB/s) with 38 file(s) remaining\nCompleted 48.2 MiB/6.5 GiB (138.3 MiB/s) with 38 file(s) remaining\nCompleted 48.5 MiB/6.5 GiB (138.0 MiB/s) with 38 file(s) remaining\nCompleted 48.8 MiB/6.5 GiB (138.7 MiB/s) with 38 file(s) remaining\nCompleted 49.0 MiB/6.5 GiB (139.3 MiB/s) with 38 file(s) remaining\nCompleted 49.2 MiB/6.5 GiB (139.6 MiB/s) with 38 file(s) remaining\nCompleted 49.5 MiB/6.5 GiB (140.0 MiB/s) with 38 file(s) remaining\nCompleted 49.8 MiB/6.5 GiB (140.7 MiB/s) with 38 file(s) remaining\nCompleted 50.0 MiB/6.5 GiB (140.1 MiB/s) with 38 file(s) remaining\nCompleted 50.2 MiB/6.5 GiB (140.6 MiB/s) with 38 file(s) remaining\nCompleted 50.5 MiB/6.5 GiB (141.0 MiB/s) with 38 file(s) remaining\nCompleted 50.8 MiB/6.5 GiB (140.6 MiB/s) with 38 file(s) remaining\nCompleted 51.0 MiB/6.5 GiB (141.2 MiB/s) with 38 file(s) remaining\nCompleted 51.2 MiB/6.5 GiB (141.7 MiB/s) with 38 file(s) remaining\nCompleted 51.5 MiB/6.5 GiB (142.3 MiB/s) with 38 file(s) remaining\nCompleted 51.8 MiB/6.5 GiB (142.6 MiB/s) with 38 file(s) remaining\nCompleted 52.0 MiB/6.5 GiB (142.8 MiB/s) with 38 file(s) remaining\nCompleted 52.2 MiB/6.5 GiB (142.0 MiB/s) with 38 file(s) remaining\nCompleted 52.5 MiB/6.5 GiB (142.4 MiB/s) with 38 file(s) remaining\nCompleted 52.8 MiB/6.5 GiB (143.0 MiB/s) with 38 file(s) remaining\nCompleted 53.0 MiB/6.5 GiB (143.5 MiB/s) with 38 file(s) remaining\nCompleted 53.2 MiB/6.5 GiB (143.9 MiB/s) with 38 file(s) remaining\nCompleted 53.5 MiB/6.5 GiB (144.5 MiB/s) with 38 file(s) remaining\nCompleted 53.8 MiB/6.5 GiB (144.9 MiB/s) with 38 file(s) remaining\nCompleted 54.0 MiB/6.5 GiB (144.8 MiB/s) with 38 file(s) remaining\nCompleted 54.2 MiB/6.5 GiB (145.1 MiB/s) with 38 file(s) remaining\nCompleted 54.5 MiB/6.5 GiB (145.2 MiB/s) with 38 file(s) remaining\nCompleted 54.8 MiB/6.5 GiB (145.3 MiB/s) with 38 file(s) remaining\nCompleted 55.0 MiB/6.5 GiB (145.5 MiB/s) with 38 file(s) remaining\nCompleted 55.2 MiB/6.5 GiB (145.8 MiB/s) with 38 file(s) remaining\nCompleted 55.5 MiB/6.5 GiB (146.2 MiB/s) with 38 file(s) remaining\nCompleted 55.8 MiB/6.5 GiB (146.6 MiB/s) with 38 file(s) remaining\nCompleted 56.0 MiB/6.5 GiB (147.1 MiB/s) with 38 file(s) remaining\nCompleted 56.2 MiB/6.5 GiB (146.6 MiB/s) with 38 file(s) remaining\nCompleted 56.5 MiB/6.5 GiB (147.2 MiB/s) with 38 file(s) remaining\nCompleted 56.8 MiB/6.5 GiB (147.4 MiB/s) with 38 file(s) remaining\nCompleted 57.0 MiB/6.5 GiB (148.0 MiB/s) with 38 file(s) remaining\nCompleted 57.2 MiB/6.5 GiB (147.4 MiB/s) with 38 file(s) remaining\nCompleted 57.5 MiB/6.5 GiB (147.8 MiB/s) with 38 file(s) remaining\nCompleted 57.8 MiB/6.5 GiB (148.4 MiB/s) with 38 file(s) remaining\nCompleted 58.0 MiB/6.5 GiB (148.5 MiB/s) with 38 file(s) remaining\nCompleted 58.2 MiB/6.5 GiB (149.0 MiB/s) with 38 file(s) remaining\nCompleted 58.5 MiB/6.5 GiB (149.7 MiB/s) with 38 file(s) remaining\nCompleted 58.8 MiB/6.5 GiB (150.0 MiB/s) with 38 file(s) remaining\nCompleted 59.0 MiB/6.5 GiB (150.5 MiB/s) with 38 file(s) remaining\nCompleted 59.2 MiB/6.5 GiB (150.3 MiB/s) with 38 file(s) remaining\nCompleted 59.5 MiB/6.5 GiB (149.6 MiB/s) with 38 file(s) remaining\nCompleted 59.8 MiB/6.5 GiB (149.8 MiB/s) with 38 file(s) remaining\nCompleted 60.0 MiB/6.5 GiB (150.1 MiB/s) with 38 file(s) remaining\nCompleted 60.2 MiB/6.5 GiB (150.5 MiB/s) with 38 file(s) remaining\nCompleted 60.5 MiB/6.5 GiB (151.1 MiB/s) with 38 file(s) remaining\nCompleted 60.8 MiB/6.5 GiB (151.2 MiB/s) with 38 file(s) remaining\nCompleted 61.0 MiB/6.5 GiB (151.7 MiB/s) with 38 file(s) remaining\nCompleted 61.2 MiB/6.5 GiB (152.0 MiB/s) with 38 file(s) remaining\nCompleted 61.5 MiB/6.5 GiB (151.3 MiB/s) with 38 file(s) remaining\nCompleted 61.8 MiB/6.5 GiB (151.3 MiB/s) with 38 file(s) remaining\nCompleted 62.0 MiB/6.5 GiB (151.9 MiB/s) with 38 file(s) remaining\nCompleted 62.2 MiB/6.5 GiB (152.3 MiB/s) with 38 file(s) remaining\nCompleted 62.5 MiB/6.5 GiB (152.8 MiB/s) with 38 file(s) remaining\nCompleted 62.8 MiB/6.5 GiB (152.0 MiB/s) with 38 file(s) remaining\nCompleted 63.0 MiB/6.5 GiB (152.6 MiB/s) with 38 file(s) remaining\nCompleted 63.2 MiB/6.5 GiB (153.0 MiB/s) with 38 file(s) remaining\nCompleted 63.5 MiB/6.5 GiB (153.1 MiB/s) with 38 file(s) remaining\nCompleted 63.8 MiB/6.5 GiB (153.6 MiB/s) with 38 file(s) remaining\nCompleted 64.0 MiB/6.5 GiB (154.1 MiB/s) with 38 file(s) remaining\nCompleted 64.2 MiB/6.5 GiB (153.4 MiB/s) with 38 file(s) remaining\nCompleted 64.5 MiB/6.5 GiB (154.0 MiB/s) with 38 file(s) remaining\nCompleted 64.8 MiB/6.5 GiB (154.5 MiB/s) with 38 file(s) remaining\nCompleted 65.0 MiB/6.5 GiB (154.4 MiB/s) with 38 file(s) remaining\nCompleted 65.2 MiB/6.5 GiB (153.6 MiB/s) with 38 file(s) remaining\nCompleted 65.5 MiB/6.5 GiB (153.6 MiB/s) with 38 file(s) remaining\nCompleted 65.8 MiB/6.5 GiB (154.1 MiB/s) with 38 file(s) remaining\nCompleted 66.0 MiB/6.5 GiB (153.9 MiB/s) with 38 file(s) remaining\nCompleted 66.2 MiB/6.5 GiB (154.2 MiB/s) with 38 file(s) remaining\nCompleted 66.5 MiB/6.5 GiB (154.7 MiB/s) with 38 file(s) remaining\nCompleted 66.8 MiB/6.5 GiB (154.4 MiB/s) with 38 file(s) remaining\nCompleted 67.0 MiB/6.5 GiB (154.6 MiB/s) with 38 file(s) remaining\nCompleted 67.2 MiB/6.5 GiB (154.8 MiB/s) with 38 file(s) remaining\nCompleted 67.5 MiB/6.5 GiB (155.3 MiB/s) with 38 file(s) remaining\nCompleted 67.8 MiB/6.5 GiB (155.1 MiB/s) with 38 file(s) remaining\nCompleted 68.0 MiB/6.5 GiB (155.5 MiB/s) with 38 file(s) remaining\nCompleted 68.2 MiB/6.5 GiB (155.5 MiB/s) with 38 file(s) remaining\nCompleted 68.5 MiB/6.5 GiB (155.2 MiB/s) with 38 file(s) remaining\nCompleted 68.8 MiB/6.5 GiB (155.7 MiB/s) with 38 file(s) remaining\nCompleted 69.0 MiB/6.5 GiB (155.5 MiB/s) with 38 file(s) remaining\nCompleted 69.2 MiB/6.5 GiB (155.3 MiB/s) with 38 file(s) remaining\nCompleted 69.5 MiB/6.5 GiB (155.6 MiB/s) with 38 file(s) remaining\nCompleted 69.8 MiB/6.5 GiB (155.6 MiB/s) with 38 file(s) remaining\nCompleted 70.0 MiB/6.5 GiB (155.9 MiB/s) with 38 file(s) remaining\nCompleted 70.2 MiB/6.5 GiB (156.0 MiB/s) with 38 file(s) remaining\nCompleted 70.5 MiB/6.5 GiB (156.4 MiB/s) with 38 file(s) remaining\nCompleted 70.8 MiB/6.5 GiB (156.7 MiB/s) with 38 file(s) remaining\nCompleted 71.0 MiB/6.5 GiB (156.4 MiB/s) with 38 file(s) remaining\nCompleted 71.2 MiB/6.5 GiB (156.7 MiB/s) with 38 file(s) remaining\nCompleted 71.5 MiB/6.5 GiB (156.7 MiB/s) with 38 file(s) remaining\nCompleted 71.8 MiB/6.5 GiB (157.0 MiB/s) with 38 file(s) remaining\nCompleted 72.0 MiB/6.5 GiB (157.4 MiB/s) with 38 file(s) remaining\nCompleted 72.2 MiB/6.5 GiB (157.1 MiB/s) with 38 file(s) remaining\nCompleted 72.5 MiB/6.5 GiB (157.5 MiB/s) with 38 file(s) remaining\nCompleted 72.8 MiB/6.5 GiB (157.9 MiB/s) with 38 file(s) remaining\nCompleted 73.0 MiB/6.5 GiB (158.1 MiB/s) with 38 file(s) remaining\nCompleted 73.2 MiB/6.5 GiB (157.6 MiB/s) with 38 file(s) remaining\nCompleted 73.5 MiB/6.5 GiB (157.7 MiB/s) with 38 file(s) remaining\nCompleted 73.8 MiB/6.5 GiB (157.8 MiB/s) with 38 file(s) remaining\nCompleted 74.0 MiB/6.5 GiB (158.2 MiB/s) with 38 file(s) remaining\nCompleted 74.2 MiB/6.5 GiB (158.6 MiB/s) with 38 file(s) remaining\nCompleted 74.5 MiB/6.5 GiB (158.4 MiB/s) with 38 file(s) remaining\nCompleted 74.8 MiB/6.5 GiB (158.3 MiB/s) with 38 file(s) remaining\nCompleted 75.0 MiB/6.5 GiB (158.3 MiB/s) with 38 file(s) remaining\nCompleted 75.2 MiB/6.5 GiB (158.8 MiB/s) with 38 file(s) remaining\nCompleted 75.5 MiB/6.5 GiB (158.8 MiB/s) with 38 file(s) remaining\nCompleted 75.8 MiB/6.5 GiB (158.6 MiB/s) with 38 file(s) remaining\nCompleted 76.0 MiB/6.5 GiB (158.6 MiB/s) with 38 file(s) remaining\nCompleted 76.2 MiB/6.5 GiB (159.0 MiB/s) with 38 file(s) remaining\nCompleted 76.5 MiB/6.5 GiB (159.4 MiB/s) with 38 file(s) remaining\nCompleted 76.8 MiB/6.5 GiB (158.9 MiB/s) with 38 file(s) remaining\nCompleted 77.0 MiB/6.5 GiB (158.7 MiB/s) with 38 file(s) remaining\nCompleted 77.2 MiB/6.5 GiB (159.0 MiB/s) with 38 file(s) remaining\nCompleted 77.5 MiB/6.5 GiB (159.4 MiB/s) with 38 file(s) remaining\nCompleted 77.8 MiB/6.5 GiB (158.9 MiB/s) with 38 file(s) remaining\nCompleted 78.0 MiB/6.5 GiB (159.0 MiB/s) with 38 file(s) remaining\nCompleted 78.2 MiB/6.5 GiB (159.3 MiB/s) with 38 file(s) remaining\nCompleted 78.5 MiB/6.5 GiB (159.5 MiB/s) with 38 file(s) remaining\nCompleted 78.8 MiB/6.5 GiB (159.6 MiB/s) with 38 file(s) remaining\nCompleted 79.0 MiB/6.5 GiB (159.4 MiB/s) with 38 file(s) remaining\nCompleted 79.2 MiB/6.5 GiB (159.8 MiB/s) with 38 file(s) remaining\nCompleted 79.5 MiB/6.5 GiB (159.7 MiB/s) with 38 file(s) remaining\nCompleted 79.8 MiB/6.5 GiB (160.2 MiB/s) with 38 file(s) remaining\nCompleted 80.0 MiB/6.5 GiB (159.4 MiB/s) with 38 file(s) remaining\nCompleted 80.2 MiB/6.5 GiB (159.6 MiB/s) with 38 file(s) remaining\nCompleted 80.5 MiB/6.5 GiB (159.7 MiB/s) with 38 file(s) remaining\nCompleted 80.8 MiB/6.5 GiB (160.0 MiB/s) with 38 file(s) remaining\nCompleted 81.0 MiB/6.5 GiB (160.4 MiB/s) with 38 file(s) remaining\nCompleted 81.2 MiB/6.5 GiB (160.1 MiB/s) with 38 file(s) remaining\nCompleted 81.5 MiB/6.5 GiB (160.3 MiB/s) with 38 file(s) remaining\nCompleted 81.8 MiB/6.5 GiB (160.5 MiB/s) with 38 file(s) remaining\nCompleted 82.0 MiB/6.5 GiB (160.8 MiB/s) with 38 file(s) remaining\nCompleted 82.2 MiB/6.5 GiB (160.8 MiB/s) with 38 file(s) remaining\nCompleted 82.5 MiB/6.5 GiB (161.2 MiB/s) with 38 file(s) remaining\nCompleted 82.8 MiB/6.5 GiB (161.1 MiB/s) with 38 file(s) remaining\nCompleted 83.0 MiB/6.5 GiB (161.0 MiB/s) with 38 file(s) remaining\nCompleted 83.2 MiB/6.5 GiB (161.3 MiB/s) with 38 file(s) remaining\nCompleted 83.5 MiB/6.5 GiB (161.6 MiB/s) with 38 file(s) remaining\nCompleted 83.8 MiB/6.5 GiB (161.3 MiB/s) with 38 file(s) remaining\nCompleted 84.0 MiB/6.5 GiB (161.4 MiB/s) with 38 file(s) remaining\nCompleted 84.2 MiB/6.5 GiB (161.7 MiB/s) with 38 file(s) remaining\nCompleted 84.5 MiB/6.5 GiB (162.1 MiB/s) with 38 file(s) remaining\nCompleted 84.8 MiB/6.5 GiB (162.5 MiB/s) with 38 file(s) remaining\nCompleted 85.0 MiB/6.5 GiB (161.7 MiB/s) with 38 file(s) remaining\nCompleted 85.2 MiB/6.5 GiB (162.2 MiB/s) with 38 file(s) remaining\nCompleted 85.5 MiB/6.5 GiB (162.5 MiB/s) with 38 file(s) remaining\nCompleted 85.8 MiB/6.5 GiB (162.7 MiB/s) with 38 file(s) remaining\nCompleted 86.0 MiB/6.5 GiB (163.0 MiB/s) with 38 file(s) remaining\nCompleted 86.2 MiB/6.5 GiB (162.4 MiB/s) with 38 file(s) remaining\nCompleted 86.5 MiB/6.5 GiB (162.7 MiB/s) with 38 file(s) remaining\nCompleted 86.8 MiB/6.5 GiB (163.2 MiB/s) with 38 file(s) remaining\nCompleted 87.0 MiB/6.5 GiB (163.2 MiB/s) with 38 file(s) remaining\nCompleted 87.2 MiB/6.5 GiB (163.5 MiB/s) with 38 file(s) remaining\nCompleted 87.5 MiB/6.5 GiB (163.9 MiB/s) with 38 file(s) remaining\nCompleted 87.8 MiB/6.5 GiB (163.3 MiB/s) with 38 file(s) remaining\nCompleted 88.0 MiB/6.5 GiB (163.6 MiB/s) with 38 file(s) remaining\nCompleted 88.2 MiB/6.5 GiB (163.5 MiB/s) with 38 file(s) remaining\nCompleted 88.5 MiB/6.5 GiB (163.4 MiB/s) with 38 file(s) remaining\nCompleted 88.8 MiB/6.5 GiB (163.2 MiB/s) with 38 file(s) remaining\nCompleted 89.0 MiB/6.5 GiB (163.4 MiB/s) with 38 file(s) remaining\nCompleted 89.2 MiB/6.5 GiB (163.5 MiB/s) with 38 file(s) remaining\nCompleted 89.5 MiB/6.5 GiB (164.0 MiB/s) with 38 file(s) remaining\nCompleted 89.8 MiB/6.5 GiB (163.9 MiB/s) with 38 file(s) remaining\nCompleted 90.0 MiB/6.5 GiB (163.6 MiB/s) with 38 file(s) remaining\nCompleted 90.2 MiB/6.5 GiB (164.0 MiB/s) with 38 file(s) remaining\nCompleted 90.5 MiB/6.5 GiB (164.2 MiB/s) with 38 file(s) remaining\nCompleted 90.8 MiB/6.5 GiB (164.5 MiB/s) with 38 file(s) remaining\nCompleted 91.0 MiB/6.5 GiB (164.1 MiB/s) with 38 file(s) remaining\nCompleted 91.2 MiB/6.5 GiB (164.0 MiB/s) with 38 file(s) remaining\nCompleted 91.5 MiB/6.5 GiB (163.8 MiB/s) with 38 file(s) remaining\nCompleted 91.8 MiB/6.5 GiB (163.6 MiB/s) with 38 file(s) remaining\nCompleted 92.0 MiB/6.5 GiB (163.6 MiB/s) with 38 file(s) remaining\nCompleted 92.2 MiB/6.5 GiB (163.8 MiB/s) with 38 file(s) remaining\nCompleted 92.5 MiB/6.5 GiB (164.2 MiB/s) with 38 file(s) remaining\nCompleted 92.8 MiB/6.5 GiB (164.1 MiB/s) with 38 file(s) remaining\nCompleted 93.0 MiB/6.5 GiB (164.3 MiB/s) with 38 file(s) remaining\nCompleted 93.2 MiB/6.5 GiB (164.3 MiB/s) with 38 file(s) remaining\nCompleted 93.5 MiB/6.5 GiB (164.6 MiB/s) with 38 file(s) remaining\nCompleted 93.8 MiB/6.5 GiB (164.8 MiB/s) w\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nmpleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.0 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.4 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 3 file(s) remaining\nupload: TEST/epoch7_trainloss_0.084_validacc_0.935.pth to s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11072022_vnet_135_cases_304_256_320/epoch7_trainloss_0.084_validacc_0.935.pth\nCompleted 6.5 GiB/6.5 GiB (215.1 MiB/s) with 2 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (214.7 MiB/s) with 2 file(s) remaining\nCompleted 6.5 GiB/6.5 GiB (214.6 MiB/s) with 2 file(s) remaining\nupload: TEST/epoch9_trainloss_0.055_validacc_0.945.pth to s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11072022_vnet_135_cases_304_256_320/epoch9_trainloss_0.055_validacc_0.945.pth\nCompleted 6.5 GiB/6.5 GiB (214.6 MiB/s) with 1 file(s) remaining\nupload: TEST/epoch8_trainloss_0.055_validacc_0.935.pth to s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11072022_vnet_135_cases_304_256_320/epoch8_trainloss_0.055_validacc_0.935.pth\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "/databricks/python/bin/aws s3 cp --recursive /databricks/driver/TEST s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11072022_vnet_135_cases_304_256_320/ --sse --acl bucket-owner-full-control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c04e61c9-4131-4fa2-afbc-3ac93c181fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7113c9-a68f-48bb-92ab-fd348f648e09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "/databricks/python/bin/aws s3 cp /databricks/driver/TEST/weights.024-0.15-0.9238.hdf5 s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_fat/11022022_vnet_130_cases_240_256_320/409_10_slicing_240_176_224_0_32_weights.024-0.15-0.9238.hdf5 --sse --acl bucket-owner-full-control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a8163ec-055e-4d13-bc08-9fff2dbbc0f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "--- copy weights from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "592fee60-f0bd-48f5-952a-2f2c2d31e3c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completed 256.0 KiB/174.0 MiB (1.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 512.0 KiB/174.0 MiB (2.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 768.0 KiB/174.0 MiB (3.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 1.0 MiB/174.0 MiB (4.6 MiB/s) with 1 file(s) remaining  \n",
       "Completed 1.2 MiB/174.0 MiB (5.7 MiB/s) with 1 file(s) remaining  \n",
       "Completed 1.5 MiB/174.0 MiB (6.8 MiB/s) with 1 file(s) remaining  \n",
       "Completed 1.8 MiB/174.0 MiB (7.9 MiB/s) with 1 file(s) remaining  \n",
       "Completed 2.0 MiB/174.0 MiB (8.9 MiB/s) with 1 file(s) remaining  \n",
       "Completed 2.2 MiB/174.0 MiB (10.0 MiB/s) with 1 file(s) remaining \n",
       "Completed 2.5 MiB/174.0 MiB (10.9 MiB/s) with 1 file(s) remaining \n",
       "Completed 2.8 MiB/174.0 MiB (12.0 MiB/s) with 1 file(s) remaining \n",
       "Completed 3.0 MiB/174.0 MiB (12.8 MiB/s) with 1 file(s) remaining \n",
       "Completed 3.2 MiB/174.0 MiB (13.9 MiB/s) with 1 file(s) remaining \n",
       "Completed 3.5 MiB/174.0 MiB (14.8 MiB/s) with 1 file(s) remaining \n",
       "Completed 3.8 MiB/174.0 MiB (15.7 MiB/s) with 1 file(s) remaining \n",
       "Completed 4.0 MiB/174.0 MiB (16.7 MiB/s) with 1 file(s) remaining \n",
       "Completed 4.2 MiB/174.0 MiB (17.6 MiB/s) with 1 file(s) remaining \n",
       "Completed 4.5 MiB/174.0 MiB (18.6 MiB/s) with 1 file(s) remaining \n",
       "Completed 4.8 MiB/174.0 MiB (19.5 MiB/s) with 1 file(s) remaining \n",
       "Completed 5.0 MiB/174.0 MiB (20.4 MiB/s) with 1 file(s) remaining \n",
       "Completed 5.2 MiB/174.0 MiB (21.4 MiB/s) with 1 file(s) remaining \n",
       "Completed 5.5 MiB/174.0 MiB (22.2 MiB/s) with 1 file(s) remaining \n",
       "Completed 5.8 MiB/174.0 MiB (22.9 MiB/s) with 1 file(s) remaining \n",
       "Completed 6.0 MiB/174.0 MiB (23.8 MiB/s) with 1 file(s) remaining \n",
       "Completed 6.2 MiB/174.0 MiB (24.7 MiB/s) with 1 file(s) remaining \n",
       "Completed 6.5 MiB/174.0 MiB (25.7 MiB/s) with 1 file(s) remaining \n",
       "Completed 6.8 MiB/174.0 MiB (26.4 MiB/s) with 1 file(s) remaining \n",
       "Completed 7.0 MiB/174.0 MiB (27.2 MiB/s) with 1 file(s) remaining \n",
       "Completed 7.2 MiB/174.0 MiB (28.0 MiB/s) with 1 file(s) remaining \n",
       "Completed 7.5 MiB/174.0 MiB (28.9 MiB/s) with 1 file(s) remaining \n",
       "Completed 7.8 MiB/174.0 MiB (29.8 MiB/s) with 1 file(s) remaining \n",
       "Completed 8.0 MiB/174.0 MiB (30.6 MiB/s) with 1 file(s) remaining \n",
       "Completed 8.2 MiB/174.0 MiB (31.2 MiB/s) with 1 file(s) remaining \n",
       "Completed 8.5 MiB/174.0 MiB (32.1 MiB/s) with 1 file(s) remaining \n",
       "Completed 8.8 MiB/174.0 MiB (32.9 MiB/s) with 1 file(s) remaining \n",
       "Completed 9.0 MiB/174.0 MiB (33.5 MiB/s) with 1 file(s) remaining \n",
       "Completed 9.2 MiB/174.0 MiB (34.3 MiB/s) with 1 file(s) remaining \n",
       "Completed 9.5 MiB/174.0 MiB (35.2 MiB/s) with 1 file(s) remaining \n",
       "Completed 9.8 MiB/174.0 MiB (36.0 MiB/s) with 1 file(s) remaining \n",
       "Completed 10.0 MiB/174.0 MiB (36.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 10.2 MiB/174.0 MiB (37.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 10.5 MiB/174.0 MiB (37.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 10.8 MiB/174.0 MiB (38.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 11.0 MiB/174.0 MiB (39.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 11.2 MiB/174.0 MiB (39.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 11.5 MiB/174.0 MiB (40.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 11.8 MiB/174.0 MiB (41.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 12.0 MiB/174.0 MiB (41.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 12.2 MiB/174.0 MiB (42.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 12.5 MiB/174.0 MiB (42.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 12.8 MiB/174.0 MiB (43.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 13.0 MiB/174.0 MiB (44.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 13.2 MiB/174.0 MiB (45.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 13.5 MiB/174.0 MiB (45.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 13.8 MiB/174.0 MiB (46.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 14.0 MiB/174.0 MiB (46.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 14.2 MiB/174.0 MiB (47.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 14.5 MiB/174.0 MiB (47.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 14.8 MiB/174.0 MiB (48.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 15.0 MiB/174.0 MiB (49.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 15.2 MiB/174.0 MiB (49.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 15.5 MiB/174.0 MiB (50.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 15.8 MiB/174.0 MiB (51.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 16.0 MiB/174.0 MiB (51.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 16.2 MiB/174.0 MiB (51.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 16.5 MiB/174.0 MiB (52.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 16.8 MiB/174.0 MiB (52.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 17.0 MiB/174.0 MiB (53.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 17.2 MiB/174.0 MiB (54.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 17.5 MiB/174.0 MiB (54.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 17.8 MiB/174.0 MiB (55.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 18.0 MiB/174.0 MiB (55.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 18.2 MiB/174.0 MiB (56.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 18.5 MiB/174.0 MiB (56.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 18.8 MiB/174.0 MiB (57.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 19.0 MiB/174.0 MiB (57.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 19.2 MiB/174.0 MiB (58.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 19.5 MiB/174.0 MiB (58.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 19.8 MiB/174.0 MiB (59.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 20.0 MiB/174.0 MiB (59.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 20.2 MiB/174.0 MiB (60.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 20.5 MiB/174.0 MiB (61.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 20.8 MiB/174.0 MiB (61.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 21.0 MiB/174.0 MiB (61.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 21.2 MiB/174.0 MiB (62.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 21.5 MiB/174.0 MiB (62.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 21.8 MiB/174.0 MiB (63.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 22.0 MiB/174.0 MiB (63.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 22.2 MiB/174.0 MiB (64.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 22.5 MiB/174.0 MiB (64.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 22.8 MiB/174.0 MiB (65.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 23.0 MiB/174.0 MiB (65.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 23.2 MiB/174.0 MiB (66.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 23.5 MiB/174.0 MiB (66.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 23.8 MiB/174.0 MiB (67.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 24.0 MiB/174.0 MiB (67.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 24.2 MiB/174.0 MiB (68.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 24.5 MiB/174.0 MiB (68.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 24.8 MiB/174.0 MiB (69.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 25.0 MiB/174.0 MiB (69.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 25.2 MiB/174.0 MiB (70.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 25.5 MiB/174.0 MiB (70.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 25.8 MiB/174.0 MiB (70.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 26.0 MiB/174.0 MiB (71.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 26.2 MiB/174.0 MiB (71.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 26.5 MiB/174.0 MiB (71.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 26.8 MiB/174.0 MiB (72.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 27.0 MiB/174.0 MiB (72.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 27.2 MiB/174.0 MiB (73.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 27.5 MiB/174.0 MiB (74.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 27.8 MiB/174.0 MiB (74.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 28.0 MiB/174.0 MiB (74.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 28.2 MiB/174.0 MiB (74.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 28.5 MiB/174.0 MiB (75.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 28.8 MiB/174.0 MiB (75.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 29.0 MiB/174.0 MiB (76.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 29.2 MiB/174.0 MiB (76.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 29.5 MiB/174.0 MiB (76.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 29.8 MiB/174.0 MiB (77.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 30.0 MiB/174.0 MiB (77.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 30.2 MiB/174.0 MiB (78.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 30.5 MiB/174.0 MiB (78.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 30.8 MiB/174.0 MiB (78.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 31.0 MiB/174.0 MiB (79.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 31.2 MiB/174.0 MiB (79.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 31.5 MiB/174.0 MiB (79.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 31.8 MiB/174.0 MiB (80.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 32.0 MiB/174.0 MiB (80.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 32.2 MiB/174.0 MiB (80.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 32.5 MiB/174.0 MiB (81.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 32.8 MiB/174.0 MiB (81.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 33.0 MiB/174.0 MiB (82.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 33.2 MiB/174.0 MiB (82.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 33.5 MiB/174.0 MiB (82.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 33.8 MiB/174.0 MiB (83.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 34.0 MiB/174.0 MiB (83.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 34.2 MiB/174.0 MiB (83.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 34.5 MiB/174.0 MiB (84.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 34.8 MiB/174.0 MiB (84.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 35.0 MiB/174.0 MiB (84.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 35.2 MiB/174.0 MiB (85.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 35.5 MiB/174.0 MiB (85.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 35.8 MiB/174.0 MiB (85.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 36.0 MiB/174.0 MiB (86.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 36.2 MiB/174.0 MiB (86.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 36.5 MiB/174.0 MiB (86.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 36.8 MiB/174.0 MiB (87.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 37.0 MiB/174.0 MiB (87.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 37.2 MiB/174.0 MiB (87.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 37.5 MiB/174.0 MiB (88.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 37.8 MiB/174.0 MiB (88.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 38.0 MiB/174.0 MiB (88.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 38.2 MiB/174.0 MiB (89.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 38.5 MiB/174.0 MiB (89.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 38.8 MiB/174.0 MiB (89.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 39.0 MiB/174.0 MiB (89.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 39.2 MiB/174.0 MiB (89.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 39.5 MiB/174.0 MiB (90.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 39.8 MiB/174.0 MiB (90.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 40.0 MiB/174.0 MiB (91.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 40.2 MiB/174.0 MiB (91.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 40.5 MiB/174.0 MiB (92.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 40.8 MiB/174.0 MiB (92.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 41.0 MiB/174.0 MiB (92.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 41.2 MiB/174.0 MiB (92.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 41.5 MiB/174.0 MiB (93.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 41.8 MiB/174.0 MiB (93.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 42.0 MiB/174.0 MiB (93.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 42.2 MiB/174.0 MiB (94.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 42.5 MiB/174.0 MiB (94.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 42.8 MiB/174.0 MiB (94.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 43.0 MiB/174.0 MiB (95.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 43.2 MiB/174.0 MiB (95.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 43.5 MiB/174.0 MiB (95.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 43.8 MiB/174.0 MiB (95.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 44.0 MiB/174.0 MiB (96.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 44.2 MiB/174.0 MiB (96.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 44.5 MiB/174.0 MiB (96.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 44.8 MiB/174.0 MiB (96.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 45.0 MiB/174.0 MiB (97.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 45.2 MiB/174.0 MiB (97.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 45.5 MiB/174.0 MiB (98.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 45.8 MiB/174.0 MiB (98.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 46.0 MiB/174.0 MiB (98.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 46.2 MiB/174.0 MiB (98.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 46.5 MiB/174.0 MiB (98.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 46.8 MiB/174.0 MiB (99.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 47.0 MiB/174.0 MiB (99.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 47.2 MiB/174.0 MiB (99.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 47.5 MiB/174.0 MiB (100.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 47.8 MiB/174.0 MiB (100.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 48.0 MiB/174.0 MiB (100.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 48.2 MiB/174.0 MiB (100.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 48.5 MiB/174.0 MiB (101.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 48.8 MiB/174.0 MiB (101.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 49.0 MiB/174.0 MiB (101.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 49.2 MiB/174.0 MiB (102.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 49.5 MiB/174.0 MiB (102.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 49.8 MiB/174.0 MiB (102.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 50.0 MiB/174.0 MiB (102.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 50.2 MiB/174.0 MiB (102.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 50.5 MiB/174.0 MiB (102.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 50.8 MiB/174.0 MiB (103.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 51.0 MiB/174.0 MiB (103.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 51.2 MiB/174.0 MiB (103.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 51.5 MiB/174.0 MiB (104.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 51.8 MiB/174.0 MiB (104.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 52.0 MiB/174.0 MiB (104.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 52.2 MiB/174.0 MiB (105.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 52.5 MiB/174.0 MiB (105.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 52.8 MiB/174.0 MiB (105.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 53.0 MiB/174.0 MiB (105.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 53.2 MiB/174.0 MiB (105.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 53.5 MiB/174.0 MiB (105.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 53.8 MiB/174.0 MiB (106.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 54.0 MiB/174.0 MiB (106.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 54.2 MiB/174.0 MiB (107.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 54.5 MiB/174.0 MiB (107.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 54.8 MiB/174.0 MiB (107.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 55.0 MiB/174.0 MiB (108.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 55.2 MiB/174.0 MiB (107.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 55.5 MiB/174.0 MiB (108.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 55.8 MiB/174.0 MiB (108.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 56.0 MiB/174.0 MiB (108.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 56.2 MiB/174.0 MiB (108.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 56.5 MiB/174.0 MiB (109.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 56.8 MiB/174.0 MiB (109.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 57.0 MiB/174.0 MiB (109.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 57.2 MiB/174.0 MiB (110.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 57.5 MiB/174.0 MiB (110.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 57.8 MiB/174.0 MiB (110.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 58.0 MiB/174.0 MiB (110.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 58.2 MiB/174.0 MiB (110.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 58.5 MiB/174.0 MiB (110.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 58.8 MiB/174.0 MiB (111.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 59.0 MiB/174.0 MiB (111.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 59.2 MiB/174.0 MiB (111.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 59.5 MiB/174.0 MiB (112.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 59.8 MiB/174.0 MiB (112.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 60.0 MiB/174.0 MiB (111.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 60.2 MiB/174.0 MiB (112.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 60.5 MiB/174.0 MiB (112.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 60.8 MiB/174.0 MiB (112.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 61.0 MiB/174.0 MiB (112.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 61.2 MiB/174.0 MiB (112.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 61.5 MiB/174.0 MiB (113.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 61.8 MiB/174.0 MiB (113.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 62.0 MiB/174.0 MiB (113.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 62.2 MiB/174.0 MiB (113.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 62.5 MiB/174.0 MiB (114.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 62.8 MiB/174.0 MiB (114.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 63.0 MiB/174.0 MiB (114.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 63.2 MiB/174.0 MiB (114.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 63.5 MiB/174.0 MiB (114.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 63.8 MiB/174.0 MiB (115.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 64.0 MiB/174.0 MiB (115.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 64.2 MiB/174.0 MiB (115.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 64.5 MiB/174.0 MiB (116.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 64.8 MiB/174.0 MiB (115.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 65.0 MiB/174.0 MiB (115.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 65.2 MiB/174.0 MiB (115.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 65.5 MiB/174.0 MiB (115.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 65.8 MiB/174.0 MiB (116.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 66.0 MiB/174.0 MiB (116.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 66.2 MiB/174.0 MiB (116.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 66.5 MiB/174.0 MiB (116.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 66.8 MiB/174.0 MiB (116.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 67.0 MiB/174.0 MiB (117.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 67.2 MiB/174.0 MiB (117.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 67.5 MiB/174.0 MiB (117.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 67.8 MiB/174.0 MiB (117.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 68.0 MiB/174.0 MiB (117.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 68.2 MiB/174.0 MiB (117.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 68.5 MiB/174.0 MiB (118.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 68.8 MiB/174.0 MiB (118.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 69.0 MiB/174.0 MiB (118.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 69.2 MiB/174.0 MiB (118.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 69.5 MiB/174.0 MiB (118.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 69.8 MiB/174.0 MiB (118.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 70.0 MiB/174.0 MiB (118.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 70.2 MiB/174.0 MiB (118.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 70.5 MiB/174.0 MiB (118.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 70.8 MiB/174.0 MiB (118.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 71.0 MiB/174.0 MiB (119.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 71.2 MiB/174.0 MiB (119.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 71.5 MiB/174.0 MiB (119.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 71.8 MiB/174.0 MiB (119.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 72.0 MiB/174.0 MiB (119.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 72.2 MiB/174.0 MiB (119.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 72.5 MiB/174.0 MiB (119.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 72.8 MiB/174.0 MiB (119.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 73.0 MiB/174.0 MiB (119.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 73.2 MiB/174.0 MiB (119.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 73.5 MiB/174.0 MiB (119.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 73.8 MiB/174.0 MiB (120.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 74.0 MiB/174.0 MiB (120.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 74.2 MiB/174.0 MiB (120.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 74.5 MiB/174.0 MiB (120.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 74.8 MiB/174.0 MiB (120.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 75.0 MiB/174.0 MiB (121.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 75.2 MiB/174.0 MiB (121.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 75.5 MiB/174.0 MiB (121.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 75.8 MiB/174.0 MiB (121.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 76.0 MiB/174.0 MiB (121.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 76.2 MiB/174.0 MiB (121.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 76.5 MiB/174.0 MiB (122.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 76.8 MiB/174.0 MiB (122.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 77.0 MiB/174.0 MiB (122.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 77.2 MiB/174.0 MiB (122.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 77.5 MiB/174.0 MiB (123.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 77.8 MiB/174.0 MiB (123.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 78.0 MiB/174.0 MiB (123.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 78.2 MiB/174.0 MiB (123.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 78.5 MiB/174.0 MiB (123.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 78.8 MiB/174.0 MiB (123.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 79.0 MiB/174.0 MiB (124.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 79.2 MiB/174.0 MiB (124.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 79.5 MiB/174.0 MiB (124.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 79.8 MiB/174.0 MiB (124.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 80.0 MiB/174.0 MiB (125.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 80.2 MiB/174.0 MiB (120.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 80.5 MiB/174.0 MiB (120.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 80.8 MiB/174.0 MiB (121.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 81.0 MiB/174.0 MiB (121.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 81.2 MiB/174.0 MiB (121.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 81.5 MiB/174.0 MiB (121.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 81.8 MiB/174.0 MiB (121.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 82.0 MiB/174.0 MiB (122.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 82.2 MiB/174.0 MiB (122.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 82.5 MiB/174.0 MiB (122.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 82.8 MiB/174.0 MiB (122.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 83.0 MiB/174.0 MiB (122.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 83.2 MiB/174.0 MiB (122.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 83.5 MiB/174.0 MiB (122.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 83.8 MiB/174.0 MiB (122.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 84.0 MiB/174.0 MiB (122.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 84.2 MiB/174.0 MiB (123.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 84.5 MiB/174.0 MiB (123.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 84.8 MiB/174.0 MiB (123.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 85.0 MiB/174.0 MiB (123.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 85.2 MiB/174.0 MiB (124.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 85.5 MiB/174.0 MiB (124.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 85.8 MiB/174.0 MiB (124.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 86.0 MiB/174.0 MiB (124.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 86.2 MiB/174.0 MiB (124.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 86.5 MiB/174.0 MiB (124.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 86.8 MiB/174.0 MiB (125.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 87.0 MiB/174.0 MiB (125.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 87.2 MiB/174.0 MiB (125.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 87.5 MiB/174.0 MiB (125.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 87.8 MiB/174.0 MiB (125.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 88.0 MiB/174.0 MiB (125.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 88.2 MiB/174.0 MiB (125.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 88.5 MiB/174.0 MiB (125.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 88.8 MiB/174.0 MiB (125.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 89.0 MiB/174.0 MiB (126.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 89.2 MiB/174.0 MiB (126.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 89.5 MiB/174.0 MiB (126.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 89.8 MiB/174.0 MiB (126.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 90.0 MiB/174.0 MiB (126.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 90.2 MiB/174.0 MiB (126.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 90.5 MiB/174.0 MiB (126.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 90.8 MiB/174.0 MiB (126.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 91.0 MiB/174.0 MiB (126.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 91.2 MiB/174.0 MiB (127.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 91.5 MiB/174.0 MiB (127.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 91.8 MiB/174.0 MiB (127.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 92.0 MiB/174.0 MiB (127.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 92.2 MiB/174.0 MiB (127.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 92.5 MiB/174.0 MiB (127.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 92.8 MiB/174.0 MiB (127.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 93.0 MiB/174.0 MiB (127.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 93.2 MiB/174.0 MiB (128.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 93.5 MiB/174.0 MiB (127.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 93.8 MiB/174.0 MiB (128.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 94.0 MiB/174.0 MiB (128.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 94.2 MiB/174.0 MiB (128.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 94.5 MiB/174.0 MiB (128.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 94.8 MiB/174.0 MiB (128.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 95.0 MiB/174.0 MiB (128.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 95.2 MiB/174.0 MiB (128.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 95.5 MiB/174.0 MiB (129.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 95.8 MiB/174.0 MiB (128.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 96.0 MiB/174.0 MiB (129.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 96.2 MiB/174.0 MiB (129.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 96.5 MiB/174.0 MiB (129.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 96.8 MiB/174.0 MiB (129.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 97.0 MiB/174.0 MiB (129.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 97.2 MiB/174.0 MiB (128.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 97.5 MiB/174.0 MiB (129.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 97.8 MiB/174.0 MiB (129.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 98.0 MiB/174.0 MiB (129.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 98.2 MiB/174.0 MiB (129.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 98.5 MiB/174.0 MiB (129.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 98.8 MiB/174.0 MiB (129.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 99.0 MiB/174.0 MiB (130.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 99.2 MiB/174.0 MiB (130.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 99.5 MiB/174.0 MiB (130.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 99.8 MiB/174.0 MiB (130.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 100.0 MiB/174.0 MiB (130.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 100.2 MiB/174.0 MiB (130.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 100.5 MiB/174.0 MiB (130.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 100.8 MiB/174.0 MiB (130.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 101.0 MiB/174.0 MiB (130.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 101.2 MiB/174.0 MiB (131.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 101.5 MiB/174.0 MiB (131.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 101.8 MiB/174.0 MiB (131.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 102.0 MiB/174.0 MiB (131.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 102.2 MiB/174.0 MiB (131.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 102.5 MiB/174.0 MiB (131.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 102.8 MiB/174.0 MiB (131.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 103.0 MiB/174.0 MiB (131.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 103.2 MiB/174.0 MiB (132.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 103.5 MiB/174.0 MiB (132.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 103.8 MiB/174.0 MiB (132.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 104.0 MiB/174.0 MiB (132.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 104.2 MiB/174.0 MiB (132.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 104.5 MiB/174.0 MiB (132.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 104.8 MiB/174.0 MiB (132.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 105.0 MiB/174.0 MiB (132.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 105.2 MiB/174.0 MiB (133.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 105.5 MiB/174.0 MiB (133.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 105.8 MiB/174.0 MiB (133.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 106.0 MiB/174.0 MiB (133.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 106.2 MiB/174.0 MiB (132.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 106.5 MiB/174.0 MiB (132.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 106.8 MiB/174.0 MiB (133.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 107.0 MiB/174.0 MiB (133.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 107.2 MiB/174.0 MiB (133.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 107.5 MiB/174.0 MiB (133.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 107.8 MiB/174.0 MiB (133.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 108.0 MiB/174.0 MiB (133.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 108.2 MiB/174.0 MiB (133.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 108.5 MiB/174.0 MiB (133.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 108.8 MiB/174.0 MiB (133.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 109.0 MiB/174.0 MiB (134.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 109.2 MiB/174.0 MiB (134.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 109.5 MiB/174.0 MiB (134.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 109.8 MiB/174.0 MiB (134.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 110.0 MiB/174.0 MiB (134.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 110.2 MiB/174.0 MiB (134.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 110.5 MiB/174.0 MiB (134.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 110.8 MiB/174.0 MiB (134.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 111.0 MiB/174.0 MiB (134.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 111.2 MiB/174.0 MiB (134.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 111.5 MiB/174.0 MiB (134.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 111.8 MiB/174.0 MiB (135.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 112.0 MiB/174.0 MiB (135.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 112.2 MiB/174.0 MiB (135.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 112.5 MiB/174.0 MiB (135.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 112.8 MiB/174.0 MiB (135.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 113.0 MiB/174.0 MiB (135.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 113.2 MiB/174.0 MiB (135.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 113.5 MiB/174.0 MiB (135.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 113.8 MiB/174.0 MiB (135.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 114.0 MiB/174.0 MiB (135.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 114.2 MiB/174.0 MiB (136.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 114.5 MiB/174.0 MiB (136.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 114.8 MiB/174.0 MiB (136.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 115.0 MiB/174.0 MiB (136.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 115.2 MiB/174.0 MiB (136.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 115.5 MiB/174.0 MiB (136.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 115.8 MiB/174.0 MiB (136.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 116.0 MiB/174.0 MiB (136.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 116.2 MiB/174.0 MiB (136.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 116.5 MiB/174.0 MiB (137.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 116.8 MiB/174.0 MiB (137.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 117.0 MiB/174.0 MiB (137.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 117.2 MiB/174.0 MiB (137.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 117.5 MiB/174.0 MiB (137.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 117.8 MiB/174.0 MiB (137.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 118.0 MiB/174.0 MiB (137.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 118.2 MiB/174.0 MiB (137.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 118.5 MiB/174.0 MiB (137.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 118.8 MiB/174.0 MiB (137.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 119.0 MiB/174.0 MiB (137.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 119.2 MiB/174.0 MiB (137.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 119.5 MiB/174.0 MiB (137.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 119.8 MiB/174.0 MiB (137.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 120.0 MiB/174.0 MiB (138.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 120.2 MiB/174.0 MiB (138.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 120.5 MiB/174.0 MiB (138.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 120.8 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 121.0 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 121.2 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 121.5 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 121.8 MiB/174.0 MiB (138.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 122.0 MiB/174.0 MiB (138.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 122.2 MiB/174.0 MiB (139.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 122.5 MiB/174.0 MiB (139.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 122.8 MiB/174.0 MiB (139.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 123.0 MiB/174.0 MiB (139.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 123.2 MiB/174.0 MiB (139.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 123.5 MiB/174.0 MiB (139.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 123.8 MiB/174.0 MiB (139.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 124.0 MiB/174.0 MiB (139.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 124.2 MiB/174.0 MiB (139.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 124.5 MiB/174.0 MiB (139.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 124.8 MiB/174.0 MiB (139.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 125.0 MiB/174.0 MiB (140.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 125.2 MiB/174.0 MiB (139.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 125.5 MiB/174.0 MiB (139.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 125.8 MiB/174.0 MiB (140.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 126.0 MiB/174.0 MiB (140.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 126.2 MiB/174.0 MiB (139.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 126.5 MiB/174.0 MiB (139.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 126.8 MiB/174.0 MiB (140.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 127.0 MiB/174.0 MiB (140.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 127.2 MiB/174.0 MiB (140.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 127.5 MiB/174.0 MiB (140.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 127.8 MiB/174.0 MiB (140.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 128.0 MiB/174.0 MiB (140.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 128.2 MiB/174.0 MiB (140.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 128.5 MiB/174.0 MiB (140.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 128.8 MiB/174.0 MiB (140.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 129.0 MiB/174.0 MiB (140.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 129.2 MiB/174.0 MiB (140.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 129.5 MiB/174.0 MiB (140.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 129.8 MiB/174.0 MiB (141.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 130.0 MiB/174.0 MiB (141.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 130.2 MiB/174.0 MiB (141.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 130.5 MiB/174.0 MiB (141.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 130.8 MiB/174.0 MiB (141.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 131.0 MiB/174.0 MiB (141.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 131.2 MiB/174.0 MiB (141.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 131.5 MiB/174.0 MiB (141.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 131.8 MiB/174.0 MiB (141.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 132.0 MiB/174.0 MiB (141.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 132.2 MiB/174.0 MiB (142.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 132.5 MiB/174.0 MiB (142.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 132.8 MiB/174.0 MiB (141.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 133.0 MiB/174.0 MiB (142.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 133.2 MiB/174.0 MiB (142.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 133.5 MiB/174.0 MiB (142.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 133.8 MiB/174.0 MiB (142.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 134.0 MiB/174.0 MiB (142.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 134.2 MiB/174.0 MiB (142.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 134.5 MiB/174.0 MiB (142.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 134.8 MiB/174.0 MiB (142.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 135.0 MiB/174.0 MiB (142.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 135.2 MiB/174.0 MiB (142.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 135.5 MiB/174.0 MiB (142.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 135.8 MiB/174.0 MiB (143.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 136.0 MiB/174.0 MiB (142.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 136.2 MiB/174.0 MiB (143.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 136.5 MiB/174.0 MiB (143.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 136.8 MiB/174.0 MiB (143.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 137.0 MiB/174.0 MiB (143.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 137.2 MiB/174.0 MiB (143.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 137.5 MiB/174.0 MiB (143.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 137.8 MiB/174.0 MiB (143.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 138.0 MiB/174.0 MiB (143.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 138.2 MiB/174.0 MiB (143.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 138.5 MiB/174.0 MiB (143.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 138.8 MiB/174.0 MiB (143.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 139.0 MiB/174.0 MiB (143.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 139.2 MiB/174.0 MiB (143.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 139.5 MiB/174.0 MiB (143.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 139.8 MiB/174.0 MiB (144.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 140.0 MiB/174.0 MiB (144.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 140.2 MiB/174.0 MiB (144.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 140.5 MiB/174.0 MiB (144.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 140.8 MiB/174.0 MiB (144.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 141.0 MiB/174.0 MiB (144.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 141.2 MiB/174.0 MiB (144.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 141.5 MiB/174.0 MiB (144.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 141.8 MiB/174.0 MiB (144.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 142.0 MiB/174.0 MiB (144.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 142.2 MiB/174.0 MiB (144.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 142.5 MiB/174.0 MiB (144.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 142.8 MiB/174.0 MiB (145.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 143.0 MiB/174.0 MiB (145.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 143.2 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 143.5 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 143.8 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 144.0 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 144.2 MiB/174.0 MiB (145.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 144.5 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 144.8 MiB/174.0 MiB (145.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 145.0 MiB/174.0 MiB (145.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 145.2 MiB/174.0 MiB (145.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 145.5 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 145.8 MiB/174.0 MiB (145.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 146.0 MiB/174.0 MiB (145.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 146.2 MiB/174.0 MiB (145.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 146.5 MiB/174.0 MiB (145.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 146.8 MiB/174.0 MiB (145.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 147.0 MiB/174.0 MiB (145.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 147.2 MiB/174.0 MiB (145.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 147.5 MiB/174.0 MiB (145.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 147.8 MiB/174.0 MiB (146.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 148.0 MiB/174.0 MiB (146.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 148.2 MiB/174.0 MiB (146.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 148.5 MiB/174.0 MiB (146.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 148.8 MiB/174.0 MiB (146.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 149.0 MiB/174.0 MiB (146.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 149.2 MiB/174.0 MiB (146.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 149.5 MiB/174.0 MiB (146.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 149.8 MiB/174.0 MiB (146.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 150.0 MiB/174.0 MiB (146.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 150.2 MiB/174.0 MiB (146.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 150.5 MiB/174.0 MiB (146.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 150.8 MiB/174.0 MiB (146.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 151.0 MiB/174.0 MiB (146.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 151.2 MiB/174.0 MiB (146.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 151.5 MiB/174.0 MiB (147.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 151.8 MiB/174.0 MiB (147.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 152.0 MiB/174.0 MiB (147.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 152.2 MiB/174.0 MiB (147.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 152.5 MiB/174.0 MiB (147.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 152.8 MiB/174.0 MiB (147.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 153.0 MiB/174.0 MiB (147.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 153.2 MiB/174.0 MiB (147.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 153.5 MiB/174.0 MiB (147.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 153.8 MiB/174.0 MiB (147.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 154.0 MiB/174.0 MiB (147.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 154.2 MiB/174.0 MiB (148.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 154.5 MiB/174.0 MiB (147.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 154.8 MiB/174.0 MiB (147.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 155.0 MiB/174.0 MiB (147.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 155.2 MiB/174.0 MiB (148.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 155.5 MiB/174.0 MiB (148.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 155.8 MiB/174.0 MiB (148.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 156.0 MiB/174.0 MiB (148.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 156.2 MiB/174.0 MiB (148.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 156.5 MiB/174.0 MiB (148.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 156.8 MiB/174.0 MiB (148.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 157.0 MiB/174.0 MiB (148.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 157.2 MiB/174.0 MiB (148.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 157.5 MiB/174.0 MiB (148.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 157.8 MiB/174.0 MiB (148.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 158.0 MiB/174.0 MiB (148.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 158.2 MiB/174.0 MiB (148.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 158.5 MiB/174.0 MiB (148.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 158.8 MiB/174.0 MiB (148.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 159.0 MiB/174.0 MiB (148.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 159.2 MiB/174.0 MiB (148.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 159.5 MiB/174.0 MiB (148.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 159.8 MiB/174.0 MiB (149.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 160.0 MiB/174.0 MiB (149.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 160.2 MiB/174.0 MiB (149.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 160.5 MiB/174.0 MiB (149.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 160.8 MiB/174.0 MiB (149.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 161.0 MiB/174.0 MiB (149.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 161.2 MiB/174.0 MiB (149.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 161.5 MiB/174.0 MiB (149.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 161.8 MiB/174.0 MiB (149.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 162.0 MiB/174.0 MiB (149.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 162.2 MiB/174.0 MiB (149.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 162.5 MiB/174.0 MiB (149.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 162.8 MiB/174.0 MiB (149.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 163.0 MiB/174.0 MiB (149.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 163.2 MiB/174.0 MiB (150.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 163.5 MiB/174.0 MiB (149.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 163.8 MiB/174.0 MiB (149.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 164.0 MiB/174.0 MiB (149.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 164.2 MiB/174.0 MiB (150.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 164.5 MiB/174.0 MiB (149.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 164.8 MiB/174.0 MiB (150.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 165.0 MiB/174.0 MiB (150.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 165.2 MiB/174.0 MiB (150.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 165.5 MiB/174.0 MiB (150.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 165.8 MiB/174.0 MiB (150.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 166.0 MiB/174.0 MiB (150.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 166.2 MiB/174.0 MiB (150.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 166.5 MiB/174.0 MiB (150.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 166.8 MiB/174.0 MiB (150.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 167.0 MiB/174.0 MiB (151.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 167.2 MiB/174.0 MiB (151.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 167.5 MiB/174.0 MiB (151.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 167.8 MiB/174.0 MiB (151.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 168.0 MiB/174.0 MiB (151.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 168.2 MiB/174.0 MiB (151.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 168.5 MiB/174.0 MiB (151.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 168.8 MiB/174.0 MiB (151.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 169.0 MiB/174.0 MiB (151.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 169.2 MiB/174.0 MiB (152.1 MiB/s) with 1 file(s) remaining\n",
       "Completed 169.5 MiB/174.0 MiB (152.2 MiB/s) with 1 file(s) remaining\n",
       "Completed 169.8 MiB/174.0 MiB (152.3 MiB/s) with 1 file(s) remaining\n",
       "Completed 170.0 MiB/174.0 MiB (152.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 170.2 MiB/174.0 MiB (152.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 170.5 MiB/174.0 MiB (152.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 170.8 MiB/174.0 MiB (152.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 171.0 MiB/174.0 MiB (152.7 MiB/s) with 1 file(s) remaining\n",
       "Completed 171.2 MiB/174.0 MiB (152.8 MiB/s) with 1 file(s) remaining\n",
       "Completed 171.5 MiB/174.0 MiB (152.9 MiB/s) with 1 file(s) remaining\n",
       "Completed 171.8 MiB/174.0 MiB (153.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 172.0 MiB/174.0 MiB (153.0 MiB/s) with 1 file(s) remaining\n",
       "Completed 172.2 MiB/174.0 MiB (138.4 MiB/s) with 1 file(s) remaining\n",
       "Completed 172.5 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 172.8 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 173.0 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\n",
       "Completed 173.2 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 173.5 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 173.8 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 174.0 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\n",
       "Completed 174.0 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\n",
       "download: s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11022022_vnet_130_cases_288_256_320/epoch5_trainloss_0.066_validacc_0.937.pth to TEST/epoch5_trainloss_0.066_validacc_0.937.pth\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Completed 256.0 KiB/174.0 MiB (1.2 MiB/s) with 1 file(s) remaining\nCompleted 512.0 KiB/174.0 MiB (2.3 MiB/s) with 1 file(s) remaining\nCompleted 768.0 KiB/174.0 MiB (3.5 MiB/s) with 1 file(s) remaining\nCompleted 1.0 MiB/174.0 MiB (4.6 MiB/s) with 1 file(s) remaining  \nCompleted 1.2 MiB/174.0 MiB (5.7 MiB/s) with 1 file(s) remaining  \nCompleted 1.5 MiB/174.0 MiB (6.8 MiB/s) with 1 file(s) remaining  \nCompleted 1.8 MiB/174.0 MiB (7.9 MiB/s) with 1 file(s) remaining  \nCompleted 2.0 MiB/174.0 MiB (8.9 MiB/s) with 1 file(s) remaining  \nCompleted 2.2 MiB/174.0 MiB (10.0 MiB/s) with 1 file(s) remaining \nCompleted 2.5 MiB/174.0 MiB (10.9 MiB/s) with 1 file(s) remaining \nCompleted 2.8 MiB/174.0 MiB (12.0 MiB/s) with 1 file(s) remaining \nCompleted 3.0 MiB/174.0 MiB (12.8 MiB/s) with 1 file(s) remaining \nCompleted 3.2 MiB/174.0 MiB (13.9 MiB/s) with 1 file(s) remaining \nCompleted 3.5 MiB/174.0 MiB (14.8 MiB/s) with 1 file(s) remaining \nCompleted 3.8 MiB/174.0 MiB (15.7 MiB/s) with 1 file(s) remaining \nCompleted 4.0 MiB/174.0 MiB (16.7 MiB/s) with 1 file(s) remaining \nCompleted 4.2 MiB/174.0 MiB (17.6 MiB/s) with 1 file(s) remaining \nCompleted 4.5 MiB/174.0 MiB (18.6 MiB/s) with 1 file(s) remaining \nCompleted 4.8 MiB/174.0 MiB (19.5 MiB/s) with 1 file(s) remaining \nCompleted 5.0 MiB/174.0 MiB (20.4 MiB/s) with 1 file(s) remaining \nCompleted 5.2 MiB/174.0 MiB (21.4 MiB/s) with 1 file(s) remaining \nCompleted 5.5 MiB/174.0 MiB (22.2 MiB/s) with 1 file(s) remaining \nCompleted 5.8 MiB/174.0 MiB (22.9 MiB/s) with 1 file(s) remaining \nCompleted 6.0 MiB/174.0 MiB (23.8 MiB/s) with 1 file(s) remaining \nCompleted 6.2 MiB/174.0 MiB (24.7 MiB/s) with 1 file(s) remaining \nCompleted 6.5 MiB/174.0 MiB (25.7 MiB/s) with 1 file(s) remaining \nCompleted 6.8 MiB/174.0 MiB (26.4 MiB/s) with 1 file(s) remaining \nCompleted 7.0 MiB/174.0 MiB (27.2 MiB/s) with 1 file(s) remaining \nCompleted 7.2 MiB/174.0 MiB (28.0 MiB/s) with 1 file(s) remaining \nCompleted 7.5 MiB/174.0 MiB (28.9 MiB/s) with 1 file(s) remaining \nCompleted 7.8 MiB/174.0 MiB (29.8 MiB/s) with 1 file(s) remaining \nCompleted 8.0 MiB/174.0 MiB (30.6 MiB/s) with 1 file(s) remaining \nCompleted 8.2 MiB/174.0 MiB (31.2 MiB/s) with 1 file(s) remaining \nCompleted 8.5 MiB/174.0 MiB (32.1 MiB/s) with 1 file(s) remaining \nCompleted 8.8 MiB/174.0 MiB (32.9 MiB/s) with 1 file(s) remaining \nCompleted 9.0 MiB/174.0 MiB (33.5 MiB/s) with 1 file(s) remaining \nCompleted 9.2 MiB/174.0 MiB (34.3 MiB/s) with 1 file(s) remaining \nCompleted 9.5 MiB/174.0 MiB (35.2 MiB/s) with 1 file(s) remaining \nCompleted 9.8 MiB/174.0 MiB (36.0 MiB/s) with 1 file(s) remaining \nCompleted 10.0 MiB/174.0 MiB (36.4 MiB/s) with 1 file(s) remaining\nCompleted 10.2 MiB/174.0 MiB (37.0 MiB/s) with 1 file(s) remaining\nCompleted 10.5 MiB/174.0 MiB (37.5 MiB/s) with 1 file(s) remaining\nCompleted 10.8 MiB/174.0 MiB (38.3 MiB/s) with 1 file(s) remaining\nCompleted 11.0 MiB/174.0 MiB (39.1 MiB/s) with 1 file(s) remaining\nCompleted 11.2 MiB/174.0 MiB (39.8 MiB/s) with 1 file(s) remaining\nCompleted 11.5 MiB/174.0 MiB (40.6 MiB/s) with 1 file(s) remaining\nCompleted 11.8 MiB/174.0 MiB (41.3 MiB/s) with 1 file(s) remaining\nCompleted 12.0 MiB/174.0 MiB (41.5 MiB/s) with 1 file(s) remaining\nCompleted 12.2 MiB/174.0 MiB (42.0 MiB/s) with 1 file(s) remaining\nCompleted 12.5 MiB/174.0 MiB (42.7 MiB/s) with 1 file(s) remaining\nCompleted 12.8 MiB/174.0 MiB (43.5 MiB/s) with 1 file(s) remaining\nCompleted 13.0 MiB/174.0 MiB (44.3 MiB/s) with 1 file(s) remaining\nCompleted 13.2 MiB/174.0 MiB (45.1 MiB/s) with 1 file(s) remaining\nCompleted 13.5 MiB/174.0 MiB (45.2 MiB/s) with 1 file(s) remaining\nCompleted 13.8 MiB/174.0 MiB (46.0 MiB/s) with 1 file(s) remaining\nCompleted 14.0 MiB/174.0 MiB (46.7 MiB/s) with 1 file(s) remaining\nCompleted 14.2 MiB/174.0 MiB (47.0 MiB/s) with 1 file(s) remaining\nCompleted 14.5 MiB/174.0 MiB (47.7 MiB/s) with 1 file(s) remaining\nCompleted 14.8 MiB/174.0 MiB (48.4 MiB/s) with 1 file(s) remaining\nCompleted 15.0 MiB/174.0 MiB (49.0 MiB/s) with 1 file(s) remaining\nCompleted 15.2 MiB/174.0 MiB (49.7 MiB/s) with 1 file(s) remaining\nCompleted 15.5 MiB/174.0 MiB (50.4 MiB/s) with 1 file(s) remaining\nCompleted 15.8 MiB/174.0 MiB (51.1 MiB/s) with 1 file(s) remaining\nCompleted 16.0 MiB/174.0 MiB (51.5 MiB/s) with 1 file(s) remaining\nCompleted 16.2 MiB/174.0 MiB (51.6 MiB/s) with 1 file(s) remaining\nCompleted 16.5 MiB/174.0 MiB (52.3 MiB/s) with 1 file(s) remaining\nCompleted 16.8 MiB/174.0 MiB (52.8 MiB/s) with 1 file(s) remaining\nCompleted 17.0 MiB/174.0 MiB (53.5 MiB/s) with 1 file(s) remaining\nCompleted 17.2 MiB/174.0 MiB (54.1 MiB/s) with 1 file(s) remaining\nCompleted 17.5 MiB/174.0 MiB (54.8 MiB/s) with 1 file(s) remaining\nCompleted 17.8 MiB/174.0 MiB (55.3 MiB/s) with 1 file(s) remaining\nCompleted 18.0 MiB/174.0 MiB (55.8 MiB/s) with 1 file(s) remaining\nCompleted 18.2 MiB/174.0 MiB (56.1 MiB/s) with 1 file(s) remaining\nCompleted 18.5 MiB/174.0 MiB (56.6 MiB/s) with 1 file(s) remaining\nCompleted 18.8 MiB/174.0 MiB (57.1 MiB/s) with 1 file(s) remaining\nCompleted 19.0 MiB/174.0 MiB (57.8 MiB/s) with 1 file(s) remaining\nCompleted 19.2 MiB/174.0 MiB (58.2 MiB/s) with 1 file(s) remaining\nCompleted 19.5 MiB/174.0 MiB (58.8 MiB/s) with 1 file(s) remaining\nCompleted 19.8 MiB/174.0 MiB (59.2 MiB/s) with 1 file(s) remaining\nCompleted 20.0 MiB/174.0 MiB (59.9 MiB/s) with 1 file(s) remaining\nCompleted 20.2 MiB/174.0 MiB (60.4 MiB/s) with 1 file(s) remaining\nCompleted 20.5 MiB/174.0 MiB (61.0 MiB/s) with 1 file(s) remaining\nCompleted 20.8 MiB/174.0 MiB (61.1 MiB/s) with 1 file(s) remaining\nCompleted 21.0 MiB/174.0 MiB (61.7 MiB/s) with 1 file(s) remaining\nCompleted 21.2 MiB/174.0 MiB (62.2 MiB/s) with 1 file(s) remaining\nCompleted 21.5 MiB/174.0 MiB (62.7 MiB/s) with 1 file(s) remaining\nCompleted 21.8 MiB/174.0 MiB (63.3 MiB/s) with 1 file(s) remaining\nCompleted 22.0 MiB/174.0 MiB (63.7 MiB/s) with 1 file(s) remaining\nCompleted 22.2 MiB/174.0 MiB (64.2 MiB/s) with 1 file(s) remaining\nCompleted 22.5 MiB/174.0 MiB (64.7 MiB/s) with 1 file(s) remaining\nCompleted 22.8 MiB/174.0 MiB (65.3 MiB/s) with 1 file(s) remaining\nCompleted 23.0 MiB/174.0 MiB (65.6 MiB/s) with 1 file(s) remaining\nCompleted 23.2 MiB/174.0 MiB (66.0 MiB/s) with 1 file(s) remaining\nCompleted 23.5 MiB/174.0 MiB (66.7 MiB/s) with 1 file(s) remaining\nCompleted 23.8 MiB/174.0 MiB (67.0 MiB/s) with 1 file(s) remaining\nCompleted 24.0 MiB/174.0 MiB (67.6 MiB/s) with 1 file(s) remaining\nCompleted 24.2 MiB/174.0 MiB (68.1 MiB/s) with 1 file(s) remaining\nCompleted 24.5 MiB/174.0 MiB (68.6 MiB/s) with 1 file(s) remaining\nCompleted 24.8 MiB/174.0 MiB (69.1 MiB/s) with 1 file(s) remaining\nCompleted 25.0 MiB/174.0 MiB (69.4 MiB/s) with 1 file(s) remaining\nCompleted 25.2 MiB/174.0 MiB (70.0 MiB/s) with 1 file(s) remaining\nCompleted 25.5 MiB/174.0 MiB (70.1 MiB/s) with 1 file(s) remaining\nCompleted 25.8 MiB/174.0 MiB (70.5 MiB/s) with 1 file(s) remaining\nCompleted 26.0 MiB/174.0 MiB (71.0 MiB/s) with 1 file(s) remaining\nCompleted 26.2 MiB/174.0 MiB (71.6 MiB/s) with 1 file(s) remaining\nCompleted 26.5 MiB/174.0 MiB (71.8 MiB/s) with 1 file(s) remaining\nCompleted 26.8 MiB/174.0 MiB (72.3 MiB/s) with 1 file(s) remaining\nCompleted 27.0 MiB/174.0 MiB (72.8 MiB/s) with 1 file(s) remaining\nCompleted 27.2 MiB/174.0 MiB (73.4 MiB/s) with 1 file(s) remaining\nCompleted 27.5 MiB/174.0 MiB (74.0 MiB/s) with 1 file(s) remaining\nCompleted 27.8 MiB/174.0 MiB (74.4 MiB/s) with 1 file(s) remaining\nCompleted 28.0 MiB/174.0 MiB (74.6 MiB/s) with 1 file(s) remaining\nCompleted 28.2 MiB/174.0 MiB (74.8 MiB/s) with 1 file(s) remaining\nCompleted 28.5 MiB/174.0 MiB (75.2 MiB/s) with 1 file(s) remaining\nCompleted 28.8 MiB/174.0 MiB (75.7 MiB/s) with 1 file(s) remaining\nCompleted 29.0 MiB/174.0 MiB (76.3 MiB/s) with 1 file(s) remaining\nCompleted 29.2 MiB/174.0 MiB (76.4 MiB/s) with 1 file(s) remaining\nCompleted 29.5 MiB/174.0 MiB (76.8 MiB/s) with 1 file(s) remaining\nCompleted 29.8 MiB/174.0 MiB (77.2 MiB/s) with 1 file(s) remaining\nCompleted 30.0 MiB/174.0 MiB (77.8 MiB/s) with 1 file(s) remaining\nCompleted 30.2 MiB/174.0 MiB (78.3 MiB/s) with 1 file(s) remaining\nCompleted 30.5 MiB/174.0 MiB (78.7 MiB/s) with 1 file(s) remaining\nCompleted 30.8 MiB/174.0 MiB (78.7 MiB/s) with 1 file(s) remaining\nCompleted 31.0 MiB/174.0 MiB (79.1 MiB/s) with 1 file(s) remaining\nCompleted 31.2 MiB/174.0 MiB (79.7 MiB/s) with 1 file(s) remaining\nCompleted 31.5 MiB/174.0 MiB (79.9 MiB/s) with 1 file(s) remaining\nCompleted 31.8 MiB/174.0 MiB (80.3 MiB/s) with 1 file(s) remaining\nCompleted 32.0 MiB/174.0 MiB (80.8 MiB/s) with 1 file(s) remaining\nCompleted 32.2 MiB/174.0 MiB (80.8 MiB/s) with 1 file(s) remaining\nCompleted 32.5 MiB/174.0 MiB (81.4 MiB/s) with 1 file(s) remaining\nCompleted 32.8 MiB/174.0 MiB (81.8 MiB/s) with 1 file(s) remaining\nCompleted 33.0 MiB/174.0 MiB (82.4 MiB/s) with 1 file(s) remaining\nCompleted 33.2 MiB/174.0 MiB (82.4 MiB/s) with 1 file(s) remaining\nCompleted 33.5 MiB/174.0 MiB (82.7 MiB/s) with 1 file(s) remaining\nCompleted 33.8 MiB/174.0 MiB (83.2 MiB/s) with 1 file(s) remaining\nCompleted 34.0 MiB/174.0 MiB (83.8 MiB/s) with 1 file(s) remaining\nCompleted 34.2 MiB/174.0 MiB (83.9 MiB/s) with 1 file(s) remaining\nCompleted 34.5 MiB/174.0 MiB (84.3 MiB/s) with 1 file(s) remaining\nCompleted 34.8 MiB/174.0 MiB (84.5 MiB/s) with 1 file(s) remaining\nCompleted 35.0 MiB/174.0 MiB (84.7 MiB/s) with 1 file(s) remaining\nCompleted 35.2 MiB/174.0 MiB (85.0 MiB/s) with 1 file(s) remaining\nCompleted 35.5 MiB/174.0 MiB (85.5 MiB/s) with 1 file(s) remaining\nCompleted 35.8 MiB/174.0 MiB (85.8 MiB/s) with 1 file(s) remaining\nCompleted 36.0 MiB/174.0 MiB (86.3 MiB/s) with 1 file(s) remaining\nCompleted 36.2 MiB/174.0 MiB (86.7 MiB/s) with 1 file(s) remaining\nCompleted 36.5 MiB/174.0 MiB (86.5 MiB/s) with 1 file(s) remaining\nCompleted 36.8 MiB/174.0 MiB (87.0 MiB/s) with 1 file(s) remaining\nCompleted 37.0 MiB/174.0 MiB (87.2 MiB/s) with 1 file(s) remaining\nCompleted 37.2 MiB/174.0 MiB (87.6 MiB/s) with 1 file(s) remaining\nCompleted 37.5 MiB/174.0 MiB (88.1 MiB/s) with 1 file(s) remaining\nCompleted 37.8 MiB/174.0 MiB (88.5 MiB/s) with 1 file(s) remaining\nCompleted 38.0 MiB/174.0 MiB (88.9 MiB/s) with 1 file(s) remaining\nCompleted 38.2 MiB/174.0 MiB (89.2 MiB/s) with 1 file(s) remaining\nCompleted 38.5 MiB/174.0 MiB (89.4 MiB/s) with 1 file(s) remaining\nCompleted 38.8 MiB/174.0 MiB (89.6 MiB/s) with 1 file(s) remaining\nCompleted 39.0 MiB/174.0 MiB (89.6 MiB/s) with 1 file(s) remaining\nCompleted 39.2 MiB/174.0 MiB (89.9 MiB/s) with 1 file(s) remaining\nCompleted 39.5 MiB/174.0 MiB (90.4 MiB/s) with 1 file(s) remaining\nCompleted 39.8 MiB/174.0 MiB (90.9 MiB/s) with 1 file(s) remaining\nCompleted 40.0 MiB/174.0 MiB (91.4 MiB/s) with 1 file(s) remaining\nCompleted 40.2 MiB/174.0 MiB (91.6 MiB/s) with 1 file(s) remaining\nCompleted 40.5 MiB/174.0 MiB (92.1 MiB/s) with 1 file(s) remaining\nCompleted 40.8 MiB/174.0 MiB (92.4 MiB/s) with 1 file(s) remaining\nCompleted 41.0 MiB/174.0 MiB (92.3 MiB/s) with 1 file(s) remaining\nCompleted 41.2 MiB/174.0 MiB (92.6 MiB/s) with 1 file(s) remaining\nCompleted 41.5 MiB/174.0 MiB (93.0 MiB/s) with 1 file(s) remaining\nCompleted 41.8 MiB/174.0 MiB (93.4 MiB/s) with 1 file(s) remaining\nCompleted 42.0 MiB/174.0 MiB (93.9 MiB/s) with 1 file(s) remaining\nCompleted 42.2 MiB/174.0 MiB (94.0 MiB/s) with 1 file(s) remaining\nCompleted 42.5 MiB/174.0 MiB (94.4 MiB/s) with 1 file(s) remaining\nCompleted 42.8 MiB/174.0 MiB (94.9 MiB/s) with 1 file(s) remaining\nCompleted 43.0 MiB/174.0 MiB (95.2 MiB/s) with 1 file(s) remaining\nCompleted 43.2 MiB/174.0 MiB (95.7 MiB/s) with 1 file(s) remaining\nCompleted 43.5 MiB/174.0 MiB (95.7 MiB/s) with 1 file(s) remaining\nCompleted 43.8 MiB/174.0 MiB (95.6 MiB/s) with 1 file(s) remaining\nCompleted 44.0 MiB/174.0 MiB (96.1 MiB/s) with 1 file(s) remaining\nCompleted 44.2 MiB/174.0 MiB (96.4 MiB/s) with 1 file(s) remaining\nCompleted 44.5 MiB/174.0 MiB (96.6 MiB/s) with 1 file(s) remaining\nCompleted 44.8 MiB/174.0 MiB (96.8 MiB/s) with 1 file(s) remaining\nCompleted 45.0 MiB/174.0 MiB (97.1 MiB/s) with 1 file(s) remaining\nCompleted 45.2 MiB/174.0 MiB (97.5 MiB/s) with 1 file(s) remaining\nCompleted 45.5 MiB/174.0 MiB (98.0 MiB/s) with 1 file(s) remaining\nCompleted 45.8 MiB/174.0 MiB (98.4 MiB/s) with 1 file(s) remaining\nCompleted 46.0 MiB/174.0 MiB (98.3 MiB/s) with 1 file(s) remaining\nCompleted 46.2 MiB/174.0 MiB (98.5 MiB/s) with 1 file(s) remaining\nCompleted 46.5 MiB/174.0 MiB (98.8 MiB/s) with 1 file(s) remaining\nCompleted 46.8 MiB/174.0 MiB (99.2 MiB/s) with 1 file(s) remaining\nCompleted 47.0 MiB/174.0 MiB (99.6 MiB/s) with 1 file(s) remaining\nCompleted 47.2 MiB/174.0 MiB (99.9 MiB/s) with 1 file(s) remaining\nCompleted 47.5 MiB/174.0 MiB (100.1 MiB/s) with 1 file(s) remaining\nCompleted 47.8 MiB/174.0 MiB (100.2 MiB/s) with 1 file(s) remaining\nCompleted 48.0 MiB/174.0 MiB (100.3 MiB/s) with 1 file(s) remaining\nCompleted 48.2 MiB/174.0 MiB (100.6 MiB/s) with 1 file(s) remaining\nCompleted 48.5 MiB/174.0 MiB (101.1 MiB/s) with 1 file(s) remaining\nCompleted 48.8 MiB/174.0 MiB (101.3 MiB/s) with 1 file(s) remaining\nCompleted 49.0 MiB/174.0 MiB (101.7 MiB/s) with 1 file(s) remaining\nCompleted 49.2 MiB/174.0 MiB (102.1 MiB/s) with 1 file(s) remaining\nCompleted 49.5 MiB/174.0 MiB (102.1 MiB/s) with 1 file(s) remaining\nCompleted 49.8 MiB/174.0 MiB (102.4 MiB/s) with 1 file(s) remaining\nCompleted 50.0 MiB/174.0 MiB (102.5 MiB/s) with 1 file(s) remaining\nCompleted 50.2 MiB/174.0 MiB (102.9 MiB/s) with 1 file(s) remaining\nCompleted 50.5 MiB/174.0 MiB (102.9 MiB/s) with 1 file(s) remaining\nCompleted 50.8 MiB/174.0 MiB (103.2 MiB/s) with 1 file(s) remaining\nCompleted 51.0 MiB/174.0 MiB (103.5 MiB/s) with 1 file(s) remaining\nCompleted 51.2 MiB/174.0 MiB (103.8 MiB/s) with 1 file(s) remaining\nCompleted 51.5 MiB/174.0 MiB (104.2 MiB/s) with 1 file(s) remaining\nCompleted 51.8 MiB/174.0 MiB (104.4 MiB/s) with 1 file(s) remaining\nCompleted 52.0 MiB/174.0 MiB (104.8 MiB/s) with 1 file(s) remaining\nCompleted 52.2 MiB/174.0 MiB (105.2 MiB/s) with 1 file(s) remaining\nCompleted 52.5 MiB/174.0 MiB (105.0 MiB/s) with 1 file(s) remaining\nCompleted 52.8 MiB/174.0 MiB (105.2 MiB/s) with 1 file(s) remaining\nCompleted 53.0 MiB/174.0 MiB (105.4 MiB/s) with 1 file(s) remaining\nCompleted 53.2 MiB/174.0 MiB (105.7 MiB/s) with 1 file(s) remaining\nCompleted 53.5 MiB/174.0 MiB (105.9 MiB/s) with 1 file(s) remaining\nCompleted 53.8 MiB/174.0 MiB (106.2 MiB/s) with 1 file(s) remaining\nCompleted 54.0 MiB/174.0 MiB (106.7 MiB/s) with 1 file(s) remaining\nCompleted 54.2 MiB/174.0 MiB (107.1 MiB/s) with 1 file(s) remaining\nCompleted 54.5 MiB/174.0 MiB (107.5 MiB/s) with 1 file(s) remaining\nCompleted 54.8 MiB/174.0 MiB (107.8 MiB/s) with 1 file(s) remaining\nCompleted 55.0 MiB/174.0 MiB (108.1 MiB/s) with 1 file(s) remaining\nCompleted 55.2 MiB/174.0 MiB (107.8 MiB/s) with 1 file(s) remaining\nCompleted 55.5 MiB/174.0 MiB (108.1 MiB/s) with 1 file(s) remaining\nCompleted 55.8 MiB/174.0 MiB (108.5 MiB/s) with 1 file(s) remaining\nCompleted 56.0 MiB/174.0 MiB (108.9 MiB/s) with 1 file(s) remaining\nCompleted 56.2 MiB/174.0 MiB (108.9 MiB/s) with 1 file(s) remaining\nCompleted 56.5 MiB/174.0 MiB (109.1 MiB/s) with 1 file(s) remaining\nCompleted 56.8 MiB/174.0 MiB (109.5 MiB/s) with 1 file(s) remaining\nCompleted 57.0 MiB/174.0 MiB (109.7 MiB/s) with 1 file(s) remaining\nCompleted 57.2 MiB/174.0 MiB (110.0 MiB/s) with 1 file(s) remaining\nCompleted 57.5 MiB/174.0 MiB (110.3 MiB/s) with 1 file(s) remaining\nCompleted 57.8 MiB/174.0 MiB (110.6 MiB/s) with 1 file(s) remaining\nCompleted 58.0 MiB/174.0 MiB (110.5 MiB/s) with 1 file(s) remaining\nCompleted 58.2 MiB/174.0 MiB (110.6 MiB/s) with 1 file(s) remaining\nCompleted 58.5 MiB/174.0 MiB (110.9 MiB/s) with 1 file(s) remaining\nCompleted 58.8 MiB/174.0 MiB (111.1 MiB/s) with 1 file(s) remaining\nCompleted 59.0 MiB/174.0 MiB (111.4 MiB/s) with 1 file(s) remaining\nCompleted 59.2 MiB/174.0 MiB (111.8 MiB/s) with 1 file(s) remaining\nCompleted 59.5 MiB/174.0 MiB (112.1 MiB/s) with 1 file(s) remaining\nCompleted 59.8 MiB/174.0 MiB (112.2 MiB/s) with 1 file(s) remaining\nCompleted 60.0 MiB/174.0 MiB (111.9 MiB/s) with 1 file(s) remaining\nCompleted 60.2 MiB/174.0 MiB (112.2 MiB/s) with 1 file(s) remaining\nCompleted 60.5 MiB/174.0 MiB (112.5 MiB/s) with 1 file(s) remaining\nCompleted 60.8 MiB/174.0 MiB (112.9 MiB/s) with 1 file(s) remaining\nCompleted 61.0 MiB/174.0 MiB (112.6 MiB/s) with 1 file(s) remaining\nCompleted 61.2 MiB/174.0 MiB (112.9 MiB/s) with 1 file(s) remaining\nCompleted 61.5 MiB/174.0 MiB (113.3 MiB/s) with 1 file(s) remaining\nCompleted 61.8 MiB/174.0 MiB (113.6 MiB/s) with 1 file(s) remaining\nCompleted 62.0 MiB/174.0 MiB (113.8 MiB/s) with 1 file(s) remaining\nCompleted 62.2 MiB/174.0 MiB (113.9 MiB/s) with 1 file(s) remaining\nCompleted 62.5 MiB/174.0 MiB (114.3 MiB/s) with 1 file(s) remaining\nCompleted 62.8 MiB/174.0 MiB (114.4 MiB/s) with 1 file(s) remaining\nCompleted 63.0 MiB/174.0 MiB (114.2 MiB/s) with 1 file(s) remaining\nCompleted 63.2 MiB/174.0 MiB (114.5 MiB/s) with 1 file(s) remaining\nCompleted 63.5 MiB/174.0 MiB (114.7 MiB/s) with 1 file(s) remaining\nCompleted 63.8 MiB/174.0 MiB (115.1 MiB/s) with 1 file(s) remaining\nCompleted 64.0 MiB/174.0 MiB (115.3 MiB/s) with 1 file(s) remaining\nCompleted 64.2 MiB/174.0 MiB (115.7 MiB/s) with 1 file(s) remaining\nCompleted 64.5 MiB/174.0 MiB (116.1 MiB/s) with 1 file(s) remaining\nCompleted 64.8 MiB/174.0 MiB (115.5 MiB/s) with 1 file(s) remaining\nCompleted 65.0 MiB/174.0 MiB (115.5 MiB/s) with 1 file(s) remaining\nCompleted 65.2 MiB/174.0 MiB (115.6 MiB/s) with 1 file(s) remaining\nCompleted 65.5 MiB/174.0 MiB (115.9 MiB/s) with 1 file(s) remaining\nCompleted 65.8 MiB/174.0 MiB (116.3 MiB/s) with 1 file(s) remaining\nCompleted 66.0 MiB/174.0 MiB (116.6 MiB/s) with 1 file(s) remaining\nCompleted 66.2 MiB/174.0 MiB (116.8 MiB/s) with 1 file(s) remaining\nCompleted 66.5 MiB/174.0 MiB (116.9 MiB/s) with 1 file(s) remaining\nCompleted 66.8 MiB/174.0 MiB (116.6 MiB/s) with 1 file(s) remaining\nCompleted 67.0 MiB/174.0 MiB (117.0 MiB/s) with 1 file(s) remaining\nCompleted 67.2 MiB/174.0 MiB (117.1 MiB/s) with 1 file(s) remaining\nCompleted 67.5 MiB/174.0 MiB (117.3 MiB/s) with 1 file(s) remaining\nCompleted 67.8 MiB/174.0 MiB (117.4 MiB/s) with 1 file(s) remaining\nCompleted 68.0 MiB/174.0 MiB (117.8 MiB/s) with 1 file(s) remaining\nCompleted 68.2 MiB/174.0 MiB (117.8 MiB/s) with 1 file(s) remaining\nCompleted 68.5 MiB/174.0 MiB (118.1 MiB/s) with 1 file(s) remaining\nCompleted 68.8 MiB/174.0 MiB (118.1 MiB/s) with 1 file(s) remaining\nCompleted 69.0 MiB/174.0 MiB (118.3 MiB/s) with 1 file(s) remaining\nCompleted 69.2 MiB/174.0 MiB (118.7 MiB/s) with 1 file(s) remaining\nCompleted 69.5 MiB/174.0 MiB (118.6 MiB/s) with 1 file(s) remaining\nCompleted 69.8 MiB/174.0 MiB (118.6 MiB/s) with 1 file(s) remaining\nCompleted 70.0 MiB/174.0 MiB (118.2 MiB/s) with 1 file(s) remaining\nCompleted 70.2 MiB/174.0 MiB (118.6 MiB/s) with 1 file(s) remaining\nCompleted 70.5 MiB/174.0 MiB (118.5 MiB/s) with 1 file(s) remaining\nCompleted 70.8 MiB/174.0 MiB (118.8 MiB/s) with 1 file(s) remaining\nCompleted 71.0 MiB/174.0 MiB (119.0 MiB/s) with 1 file(s) remaining\nCompleted 71.2 MiB/174.0 MiB (119.2 MiB/s) with 1 file(s) remaining\nCompleted 71.5 MiB/174.0 MiB (119.1 MiB/s) with 1 file(s) remaining\nCompleted 71.8 MiB/174.0 MiB (119.0 MiB/s) with 1 file(s) remaining\nCompleted 72.0 MiB/174.0 MiB (119.2 MiB/s) with 1 file(s) remaining\nCompleted 72.2 MiB/174.0 MiB (119.2 MiB/s) with 1 file(s) remaining\nCompleted 72.5 MiB/174.0 MiB (119.3 MiB/s) with 1 file(s) remaining\nCompleted 72.8 MiB/174.0 MiB (119.5 MiB/s) with 1 file(s) remaining\nCompleted 73.0 MiB/174.0 MiB (119.7 MiB/s) with 1 file(s) remaining\nCompleted 73.2 MiB/174.0 MiB (119.6 MiB/s) with 1 file(s) remaining\nCompleted 73.5 MiB/174.0 MiB (119.8 MiB/s) with 1 file(s) remaining\nCompleted 73.8 MiB/174.0 MiB (120.1 MiB/s) with 1 file(s) remaining\nCompleted 74.0 MiB/174.0 MiB (120.2 MiB/s) with 1 file(s) remaining\nCompleted 74.2 MiB/174.0 MiB (120.4 MiB/s) with 1 file(s) remaining\nCompleted 74.5 MiB/174.0 MiB (120.6 MiB/s) with 1 file(s) remaining\nCompleted 74.8 MiB/174.0 MiB (120.9 MiB/s) with 1 file(s) remaining\nCompleted 75.0 MiB/174.0 MiB (121.0 MiB/s) with 1 file(s) remaining\nCompleted 75.2 MiB/174.0 MiB (121.2 MiB/s) with 1 file(s) remaining\nCompleted 75.5 MiB/174.0 MiB (121.5 MiB/s) with 1 file(s) remaining\nCompleted 75.8 MiB/174.0 MiB (121.6 MiB/s) with 1 file(s) remaining\nCompleted 76.0 MiB/174.0 MiB (121.8 MiB/s) with 1 file(s) remaining\nCompleted 76.2 MiB/174.0 MiB (121.8 MiB/s) with 1 file(s) remaining\nCompleted 76.5 MiB/174.0 MiB (122.0 MiB/s) with 1 file(s) remaining\nCompleted 76.8 MiB/174.0 MiB (122.3 MiB/s) with 1 file(s) remaining\nCompleted 77.0 MiB/174.0 MiB (122.5 MiB/s) with 1 file(s) remaining\nCompleted 77.2 MiB/174.0 MiB (122.7 MiB/s) with 1 file(s) remaining\nCompleted 77.5 MiB/174.0 MiB (123.0 MiB/s) with 1 file(s) remaining\nCompleted 77.8 MiB/174.0 MiB (123.2 MiB/s) with 1 file(s) remaining\nCompleted 78.0 MiB/174.0 MiB (123.4 MiB/s) with 1 file(s) remaining\nCompleted 78.2 MiB/174.0 MiB (123.4 MiB/s) with 1 file(s) remaining\nCompleted 78.5 MiB/174.0 MiB (123.6 MiB/s) with 1 file(s) remaining\nCompleted 78.8 MiB/174.0 MiB (123.8 MiB/s) with 1 file(s) remaining\nCompleted 79.0 MiB/174.0 MiB (124.0 MiB/s) with 1 file(s) remaining\nCompleted 79.2 MiB/174.0 MiB (124.2 MiB/s) with 1 file(s) remaining\nCompleted 79.5 MiB/174.0 MiB (124.5 MiB/s) with 1 file(s) remaining\nCompleted 79.8 MiB/174.0 MiB (124.7 MiB/s) with 1 file(s) remaining\nCompleted 80.0 MiB/174.0 MiB (125.0 MiB/s) with 1 file(s) remaining\nCompleted 80.2 MiB/174.0 MiB (120.8 MiB/s) with 1 file(s) remaining\nCompleted 80.5 MiB/174.0 MiB (120.9 MiB/s) with 1 file(s) remaining\nCompleted 80.8 MiB/174.0 MiB (121.2 MiB/s) with 1 file(s) remaining\nCompleted 81.0 MiB/174.0 MiB (121.3 MiB/s) with 1 file(s) remaining\nCompleted 81.2 MiB/174.0 MiB (121.5 MiB/s) with 1 file(s) remaining\nCompleted 81.5 MiB/174.0 MiB (121.7 MiB/s) with 1 file(s) remaining\nCompleted 81.8 MiB/174.0 MiB (121.9 MiB/s) with 1 file(s) remaining\nCompleted 82.0 MiB/174.0 MiB (122.1 MiB/s) with 1 file(s) remaining\nCompleted 82.2 MiB/174.0 MiB (122.3 MiB/s) with 1 file(s) remaining\nCompleted 82.5 MiB/174.0 MiB (122.4 MiB/s) with 1 file(s) remaining\nCompleted 82.8 MiB/174.0 MiB (122.3 MiB/s) with 1 file(s) remaining\nCompleted 83.0 MiB/174.0 MiB (122.4 MiB/s) with 1 file(s) remaining\nCompleted 83.2 MiB/174.0 MiB (122.5 MiB/s) with 1 file(s) remaining\nCompleted 83.5 MiB/174.0 MiB (122.7 MiB/s) with 1 file(s) remaining\nCompleted 83.8 MiB/174.0 MiB (122.8 MiB/s) with 1 file(s) remaining\nCompleted 84.0 MiB/174.0 MiB (122.9 MiB/s) with 1 file(s) remaining\nCompleted 84.2 MiB/174.0 MiB (123.1 MiB/s) with 1 file(s) remaining\nCompleted 84.5 MiB/174.0 MiB (123.3 MiB/s) with 1 file(s) remaining\nCompleted 84.8 MiB/174.0 MiB (123.5 MiB/s) with 1 file(s) remaining\nCompleted 85.0 MiB/174.0 MiB (123.7 MiB/s) with 1 file(s) remaining\nCompleted 85.2 MiB/174.0 MiB (124.0 MiB/s) with 1 file(s) remaining\nCompleted 85.5 MiB/174.0 MiB (124.2 MiB/s) with 1 file(s) remaining\nCompleted 85.8 MiB/174.0 MiB (124.4 MiB/s) with 1 file(s) remaining\nCompleted 86.0 MiB/174.0 MiB (124.7 MiB/s) with 1 file(s) remaining\nCompleted 86.2 MiB/174.0 MiB (124.6 MiB/s) with 1 file(s) remaining\nCompleted 86.5 MiB/174.0 MiB (124.8 MiB/s) with 1 file(s) remaining\nCompleted 86.8 MiB/174.0 MiB (125.1 MiB/s) with 1 file(s) remaining\nCompleted 87.0 MiB/174.0 MiB (125.4 MiB/s) with 1 file(s) remaining\nCompleted 87.2 MiB/174.0 MiB (125.2 MiB/s) with 1 file(s) remaining\nCompleted 87.5 MiB/174.0 MiB (125.3 MiB/s) with 1 file(s) remaining\nCompleted 87.8 MiB/174.0 MiB (125.6 MiB/s) with 1 file(s) remaining\nCompleted 88.0 MiB/174.0 MiB (125.6 MiB/s) with 1 file(s) remaining\nCompleted 88.2 MiB/174.0 MiB (125.6 MiB/s) with 1 file(s) remaining\nCompleted 88.5 MiB/174.0 MiB (125.6 MiB/s) with 1 file(s) remaining\nCompleted 88.8 MiB/174.0 MiB (125.9 MiB/s) with 1 file(s) remaining\nCompleted 89.0 MiB/174.0 MiB (126.1 MiB/s) with 1 file(s) remaining\nCompleted 89.2 MiB/174.0 MiB (126.2 MiB/s) with 1 file(s) remaining\nCompleted 89.5 MiB/174.0 MiB (126.1 MiB/s) with 1 file(s) remaining\nCompleted 89.8 MiB/174.0 MiB (126.2 MiB/s) with 1 file(s) remaining\nCompleted 90.0 MiB/174.0 MiB (126.4 MiB/s) with 1 file(s) remaining\nCompleted 90.2 MiB/174.0 MiB (126.6 MiB/s) with 1 file(s) remaining\nCompleted 90.5 MiB/174.0 MiB (126.8 MiB/s) with 1 file(s) remaining\nCompleted 90.8 MiB/174.0 MiB (126.8 MiB/s) with 1 file(s) remaining\nCompleted 91.0 MiB/174.0 MiB (126.8 MiB/s) with 1 file(s) remaining\nCompleted 91.2 MiB/174.0 MiB (127.1 MiB/s) with 1 file(s) remaining\nCompleted 91.5 MiB/174.0 MiB (127.2 MiB/s) with 1 file(s) remaining\nCompleted 91.8 MiB/174.0 MiB (127.4 MiB/s) with 1 file(s) remaining\nCompleted 92.0 MiB/174.0 MiB (127.5 MiB/s) with 1 file(s) remaining\nCompleted 92.2 MiB/174.0 MiB (127.4 MiB/s) with 1 file(s) remaining\nCompleted 92.5 MiB/174.0 MiB (127.5 MiB/s) with 1 file(s) remaining\nCompleted 92.8 MiB/174.0 MiB (127.5 MiB/s) with 1 file(s) remaining\nCompleted 93.0 MiB/174.0 MiB (127.7 MiB/s) with 1 file(s) remaining\nCompleted 93.2 MiB/174.0 MiB (128.0 MiB/s) with 1 file(s) remaining\nCompleted 93.5 MiB/174.0 MiB (127.9 MiB/s) with 1 file(s) remaining\nCompleted 93.8 MiB/174.0 MiB (128.1 MiB/s) with 1 file(s) remaining\nCompleted 94.0 MiB/174.0 MiB (128.4 MiB/s) with 1 file(s) remaining\nCompleted 94.2 MiB/174.0 MiB (128.2 MiB/s) with 1 file(s) remaining\nCompleted 94.5 MiB/174.0 MiB (128.4 MiB/s) with 1 file(s) remaining\nCompleted 94.8 MiB/174.0 MiB (128.6 MiB/s) with 1 file(s) remaining\nCompleted 95.0 MiB/174.0 MiB (128.9 MiB/s) with 1 file(s) remaining\nCompleted 95.2 MiB/174.0 MiB (128.9 MiB/s) with 1 file(s) remaining\nCompleted 95.5 MiB/174.0 MiB (129.2 MiB/s) with 1 file(s) remaining\nCompleted 95.8 MiB/174.0 MiB (128.9 MiB/s) with 1 file(s) remaining\nCompleted 96.0 MiB/174.0 MiB (129.1 MiB/s) with 1 file(s) remaining\nCompleted 96.2 MiB/174.0 MiB (129.4 MiB/s) with 1 file(s) remaining\nCompleted 96.5 MiB/174.0 MiB (129.5 MiB/s) with 1 file(s) remaining\nCompleted 96.8 MiB/174.0 MiB (129.7 MiB/s) with 1 file(s) remaining\nCompleted 97.0 MiB/174.0 MiB (129.2 MiB/s) with 1 file(s) remaining\nCompleted 97.2 MiB/174.0 MiB (128.9 MiB/s) with 1 file(s) remaining\nCompleted 97.5 MiB/174.0 MiB (129.2 MiB/s) with 1 file(s) remaining\nCompleted 97.8 MiB/174.0 MiB (129.4 MiB/s) with 1 file(s) remaining\nCompleted 98.0 MiB/174.0 MiB (129.7 MiB/s) with 1 file(s) remaining\nCompleted 98.2 MiB/174.0 MiB (129.4 MiB/s) with 1 file(s) remaining\nCompleted 98.5 MiB/174.0 MiB (129.6 MiB/s) with 1 file(s) remaining\nCompleted 98.8 MiB/174.0 MiB (129.9 MiB/s) with 1 file(s) remaining\nCompleted 99.0 MiB/174.0 MiB (130.1 MiB/s) with 1 file(s) remaining\nCompleted 99.2 MiB/174.0 MiB (130.3 MiB/s) with 1 file(s) remaining\nCompleted 99.5 MiB/174.0 MiB (130.4 MiB/s) with 1 file(s) remaining\nCompleted 99.8 MiB/174.0 MiB (130.5 MiB/s) with 1 file(s) remaining\nCompleted 100.0 MiB/174.0 MiB (130.6 MiB/s) with 1 file(s) remaining\nCompleted 100.2 MiB/174.0 MiB (130.9 MiB/s) with 1 file(s) remaining\nCompleted 100.5 MiB/174.0 MiB (130.8 MiB/s) with 1 file(s) remaining\nCompleted 100.8 MiB/174.0 MiB (130.6 MiB/s) with 1 file(s) remaining\nCompleted 101.0 MiB/174.0 MiB (130.8 MiB/s) with 1 file(s) remaining\nCompleted 101.2 MiB/174.0 MiB (131.0 MiB/s) with 1 file(s) remaining\nCompleted 101.5 MiB/174.0 MiB (131.3 MiB/s) with 1 file(s) remaining\nCompleted 101.8 MiB/174.0 MiB (131.4 MiB/s) with 1 file(s) remaining\nCompleted 102.0 MiB/174.0 MiB (131.3 MiB/s) with 1 file(s) remaining\nCompleted 102.2 MiB/174.0 MiB (131.5 MiB/s) with 1 file(s) remaining\nCompleted 102.5 MiB/174.0 MiB (131.7 MiB/s) with 1 file(s) remaining\nCompleted 102.8 MiB/174.0 MiB (131.9 MiB/s) with 1 file(s) remaining\nCompleted 103.0 MiB/174.0 MiB (131.9 MiB/s) with 1 file(s) remaining\nCompleted 103.2 MiB/174.0 MiB (132.0 MiB/s) with 1 file(s) remaining\nCompleted 103.5 MiB/174.0 MiB (132.0 MiB/s) with 1 file(s) remaining\nCompleted 103.8 MiB/174.0 MiB (132.1 MiB/s) with 1 file(s) remaining\nCompleted 104.0 MiB/174.0 MiB (132.2 MiB/s) with 1 file(s) remaining\nCompleted 104.2 MiB/174.0 MiB (132.2 MiB/s) with 1 file(s) remaining\nCompleted 104.5 MiB/174.0 MiB (132.5 MiB/s) with 1 file(s) remaining\nCompleted 104.8 MiB/174.0 MiB (132.6 MiB/s) with 1 file(s) remaining\nCompleted 105.0 MiB/174.0 MiB (132.8 MiB/s) with 1 file(s) remaining\nCompleted 105.2 MiB/174.0 MiB (133.0 MiB/s) with 1 file(s) remaining\nCompleted 105.5 MiB/174.0 MiB (133.2 MiB/s) with 1 file(s) remaining\nCompleted 105.8 MiB/174.0 MiB (133.0 MiB/s) with 1 file(s) remaining\nCompleted 106.0 MiB/174.0 MiB (133.0 MiB/s) with 1 file(s) remaining\nCompleted 106.2 MiB/174.0 MiB (132.6 MiB/s) with 1 file(s) remaining\nCompleted 106.5 MiB/174.0 MiB (132.8 MiB/s) with 1 file(s) remaining\nCompleted 106.8 MiB/174.0 MiB (133.0 MiB/s) with 1 file(s) remaining\nCompleted 107.0 MiB/174.0 MiB (133.1 MiB/s) with 1 file(s) remaining\nCompleted 107.2 MiB/174.0 MiB (133.4 MiB/s) with 1 file(s) remaining\nCompleted 107.5 MiB/174.0 MiB (133.6 MiB/s) with 1 file(s) remaining\nCompleted 107.8 MiB/174.0 MiB (133.6 MiB/s) with 1 file(s) remaining\nCompleted 108.0 MiB/174.0 MiB (133.7 MiB/s) with 1 file(s) remaining\nCompleted 108.2 MiB/174.0 MiB (133.7 MiB/s) with 1 file(s) remaining\nCompleted 108.5 MiB/174.0 MiB (133.8 MiB/s) with 1 file(s) remaining\nCompleted 108.8 MiB/174.0 MiB (133.9 MiB/s) with 1 file(s) remaining\nCompleted 109.0 MiB/174.0 MiB (134.0 MiB/s) with 1 file(s) remaining\nCompleted 109.2 MiB/174.0 MiB (134.1 MiB/s) with 1 file(s) remaining\nCompleted 109.5 MiB/174.0 MiB (134.0 MiB/s) with 1 file(s) remaining\nCompleted 109.8 MiB/174.0 MiB (134.1 MiB/s) with 1 file(s) remaining\nCompleted 110.0 MiB/174.0 MiB (134.3 MiB/s) with 1 file(s) remaining\nCompleted 110.2 MiB/174.0 MiB (134.5 MiB/s) with 1 file(s) remaining\nCompleted 110.5 MiB/174.0 MiB (134.7 MiB/s) with 1 file(s) remaining\nCompleted 110.8 MiB/174.0 MiB (134.7 MiB/s) with 1 file(s) remaining\nCompleted 111.0 MiB/174.0 MiB (134.6 MiB/s) with 1 file(s) remaining\nCompleted 111.2 MiB/174.0 MiB (134.7 MiB/s) with 1 file(s) remaining\nCompleted 111.5 MiB/174.0 MiB (134.9 MiB/s) with 1 file(s) remaining\nCompleted 111.8 MiB/174.0 MiB (135.0 MiB/s) with 1 file(s) remaining\nCompleted 112.0 MiB/174.0 MiB (135.2 MiB/s) with 1 file(s) remaining\nCompleted 112.2 MiB/174.0 MiB (135.3 MiB/s) with 1 file(s) remaining\nCompleted 112.5 MiB/174.0 MiB (135.4 MiB/s) with 1 file(s) remaining\nCompleted 112.8 MiB/174.0 MiB (135.3 MiB/s) with 1 file(s) remaining\nCompleted 113.0 MiB/174.0 MiB (135.5 MiB/s) with 1 file(s) remaining\nCompleted 113.2 MiB/174.0 MiB (135.5 MiB/s) with 1 file(s) remaining\nCompleted 113.5 MiB/174.0 MiB (135.8 MiB/s) with 1 file(s) remaining\nCompleted 113.8 MiB/174.0 MiB (135.7 MiB/s) with 1 file(s) remaining\nCompleted 114.0 MiB/174.0 MiB (135.8 MiB/s) with 1 file(s) remaining\nCompleted 114.2 MiB/174.0 MiB (136.1 MiB/s) with 1 file(s) remaining\nCompleted 114.5 MiB/174.0 MiB (136.3 MiB/s) with 1 file(s) remaining\nCompleted 114.8 MiB/174.0 MiB (136.4 MiB/s) with 1 file(s) remaining\nCompleted 115.0 MiB/174.0 MiB (136.4 MiB/s) with 1 file(s) remaining\nCompleted 115.2 MiB/174.0 MiB (136.2 MiB/s) with 1 file(s) remaining\nCompleted 115.5 MiB/174.0 MiB (136.3 MiB/s) with 1 file(s) remaining\nCompleted 115.8 MiB/174.0 MiB (136.3 MiB/s) with 1 file(s) remaining\nCompleted 116.0 MiB/174.0 MiB (136.6 MiB/s) with 1 file(s) remaining\nCompleted 116.2 MiB/174.0 MiB (136.7 MiB/s) with 1 file(s) remaining\nCompleted 116.5 MiB/174.0 MiB (137.0 MiB/s) with 1 file(s) remaining\nCompleted 116.8 MiB/174.0 MiB (137.1 MiB/s) with 1 file(s) remaining\nCompleted 117.0 MiB/174.0 MiB (137.3 MiB/s) with 1 file(s) remaining\nCompleted 117.2 MiB/174.0 MiB (137.1 MiB/s) with 1 file(s) remaining\nCompleted 117.5 MiB/174.0 MiB (137.3 MiB/s) with 1 file(s) remaining\nCompleted 117.8 MiB/174.0 MiB (137.1 MiB/s) with 1 file(s) remaining\nCompleted 118.0 MiB/174.0 MiB (137.3 MiB/s) with 1 file(s) remaining\nCompleted 118.2 MiB/174.0 MiB (137.4 MiB/s) with 1 file(s) remaining\nCompleted 118.5 MiB/174.0 MiB (137.5 MiB/s) with 1 file(s) remaining\nCompleted 118.8 MiB/174.0 MiB (137.7 MiB/s) with 1 file(s) remaining\nCompleted 119.0 MiB/174.0 MiB (137.6 MiB/s) with 1 file(s) remaining\nCompleted 119.2 MiB/174.0 MiB (137.8 MiB/s) with 1 file(s) remaining\nCompleted 119.5 MiB/174.0 MiB (137.8 MiB/s) with 1 file(s) remaining\nCompleted 119.8 MiB/174.0 MiB (137.9 MiB/s) with 1 file(s) remaining\nCompleted 120.0 MiB/174.0 MiB (138.0 MiB/s) with 1 file(s) remaining\nCompleted 120.2 MiB/174.0 MiB (138.1 MiB/s) with 1 file(s) remaining\nCompleted 120.5 MiB/174.0 MiB (138.4 MiB/s) with 1 file(s) remaining\nCompleted 120.8 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\nCompleted 121.0 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\nCompleted 121.2 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\nCompleted 121.5 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\nCompleted 121.8 MiB/174.0 MiB (138.7 MiB/s) with 1 file(s) remaining\nCompleted 122.0 MiB/174.0 MiB (138.9 MiB/s) with 1 file(s) remaining\nCompleted 122.2 MiB/174.0 MiB (139.1 MiB/s) with 1 file(s) remaining\nCompleted 122.5 MiB/174.0 MiB (139.1 MiB/s) with 1 file(s) remaining\nCompleted 122.8 MiB/174.0 MiB (139.1 MiB/s) with 1 file(s) remaining\nCompleted 123.0 MiB/174.0 MiB (139.3 MiB/s) with 1 file(s) remaining\nCompleted 123.2 MiB/174.0 MiB (139.2 MiB/s) with 1 file(s) remaining\nCompleted 123.5 MiB/174.0 MiB (139.3 MiB/s) with 1 file(s) remaining\nCompleted 123.8 MiB/174.0 MiB (139.4 MiB/s) with 1 file(s) remaining\nCompleted 124.0 MiB/174.0 MiB (139.5 MiB/s) with 1 file(s) remaining\nCompleted 124.2 MiB/174.0 MiB (139.7 MiB/s) with 1 file(s) remaining\nCompleted 124.5 MiB/174.0 MiB (139.8 MiB/s) with 1 file(s) remaining\nCompleted 124.8 MiB/174.0 MiB (139.8 MiB/s) with 1 file(s) remaining\nCompleted 125.0 MiB/174.0 MiB (140.0 MiB/s) with 1 file(s) remaining\nCompleted 125.2 MiB/174.0 MiB (139.8 MiB/s) with 1 file(s) remaining\nCompleted 125.5 MiB/174.0 MiB (139.9 MiB/s) with 1 file(s) remaining\nCompleted 125.8 MiB/174.0 MiB (140.0 MiB/s) with 1 file(s) remaining\nCompleted 126.0 MiB/174.0 MiB (140.0 MiB/s) with 1 file(s) remaining\nCompleted 126.2 MiB/174.0 MiB (139.8 MiB/s) with 1 file(s) remaining\nCompleted 126.5 MiB/174.0 MiB (139.9 MiB/s) with 1 file(s) remaining\nCompleted 126.8 MiB/174.0 MiB (140.0 MiB/s) with 1 file(s) remaining\nCompleted 127.0 MiB/174.0 MiB (140.2 MiB/s) with 1 file(s) remaining\nCompleted 127.2 MiB/174.0 MiB (140.4 MiB/s) with 1 file(s) remaining\nCompleted 127.5 MiB/174.0 MiB (140.4 MiB/s) with 1 file(s) remaining\nCompleted 127.8 MiB/174.0 MiB (140.5 MiB/s) with 1 file(s) remaining\nCompleted 128.0 MiB/174.0 MiB (140.5 MiB/s) with 1 file(s) remaining\nCompleted 128.2 MiB/174.0 MiB (140.6 MiB/s) with 1 file(s) remaining\nCompleted 128.5 MiB/174.0 MiB (140.8 MiB/s) with 1 file(s) remaining\nCompleted 128.8 MiB/174.0 MiB (140.9 MiB/s) with 1 file(s) remaining\nCompleted 129.0 MiB/174.0 MiB (140.8 MiB/s) with 1 file(s) remaining\nCompleted 129.2 MiB/174.0 MiB (140.7 MiB/s) with 1 file(s) remaining\nCompleted 129.5 MiB/174.0 MiB (140.9 MiB/s) with 1 file(s) remaining\nCompleted 129.8 MiB/174.0 MiB (141.1 MiB/s) with 1 file(s) remaining\nCompleted 130.0 MiB/174.0 MiB (141.1 MiB/s) with 1 file(s) remaining\nCompleted 130.2 MiB/174.0 MiB (141.3 MiB/s) with 1 file(s) remaining\nCompleted 130.5 MiB/174.0 MiB (141.2 MiB/s) with 1 file(s) remaining\nCompleted 130.8 MiB/174.0 MiB (141.3 MiB/s) with 1 file(s) remaining\nCompleted 131.0 MiB/174.0 MiB (141.4 MiB/s) with 1 file(s) remaining\nCompleted 131.2 MiB/174.0 MiB (141.6 MiB/s) with 1 file(s) remaining\nCompleted 131.5 MiB/174.0 MiB (141.6 MiB/s) with 1 file(s) remaining\nCompleted 131.8 MiB/174.0 MiB (141.8 MiB/s) with 1 file(s) remaining\nCompleted 132.0 MiB/174.0 MiB (141.8 MiB/s) with 1 file(s) remaining\nCompleted 132.2 MiB/174.0 MiB (142.0 MiB/s) with 1 file(s) remaining\nCompleted 132.5 MiB/174.0 MiB (142.0 MiB/s) with 1 file(s) remaining\nCompleted 132.8 MiB/174.0 MiB (141.9 MiB/s) with 1 file(s) remaining\nCompleted 133.0 MiB/174.0 MiB (142.0 MiB/s) with 1 file(s) remaining\nCompleted 133.2 MiB/174.0 MiB (142.2 MiB/s) with 1 file(s) remaining\nCompleted 133.5 MiB/174.0 MiB (142.2 MiB/s) with 1 file(s) remaining\nCompleted 133.8 MiB/174.0 MiB (142.4 MiB/s) with 1 file(s) remaining\nCompleted 134.0 MiB/174.0 MiB (142.6 MiB/s) with 1 file(s) remaining\nCompleted 134.2 MiB/174.0 MiB (142.6 MiB/s) with 1 file(s) remaining\nCompleted 134.5 MiB/174.0 MiB (142.6 MiB/s) with 1 file(s) remaining\nCompleted 134.8 MiB/174.0 MiB (142.6 MiB/s) with 1 file(s) remaining\nCompleted 135.0 MiB/174.0 MiB (142.7 MiB/s) with 1 file(s) remaining\nCompleted 135.2 MiB/174.0 MiB (142.8 MiB/s) with 1 file(s) remaining\nCompleted 135.5 MiB/174.0 MiB (142.9 MiB/s) with 1 file(s) remaining\nCompleted 135.8 MiB/174.0 MiB (143.0 MiB/s) with 1 file(s) remaining\nCompleted 136.0 MiB/174.0 MiB (142.9 MiB/s) with 1 file(s) remaining\nCompleted 136.2 MiB/174.0 MiB (143.1 MiB/s) with 1 file(s) remaining\nCompleted 136.5 MiB/174.0 MiB (143.3 MiB/s) with 1 file(s) remaining\nCompleted 136.8 MiB/174.0 MiB (143.3 MiB/s) with 1 file(s) remaining\nCompleted 137.0 MiB/174.0 MiB (143.3 MiB/s) with 1 file(s) remaining\nCompleted 137.2 MiB/174.0 MiB (143.3 MiB/s) with 1 file(s) remaining\nCompleted 137.5 MiB/174.0 MiB (143.5 MiB/s) with 1 file(s) remaining\nCompleted 137.8 MiB/174.0 MiB (143.5 MiB/s) with 1 file(s) remaining\nCompleted 138.0 MiB/174.0 MiB (143.6 MiB/s) with 1 file(s) remaining\nCompleted 138.2 MiB/174.0 MiB (143.6 MiB/s) with 1 file(s) remaining\nCompleted 138.5 MiB/174.0 MiB (143.5 MiB/s) with 1 file(s) remaining\nCompleted 138.8 MiB/174.0 MiB (143.6 MiB/s) with 1 file(s) remaining\nCompleted 139.0 MiB/174.0 MiB (143.7 MiB/s) with 1 file(s) remaining\nCompleted 139.2 MiB/174.0 MiB (143.9 MiB/s) with 1 file(s) remaining\nCompleted 139.5 MiB/174.0 MiB (143.9 MiB/s) with 1 file(s) remaining\nCompleted 139.8 MiB/174.0 MiB (144.1 MiB/s) with 1 file(s) remaining\nCompleted 140.0 MiB/174.0 MiB (144.3 MiB/s) with 1 file(s) remaining\nCompleted 140.2 MiB/174.0 MiB (144.4 MiB/s) with 1 file(s) remaining\nCompleted 140.5 MiB/174.0 MiB (144.3 MiB/s) with 1 file(s) remaining\nCompleted 140.8 MiB/174.0 MiB (144.5 MiB/s) with 1 file(s) remaining\nCompleted 141.0 MiB/174.0 MiB (144.2 MiB/s) with 1 file(s) remaining\nCompleted 141.2 MiB/174.0 MiB (144.2 MiB/s) with 1 file(s) remaining\nCompleted 141.5 MiB/174.0 MiB (144.3 MiB/s) with 1 file(s) remaining\nCompleted 141.8 MiB/174.0 MiB (144.4 MiB/s) with 1 file(s) remaining\nCompleted 142.0 MiB/174.0 MiB (144.4 MiB/s) with 1 file(s) remaining\nCompleted 142.2 MiB/174.0 MiB (144.6 MiB/s) with 1 file(s) remaining\nCompleted 142.5 MiB/174.0 MiB (144.8 MiB/s) with 1 file(s) remaining\nCompleted 142.8 MiB/174.0 MiB (145.0 MiB/s) with 1 file(s) remaining\nCompleted 143.0 MiB/174.0 MiB (145.1 MiB/s) with 1 file(s) remaining\nCompleted 143.2 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\nCompleted 143.5 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\nCompleted 143.8 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\nCompleted 144.0 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\nCompleted 144.2 MiB/174.0 MiB (145.0 MiB/s) with 1 file(s) remaining\nCompleted 144.5 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\nCompleted 144.8 MiB/174.0 MiB (145.4 MiB/s) with 1 file(s) remaining\nCompleted 145.0 MiB/174.0 MiB (145.1 MiB/s) with 1 file(s) remaining\nCompleted 145.2 MiB/174.0 MiB (145.3 MiB/s) with 1 file(s) remaining\nCompleted 145.5 MiB/174.0 MiB (145.2 MiB/s) with 1 file(s) remaining\nCompleted 145.8 MiB/174.0 MiB (145.4 MiB/s) with 1 file(s) remaining\nCompleted 146.0 MiB/174.0 MiB (145.7 MiB/s) with 1 file(s) remaining\nCompleted 146.2 MiB/174.0 MiB (145.8 MiB/s) with 1 file(s) remaining\nCompleted 146.5 MiB/174.0 MiB (145.9 MiB/s) with 1 file(s) remaining\nCompleted 146.8 MiB/174.0 MiB (145.7 MiB/s) with 1 file(s) remaining\nCompleted 147.0 MiB/174.0 MiB (145.6 MiB/s) with 1 file(s) remaining\nCompleted 147.2 MiB/174.0 MiB (145.7 MiB/s) with 1 file(s) remaining\nCompleted 147.5 MiB/174.0 MiB (145.9 MiB/s) with 1 file(s) remaining\nCompleted 147.8 MiB/174.0 MiB (146.0 MiB/s) with 1 file(s) remaining\nCompleted 148.0 MiB/174.0 MiB (146.1 MiB/s) with 1 file(s) remaining\nCompleted 148.2 MiB/174.0 MiB (146.3 MiB/s) with 1 file(s) remaining\nCompleted 148.5 MiB/174.0 MiB (146.2 MiB/s) with 1 file(s) remaining\nCompleted 148.8 MiB/174.0 MiB (146.3 MiB/s) with 1 file(s) remaining\nCompleted 149.0 MiB/174.0 MiB (146.3 MiB/s) with 1 file(s) remaining\nCompleted 149.2 MiB/174.0 MiB (146.2 MiB/s) with 1 file(s) remaining\nCompleted 149.5 MiB/174.0 MiB (146.3 MiB/s) with 1 file(s) remaining\nCompleted 149.8 MiB/174.0 MiB (146.5 MiB/s) with 1 file(s) remaining\nCompleted 150.0 MiB/174.0 MiB (146.6 MiB/s) with 1 file(s) remaining\nCompleted 150.2 MiB/174.0 MiB (146.8 MiB/s) with 1 file(s) remaining\nCompleted 150.5 MiB/174.0 MiB (146.8 MiB/s) with 1 file(s) remaining\nCompleted 150.8 MiB/174.0 MiB (146.6 MiB/s) with 1 file(s) remaining\nCompleted 151.0 MiB/174.0 MiB (146.8 MiB/s) with 1 file(s) remaining\nCompleted 151.2 MiB/174.0 MiB (146.8 MiB/s) with 1 file(s) remaining\nCompleted 151.5 MiB/174.0 MiB (147.0 MiB/s) with 1 file(s) remaining\nCompleted 151.8 MiB/174.0 MiB (147.1 MiB/s) with 1 file(s) remaining\nCompleted 152.0 MiB/174.0 MiB (147.3 MiB/s) with 1 file(s) remaining\nCompleted 152.2 MiB/174.0 MiB (147.4 MiB/s) with 1 file(s) remaining\nCompleted 152.5 MiB/174.0 MiB (147.1 MiB/s) with 1 file(s) remaining\nCompleted 152.8 MiB/174.0 MiB (147.2 MiB/s) with 1 file(s) remaining\nCompleted 153.0 MiB/174.0 MiB (147.4 MiB/s) with 1 file(s) remaining\nCompleted 153.2 MiB/174.0 MiB (147.5 MiB/s) with 1 file(s) remaining\nCompleted 153.5 MiB/174.0 MiB (147.6 MiB/s) with 1 file(s) remaining\nCompleted 153.8 MiB/174.0 MiB (147.7 MiB/s) with 1 file(s) remaining\nCompleted 154.0 MiB/174.0 MiB (147.8 MiB/s) with 1 file(s) remaining\nCompleted 154.2 MiB/174.0 MiB (148.0 MiB/s) with 1 file(s) remaining\nCompleted 154.5 MiB/174.0 MiB (147.6 MiB/s) with 1 file(s) remaining\nCompleted 154.8 MiB/174.0 MiB (147.7 MiB/s) with 1 file(s) remaining\nCompleted 155.0 MiB/174.0 MiB (147.9 MiB/s) with 1 file(s) remaining\nCompleted 155.2 MiB/174.0 MiB (148.1 MiB/s) with 1 file(s) remaining\nCompleted 155.5 MiB/174.0 MiB (148.2 MiB/s) with 1 file(s) remaining\nCompleted 155.8 MiB/174.0 MiB (148.4 MiB/s) with 1 file(s) remaining\nCompleted 156.0 MiB/174.0 MiB (148.4 MiB/s) with 1 file(s) remaining\nCompleted 156.2 MiB/174.0 MiB (148.1 MiB/s) with 1 file(s) remaining\nCompleted 156.5 MiB/174.0 MiB (148.2 MiB/s) with 1 file(s) remaining\nCompleted 156.8 MiB/174.0 MiB (148.3 MiB/s) with 1 file(s) remaining\nCompleted 157.0 MiB/174.0 MiB (148.5 MiB/s) with 1 file(s) remaining\nCompleted 157.2 MiB/174.0 MiB (148.6 MiB/s) with 1 file(s) remaining\nCompleted 157.5 MiB/174.0 MiB (148.8 MiB/s) with 1 file(s) remaining\nCompleted 157.8 MiB/174.0 MiB (148.6 MiB/s) with 1 file(s) remaining\nCompleted 158.0 MiB/174.0 MiB (148.7 MiB/s) with 1 file(s) remaining\nCompleted 158.2 MiB/174.0 MiB (148.7 MiB/s) with 1 file(s) remaining\nCompleted 158.5 MiB/174.0 MiB (148.7 MiB/s) with 1 file(s) remaining\nCompleted 158.8 MiB/174.0 MiB (148.8 MiB/s) with 1 file(s) remaining\nCompleted 159.0 MiB/174.0 MiB (148.9 MiB/s) with 1 file(s) remaining\nCompleted 159.2 MiB/174.0 MiB (148.9 MiB/s) with 1 file(s) remaining\nCompleted 159.5 MiB/174.0 MiB (148.9 MiB/s) with 1 file(s) remaining\nCompleted 159.8 MiB/174.0 MiB (149.1 MiB/s) with 1 file(s) remaining\nCompleted 160.0 MiB/174.0 MiB (149.2 MiB/s) with 1 file(s) remaining\nCompleted 160.2 MiB/174.0 MiB (149.1 MiB/s) with 1 file(s) remaining\nCompleted 160.5 MiB/174.0 MiB (149.1 MiB/s) with 1 file(s) remaining\nCompleted 160.8 MiB/174.0 MiB (149.3 MiB/s) with 1 file(s) remaining\nCompleted 161.0 MiB/174.0 MiB (149.3 MiB/s) with 1 file(s) remaining\nCompleted 161.2 MiB/174.0 MiB (149.5 MiB/s) with 1 file(s) remaining\nCompleted 161.5 MiB/174.0 MiB (149.5 MiB/s) with 1 file(s) remaining\nCompleted 161.8 MiB/174.0 MiB (149.6 MiB/s) with 1 file(s) remaining\nCompleted 162.0 MiB/174.0 MiB (149.5 MiB/s) with 1 file(s) remaining\nCompleted 162.2 MiB/174.0 MiB (149.7 MiB/s) with 1 file(s) remaining\nCompleted 162.5 MiB/174.0 MiB (149.7 MiB/s) with 1 file(s) remaining\nCompleted 162.8 MiB/174.0 MiB (149.9 MiB/s) with 1 file(s) remaining\nCompleted 163.0 MiB/174.0 MiB (149.8 MiB/s) with 1 file(s) remaining\nCompleted 163.2 MiB/174.0 MiB (150.0 MiB/s) with 1 file(s) remaining\nCompleted 163.5 MiB/174.0 MiB (149.8 MiB/s) with 1 file(s) remaining\nCompleted 163.8 MiB/174.0 MiB (149.9 MiB/s) with 1 file(s) remaining\nCompleted 164.0 MiB/174.0 MiB (149.8 MiB/s) with 1 file(s) remaining\nCompleted 164.2 MiB/174.0 MiB (150.0 MiB/s) with 1 file(s) remaining\nCompleted 164.5 MiB/174.0 MiB (149.9 MiB/s) with 1 file(s) remaining\nCompleted 164.8 MiB/174.0 MiB (150.1 MiB/s) with 1 file(s) remaining\nCompleted 165.0 MiB/174.0 MiB (150.1 MiB/s) with 1 file(s) remaining\nCompleted 165.2 MiB/174.0 MiB (150.3 MiB/s) with 1 file(s) remaining\nCompleted 165.5 MiB/174.0 MiB (150.5 MiB/s) with 1 file(s) remaining\nCompleted 165.8 MiB/174.0 MiB (150.6 MiB/s) with 1 file(s) remaining\nCompleted 166.0 MiB/174.0 MiB (150.6 MiB/s) with 1 file(s) remaining\nCompleted 166.2 MiB/174.0 MiB (150.7 MiB/s) with 1 file(s) remaining\nCompleted 166.5 MiB/174.0 MiB (150.9 MiB/s) with 1 file(s) remaining\nCompleted 166.8 MiB/174.0 MiB (150.9 MiB/s) with 1 file(s) remaining\nCompleted 167.0 MiB/174.0 MiB (151.1 MiB/s) with 1 file(s) remaining\nCompleted 167.2 MiB/174.0 MiB (151.2 MiB/s) with 1 file(s) remaining\nCompleted 167.5 MiB/174.0 MiB (151.2 MiB/s) with 1 file(s) remaining\nCompleted 167.8 MiB/174.0 MiB (151.4 MiB/s) with 1 file(s) remaining\nCompleted 168.0 MiB/174.0 MiB (151.5 MiB/s) with 1 file(s) remaining\nCompleted 168.2 MiB/174.0 MiB (151.6 MiB/s) with 1 file(s) remaining\nCompleted 168.5 MiB/174.0 MiB (151.7 MiB/s) with 1 file(s) remaining\nCompleted 168.8 MiB/174.0 MiB (151.8 MiB/s) with 1 file(s) remaining\nCompleted 169.0 MiB/174.0 MiB (151.9 MiB/s) with 1 file(s) remaining\nCompleted 169.2 MiB/174.0 MiB (152.1 MiB/s) with 1 file(s) remaining\nCompleted 169.5 MiB/174.0 MiB (152.2 MiB/s) with 1 file(s) remaining\nCompleted 169.8 MiB/174.0 MiB (152.3 MiB/s) with 1 file(s) remaining\nCompleted 170.0 MiB/174.0 MiB (152.4 MiB/s) with 1 file(s) remaining\nCompleted 170.2 MiB/174.0 MiB (152.5 MiB/s) with 1 file(s) remaining\nCompleted 170.5 MiB/174.0 MiB (152.5 MiB/s) with 1 file(s) remaining\nCompleted 170.8 MiB/174.0 MiB (152.6 MiB/s) with 1 file(s) remaining\nCompleted 171.0 MiB/174.0 MiB (152.7 MiB/s) with 1 file(s) remaining\nCompleted 171.2 MiB/174.0 MiB (152.8 MiB/s) with 1 file(s) remaining\nCompleted 171.5 MiB/174.0 MiB (152.9 MiB/s) with 1 file(s) remaining\nCompleted 171.8 MiB/174.0 MiB (153.0 MiB/s) with 1 file(s) remaining\nCompleted 172.0 MiB/174.0 MiB (153.0 MiB/s) with 1 file(s) remaining\nCompleted 172.2 MiB/174.0 MiB (138.4 MiB/s) with 1 file(s) remaining\nCompleted 172.5 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\nCompleted 172.8 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\nCompleted 173.0 MiB/174.0 MiB (138.5 MiB/s) with 1 file(s) remaining\nCompleted 173.2 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\nCompleted 173.5 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\nCompleted 173.8 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\nCompleted 174.0 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\nCompleted 174.0 MiB/174.0 MiB (138.6 MiB/s) with 1 file(s) remaining\ndownload: s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11022022_vnet_130_cases_288_256_320/epoch5_trainloss_0.066_validacc_0.937.pth to TEST/epoch5_trainloss_0.066_validacc_0.937.pth\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "/databricks/python/bin/aws s3 cp s3://hli-imaging-sdrad-pdx/Whole_Body_Composition_v2/trained_model_water/11022022_vnet_130_cases_288_256_320/epoch5_trainloss_0.066_validacc_0.937.pth /databricks/driver/TEST/epoch5_trainloss_0.066_validacc_0.937.pth --sse --acl bucket-owner-full-control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49acbe8c-705a-4d95-9d06-3848dd63a876",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# (7) predict cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c693a7d7-4ea2-41e4-bd86-1ef37366f524",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu_epoch34_trainloss_0.037_validacc_0.959.pth\n",
       "cpu_epoch35_trainloss_0.037_validacc_0.961.pth\n",
       "epoch10_trainloss_0.051_validacc_0.946.pth\n",
       "epoch11_trainloss_0.052_validacc_0.944.pth\n",
       "epoch12_trainloss_0.049_validacc_0.949.pth\n",
       "epoch13_trainloss_0.048_validacc_0.944.pth\n",
       "epoch14_trainloss_0.046_validacc_0.951.pth\n",
       "epoch15_trainloss_0.047_validacc_0.941.pth\n",
       "epoch16_trainloss_0.046_validacc_0.953.pth\n",
       "epoch17_trainloss_0.056_validacc_0.940.pth\n",
       "epoch18_trainloss_0.045_validacc_0.947.pth\n",
       "epoch19_trainloss_0.044_validacc_0.954.pth\n",
       "epoch1_trainloss_1.046_validacc_0.243.pth\n",
       "epoch20_trainloss_0.043_validacc_0.951.pth\n",
       "epoch21_trainloss_0.041_validacc_0.955.pth\n",
       "epoch22_trainloss_0.041_validacc_0.956.pth\n",
       "epoch23_trainloss_0.040_validacc_0.956.pth\n",
       "epoch24_trainloss_0.040_validacc_0.951.pth\n",
       "epoch25_trainloss_0.041_validacc_0.951.pth\n",
       "epoch26_trainloss_0.040_validacc_0.951.pth\n",
       "epoch27_trainloss_0.038_validacc_0.954.pth\n",
       "epoch28_trainloss_0.038_validacc_0.958.pth\n",
       "epoch29_trainloss_0.038_validacc_0.959.pth\n",
       "epoch2_trainloss_0.382_validacc_0.668.pth\n",
       "epoch30_trainloss_0.037_validacc_0.958.pth\n",
       "epoch31_trainloss_0.039_validacc_0.948.pth\n",
       "epoch32_trainloss_0.040_validacc_0.957.pth\n",
       "epoch33_trainloss_0.038_validacc_0.956.pth\n",
       "epoch34_trainloss_0.037_validacc_0.959.pth\n",
       "epoch35_trainloss_0.037_validacc_0.961.pth\n",
       "epoch36_trainloss_0.036_validacc_0.955.pth\n",
       "epoch3_trainloss_0.295_validacc_0.793.pth\n",
       "epoch4_trainloss_0.158_validacc_0.835.pth\n",
       "epoch5_trainloss_0.138_validacc_0.832.pth\n",
       "epoch6_trainloss_0.115_validacc_0.811.pth\n",
       "epoch7_trainloss_0.084_validacc_0.935.pth\n",
       "epoch8_trainloss_0.055_validacc_0.935.pth\n",
       "epoch9_trainloss_0.055_validacc_0.945.pth\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "cpu_epoch34_trainloss_0.037_validacc_0.959.pth\ncpu_epoch35_trainloss_0.037_validacc_0.961.pth\nepoch10_trainloss_0.051_validacc_0.946.pth\nepoch11_trainloss_0.052_validacc_0.944.pth\nepoch12_trainloss_0.049_validacc_0.949.pth\nepoch13_trainloss_0.048_validacc_0.944.pth\nepoch14_trainloss_0.046_validacc_0.951.pth\nepoch15_trainloss_0.047_validacc_0.941.pth\nepoch16_trainloss_0.046_validacc_0.953.pth\nepoch17_trainloss_0.056_validacc_0.940.pth\nepoch18_trainloss_0.045_validacc_0.947.pth\nepoch19_trainloss_0.044_validacc_0.954.pth\nepoch1_trainloss_1.046_validacc_0.243.pth\nepoch20_trainloss_0.043_validacc_0.951.pth\nepoch21_trainloss_0.041_validacc_0.955.pth\nepoch22_trainloss_0.041_validacc_0.956.pth\nepoch23_trainloss_0.040_validacc_0.956.pth\nepoch24_trainloss_0.040_validacc_0.951.pth\nepoch25_trainloss_0.041_validacc_0.951.pth\nepoch26_trainloss_0.040_validacc_0.951.pth\nepoch27_trainloss_0.038_validacc_0.954.pth\nepoch28_trainloss_0.038_validacc_0.958.pth\nepoch29_trainloss_0.038_validacc_0.959.pth\nepoch2_trainloss_0.382_validacc_0.668.pth\nepoch30_trainloss_0.037_validacc_0.958.pth\nepoch31_trainloss_0.039_validacc_0.948.pth\nepoch32_trainloss_0.040_validacc_0.957.pth\nepoch33_trainloss_0.038_validacc_0.956.pth\nepoch34_trainloss_0.037_validacc_0.959.pth\nepoch35_trainloss_0.037_validacc_0.961.pth\nepoch36_trainloss_0.036_validacc_0.955.pth\nepoch3_trainloss_0.295_validacc_0.793.pth\nepoch4_trainloss_0.158_validacc_0.835.pth\nepoch5_trainloss_0.138_validacc_0.832.pth\nepoch6_trainloss_0.115_validacc_0.811.pth\nepoch7_trainloss_0.084_validacc_0.935.pth\nepoch8_trainloss_0.055_validacc_0.935.pth\nepoch9_trainloss_0.055_validacc_0.945.pth\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "ls ./TEST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c95b75a1-cd22-4874-ae81-55071c79cc99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022-11-09 03:43:07.360 | INFO     | __main__:__init__:10 - begin initialize model struction\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "2022-11-09 03:43:07.360 | INFO     | __main__:__init__:10 - begin initialize model struction\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "n_classes = 2\n",
    "dim = [304, 256, 320]\n",
    "slice_interval = [197, 501]\n",
    "\n",
    "model = VNet_Parallelism(in_channels=1, classes=n_classes)\n",
    "model_dict = model.load_state_dict(torch.load('./TEST/cpu_epoch35_trainloss_0.037_validacc_0.961.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b959cfe-1d5d-4a17-9b26-c7ae10ff5f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from termcolor import *\n",
    "\n",
    "smooth = 1.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = np.ndarray.flatten(y_true)\n",
    "    y_pred_f = np.ndarray.flatten(y_pred)\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "class_map = {0: 0, 1: 1}\n",
    "def one_hot_encoding(category):\n",
    "    class_mask = np.zeros((304, 256, 320, 2), dtype=np.float32)\n",
    "    class_mask[:,:,:,0] = 1\n",
    "    for mask_value in class_map:\n",
    "        empty_holder = np.zeros((2))\n",
    "        empty_holder[class_map[mask_value]] = 1\n",
    "        class_mask[category == mask_value] = empty_holder\n",
    "    return class_mask\n",
    "\n",
    "def numerical_analysis(y_true, y_pred):\n",
    "    y_pred_f = np.ndarray.flatten(y_pred[:,:,:,1])\n",
    "    y_true_f = np.ndarray.flatten(y_true[:,:,:,1])\n",
    "    pred = np.sum(y_pred[:,:,:,1])\n",
    "    true = np.sum(y_true[:,:,:,1])\n",
    "    inter = np.sum(y_pred_f*y_true_f)\n",
    "    print('prediction: ', pred, ' true: ', true, 'intersection: ', inter, 'dice: ', 2*inter/(pred+true))\n",
    "    print(colored('--- false negative: ', 'red'), true-inter, colored('--- false positive: ', 'red'), pred-inter, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fac17f78-4e1e-423f-8a48-fb9ff1add9c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "male_val_id = ['BJ00000075', 'BJ00000060', 'BJ00000054', 'BJ00000050', 'BJ00000096']\n",
    "female_val_id = ['BJ00000071', 'BJ00000150', 'BJ00000077', 'BJ00000033', 'BJ00000120']\n",
    "test_ids = male_val_id + female_val_id\n",
    "\n",
    "# male_test_id = ['BJ00000016', 'BJ00000125', 'BJ00000147', 'BJ00000044', 'BJ00000114']\n",
    "# female_test_id = ['BJ00000100', 'BJ00000090', 'BJ00000110', 'BJ00000108', 'BJ00000002']\n",
    "male_test_id = ['BJ00000016', 'BJ00000044', 'BJ00000114']\n",
    "female_test_id = ['BJ00000100', 'BJ00000002']\n",
    "test_ids_2 = male_test_id + female_test_id\n",
    "\n",
    "test_ids_all = test_ids + test_ids_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35780acf-751d-4c4b-9a74-59b0ee8a2952",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "testset = DataGenerator(list_IDs=test_ids_all, \n",
    "                        file_path=data_path,\n",
    "                        num_classes=n_classes,\n",
    "                         resize_shape=dim,\n",
    "                         slice_interval=slice_interval,\n",
    "                         x_prefix=x_prefix, \n",
    "                         y_prefix=y_prefix,\n",
    "                         x_postfix=x_postfix, \n",
    "                         y_postfix=y_postfix)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size)\n",
    "# evalmetric = EvalMetric(dim, batch_size, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a968e4c0-071a-4a10-9848-9215bde29ea9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# --- run all cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78e4021a-3aba-4dfb-96ff-2e58b8dae995",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ------ cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "027746a7-c819-43f4-91c4-f0686e548cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BJ00000075',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9701602630808448\n",
       "[8] numerical analysis:\n",
       "prediction:  2276439.0  true:  2235329.0 intersection:  2188569.0 dice:  0.9701602564670878\n",
       "\u001b[31m--- false negative: \u001b[0m 46760.0 \u001b[31m--- false positive: \u001b[0m 87870.0 \n",
       "\n",
       "('BJ00000060',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9714418922825719\n",
       "[8] numerical analysis:\n",
       "prediction:  2188717.0  true:  2231848.0 intersection:  2147161.0 dice:  0.9714418858222874\n",
       "\u001b[31m--- false negative: \u001b[0m 84687.0 \u001b[31m--- false positive: \u001b[0m 41556.0 \n",
       "\n",
       "('BJ00000054',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9790736583116917\n",
       "[8] numerical analysis:\n",
       "prediction:  2143195.0  true:  2153685.0 intersection:  2103481.0 dice:  0.9790736534415669\n",
       "\u001b[31m--- false negative: \u001b[0m 50204.0 \u001b[31m--- false positive: \u001b[0m 39714.0 \n",
       "\n",
       "('BJ00000050',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9657918170875112\n",
       "[8] numerical analysis:\n",
       "prediction:  1807174.0  true:  1787462.0 intersection:  1735835.0 dice:  0.9657918075710586\n",
       "\u001b[31m--- false negative: \u001b[0m 51627.0 \u001b[31m--- false positive: \u001b[0m 71339.0 \n",
       "\n",
       "('BJ00000096',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9509074996393237\n",
       "[8] numerical analysis:\n",
       "prediction:  1610789.0  true:  1522214.0 intersection:  1489598.0 dice:  0.9509074839698526\n",
       "\u001b[31m--- false negative: \u001b[0m 32616.0 \u001b[31m--- false positive: \u001b[0m 121191.0 \n",
       "\n",
       "('BJ00000071',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9536359084111893\n",
       "[8] numerical analysis:\n",
       "prediction:  1480786.0  true:  1434853.0 intersection:  1390229.0 dice:  0.9536358925093265\n",
       "\u001b[31m--- false negative: \u001b[0m 44624.0 \u001b[31m--- false positive: \u001b[0m 90557.0 \n",
       "\n",
       "('BJ00000150',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9587392178348917\n",
       "[8] numerical analysis:\n",
       "prediction:  1510995.0  true:  1432986.0 intersection:  1411255.0 dice:  0.9587392038195899\n",
       "\u001b[31m--- false negative: \u001b[0m 21731.0 \u001b[31m--- false positive: \u001b[0m 99740.0 \n",
       "\n",
       "('BJ00000077',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9499994392284652\n",
       "[8] numerical analysis:\n",
       "prediction:  1393585.0  true:  1370463.0 intersection:  1312922.0 dice:  0.9499994211388514\n",
       "\u001b[31m--- false negative: \u001b[0m 57541.0 \u001b[31m--- false positive: \u001b[0m 80663.0 \n",
       "\n",
       "('BJ00000033',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9635765570892866\n",
       "[8] numerical analysis:\n",
       "prediction:  1213505.0  true:  1190001.0 intersection:  1157981.0 dice:  0.96357654193499\n",
       "\u001b[31m--- false negative: \u001b[0m 32020.0 \u001b[31m--- false positive: \u001b[0m 55524.0 \n",
       "\n",
       "('BJ00000120',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9517722005900271\n",
       "[8] numerical analysis:\n",
       "prediction:  1272968.0  true:  1227933.0 intersection:  1190144.0 dice:  0.9517721813058574\n",
       "\u001b[31m--- false negative: \u001b[0m 37789.0 \u001b[31m--- false positive: \u001b[0m 82824.0 \n",
       "\n",
       "('BJ00000016',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9728284813448417\n",
       "[8] numerical analysis:\n",
       "prediction:  2327033.0  true:  2371452.0 intersection:  2285410.0 dice:  0.9728284755618034\n",
       "\u001b[31m--- false negative: \u001b[0m 86042.0 \u001b[31m--- false positive: \u001b[0m 41623.0 \n",
       "\n",
       "('BJ00000044',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9721361363830194\n",
       "[8] numerical analysis:\n",
       "prediction:  1761583.0  true:  1709112.0 intersection:  1686994.0 dice:  0.9721361283546955\n",
       "\u001b[31m--- false negative: \u001b[0m 22118.0 \u001b[31m--- false positive: \u001b[0m 74589.0 \n",
       "\n",
       "('BJ00000114',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9489217853409411\n",
       "[8] numerical analysis:\n",
       "prediction:  1595163.0  true:  1494666.0 intersection:  1466003.0 dice:  0.9489217688098597\n",
       "\u001b[31m--- false negative: \u001b[0m 28663.0 \u001b[31m--- false positive: \u001b[0m 129160.0 \n",
       "\n",
       "('BJ00000100',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9581033892502567\n",
       "[8] numerical analysis:\n",
       "prediction:  1415251.0  true:  1359698.0 intersection:  1329344.0 dice:  0.9581033741521016\n",
       "\u001b[31m--- false negative: \u001b[0m 30354.0 \u001b[31m--- false positive: \u001b[0m 85907.0 \n",
       "\n",
       "('BJ00000002',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9722247060263315\n",
       "[8] numerical analysis:\n",
       "prediction:  1218015.0  true:  1208817.0 intersection:  1179713.0 dice:  0.9722246945812483\n",
       "\u001b[31m--- false negative: \u001b[0m 29104.0 \u001b[31m--- false positive: \u001b[0m 38302.0 \n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "('BJ00000075',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9701602630808448\n[8] numerical analysis:\nprediction:  2276439.0  true:  2235329.0 intersection:  2188569.0 dice:  0.9701602564670878\n\u001b[31m--- false negative: \u001b[0m 46760.0 \u001b[31m--- false positive: \u001b[0m 87870.0 \n\n('BJ00000060',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9714418922825719\n[8] numerical analysis:\nprediction:  2188717.0  true:  2231848.0 intersection:  2147161.0 dice:  0.9714418858222874\n\u001b[31m--- false negative: \u001b[0m 84687.0 \u001b[31m--- false positive: \u001b[0m 41556.0 \n\n('BJ00000054',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9790736583116917\n[8] numerical analysis:\nprediction:  2143195.0  true:  2153685.0 intersection:  2103481.0 dice:  0.9790736534415669\n\u001b[31m--- false negative: \u001b[0m 50204.0 \u001b[31m--- false positive: \u001b[0m 39714.0 \n\n('BJ00000050',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9657918170875112\n[8] numerical analysis:\nprediction:  1807174.0  true:  1787462.0 intersection:  1735835.0 dice:  0.9657918075710586\n\u001b[31m--- false negative: \u001b[0m 51627.0 \u001b[31m--- false positive: \u001b[0m 71339.0 \n\n('BJ00000096',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9509074996393237\n[8] numerical analysis:\nprediction:  1610789.0  true:  1522214.0 intersection:  1489598.0 dice:  0.9509074839698526\n\u001b[31m--- false negative: \u001b[0m 32616.0 \u001b[31m--- false positive: \u001b[0m 121191.0 \n\n('BJ00000071',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9536359084111893\n[8] numerical analysis:\nprediction:  1480786.0  true:  1434853.0 intersection:  1390229.0 dice:  0.9536358925093265\n\u001b[31m--- false negative: \u001b[0m 44624.0 \u001b[31m--- false positive: \u001b[0m 90557.0 \n\n('BJ00000150',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9587392178348917\n[8] numerical analysis:\nprediction:  1510995.0  true:  1432986.0 intersection:  1411255.0 dice:  0.9587392038195899\n\u001b[31m--- false negative: \u001b[0m 21731.0 \u001b[31m--- false positive: \u001b[0m 99740.0 \n\n('BJ00000077',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9499994392284652\n[8] numerical analysis:\nprediction:  1393585.0  true:  1370463.0 intersection:  1312922.0 dice:  0.9499994211388514\n\u001b[31m--- false negative: \u001b[0m 57541.0 \u001b[31m--- false positive: \u001b[0m 80663.0 \n\n('BJ00000033',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9635765570892866\n[8] numerical analysis:\nprediction:  1213505.0  true:  1190001.0 intersection:  1157981.0 dice:  0.96357654193499\n\u001b[31m--- false negative: \u001b[0m 32020.0 \u001b[31m--- false positive: \u001b[0m 55524.0 \n\n('BJ00000120',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9517722005900271\n[8] numerical analysis:\nprediction:  1272968.0  true:  1227933.0 intersection:  1190144.0 dice:  0.9517721813058574\n\u001b[31m--- false negative: \u001b[0m 37789.0 \u001b[31m--- false positive: \u001b[0m 82824.0 \n\n('BJ00000016',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9728284813448417\n[8] numerical analysis:\nprediction:  2327033.0  true:  2371452.0 intersection:  2285410.0 dice:  0.9728284755618034\n\u001b[31m--- false negative: \u001b[0m 86042.0 \u001b[31m--- false positive: \u001b[0m 41623.0 \n\n('BJ00000044',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9721361363830194\n[8] numerical analysis:\nprediction:  1761583.0  true:  1709112.0 intersection:  1686994.0 dice:  0.9721361283546955\n\u001b[31m--- false negative: \u001b[0m 22118.0 \u001b[31m--- false positive: \u001b[0m 74589.0 \n\n('BJ00000114',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9489217853409411\n[8] numerical analysis:\nprediction:  1595163.0  true:  1494666.0 intersection:  1466003.0 dice:  0.9489217688098597\n\u001b[31m--- false negative: \u001b[0m 28663.0 \u001b[31m--- false positive: \u001b[0m 129160.0 \n\n('BJ00000100',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9581033892502567\n[8] numerical analysis:\nprediction:  1415251.0  true:  1359698.0 intersection:  1329344.0 dice:  0.9581033741521016\n\u001b[31m--- false negative: \u001b[0m 30354.0 \u001b[31m--- false positive: \u001b[0m 85907.0 \n\n('BJ00000002',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9722247060263315\n[8] numerical analysis:\nprediction:  1218015.0  true:  1208817.0 intersection:  1179713.0 dice:  0.9722246945812483\n\u001b[31m--- false negative: \u001b[0m 29104.0 \u001b[31m--- false positive: \u001b[0m 38302.0 \n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cpu_epoch35_trainloss_0.037_validacc_0.961.pth\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, x, y in test_loader:\n",
    "#       if idx[0] == 'BJ00000060':\n",
    "        print(idx)\n",
    "        print('[1] shape of fat and fat_label: ', x.shape, y.shape)\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        with autocast():\n",
    "            # Step 1, predict\n",
    "            logits = model(x.to(next(model.parameters()).device))\n",
    "            logits = F.softmax(logits, dim=1)\n",
    "            print('[2] shape of logits: ', logits.shape)\n",
    "            \n",
    "            # Step 2, argmax\n",
    "            fat_argmax = np.argmax(logits.cpu(), axis=1)\n",
    "            print('[3] argmax of fat_prediction: ', fat_argmax.shape)\n",
    "            fat_argmax = np.reshape(fat_argmax, dim)\n",
    "            print('[4] reshape of fat_argmax: ', fat_argmax.shape)\n",
    "            \n",
    "            # Step 3, one-hot encoding\n",
    "            fat_argmax = zoom(fat_argmax, (304/fat_argmax.shape[0], 256/fat_argmax.shape[1], 320/fat_argmax.shape[2]), order=0, mode='nearest') # resize to [288,260,320]\n",
    "            prediction_label_origin = one_hot_encoding(fat_argmax)\n",
    "            print('[5] one-hot encoding of prediction: ', prediction_label_origin.shape)\n",
    "            input_y = np.load(data_path + '/' + idx[0] + '/water_label.npy')\n",
    "            slice_y = input_y[slice_interval[0]:slice_interval[1], 2:-2, :]\n",
    "            img_fat_label = one_hot_encoding(slice_y)\n",
    "            print('[6] one-hot encoding of origin: ', img_fat_label.shape)\n",
    "            \n",
    "            # Step 6, output dice for each category\n",
    "            print('[7] dice of muscle: ', dice_coef(img_fat_label[:,:,:,1], prediction_label_origin[:,:,:,1]))\n",
    "            print('[8] numerical analysis:')\n",
    "            numerical_analysis(img_fat_label, prediction_label_origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f3cd7c-df67-429d-b4a0-c929a31f8ca5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BJ00000075',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9701602630808448\n",
       "[8] numerical analysis:\n",
       "prediction:  2276439.0  true:  2235329.0 intersection:  2188569.0 dice:  0.9701602564670878\n",
       "\u001b[31m--- false negative: \u001b[0m 46760.0 \u001b[31m--- false positive: \u001b[0m 87870.0 \n",
       "\n",
       "('BJ00000060',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9714418922825719\n",
       "[8] numerical analysis:\n",
       "prediction:  2188717.0  true:  2231848.0 intersection:  2147161.0 dice:  0.9714418858222874\n",
       "\u001b[31m--- false negative: \u001b[0m 84687.0 \u001b[31m--- false positive: \u001b[0m 41556.0 \n",
       "\n",
       "('BJ00000054',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9790736583116917\n",
       "[8] numerical analysis:\n",
       "prediction:  2143195.0  true:  2153685.0 intersection:  2103481.0 dice:  0.9790736534415669\n",
       "\u001b[31m--- false negative: \u001b[0m 50204.0 \u001b[31m--- false positive: \u001b[0m 39714.0 \n",
       "\n",
       "('BJ00000050',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9657918170875112\n",
       "[8] numerical analysis:\n",
       "prediction:  1807174.0  true:  1787462.0 intersection:  1735835.0 dice:  0.9657918075710586\n",
       "\u001b[31m--- false negative: \u001b[0m 51627.0 \u001b[31m--- false positive: \u001b[0m 71339.0 \n",
       "\n",
       "('BJ00000096',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9509074996393237\n",
       "[8] numerical analysis:\n",
       "prediction:  1610789.0  true:  1522214.0 intersection:  1489598.0 dice:  0.9509074839698526\n",
       "\u001b[31m--- false negative: \u001b[0m 32616.0 \u001b[31m--- false positive: \u001b[0m 121191.0 \n",
       "\n",
       "('BJ00000071',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9536359084111893\n",
       "[8] numerical analysis:\n",
       "prediction:  1480786.0  true:  1434853.0 intersection:  1390229.0 dice:  0.9536358925093265\n",
       "\u001b[31m--- false negative: \u001b[0m 44624.0 \u001b[31m--- false positive: \u001b[0m 90557.0 \n",
       "\n",
       "('BJ00000150',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9587392178348917\n",
       "[8] numerical analysis:\n",
       "prediction:  1510995.0  true:  1432986.0 intersection:  1411255.0 dice:  0.9587392038195899\n",
       "\u001b[31m--- false negative: \u001b[0m 21731.0 \u001b[31m--- false positive: \u001b[0m 99740.0 \n",
       "\n",
       "('BJ00000077',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9499994392284652\n",
       "[8] numerical analysis:\n",
       "prediction:  1393585.0  true:  1370463.0 intersection:  1312922.0 dice:  0.9499994211388514\n",
       "\u001b[31m--- false negative: \u001b[0m 57541.0 \u001b[31m--- false positive: \u001b[0m 80663.0 \n",
       "\n",
       "('BJ00000033',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9635765570892866\n",
       "[8] numerical analysis:\n",
       "prediction:  1213505.0  true:  1190001.0 intersection:  1157981.0 dice:  0.96357654193499\n",
       "\u001b[31m--- false negative: \u001b[0m 32020.0 \u001b[31m--- false positive: \u001b[0m 55524.0 \n",
       "\n",
       "('BJ00000120',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9517722005900271\n",
       "[8] numerical analysis:\n",
       "prediction:  1272968.0  true:  1227933.0 intersection:  1190144.0 dice:  0.9517721813058574\n",
       "\u001b[31m--- false negative: \u001b[0m 37789.0 \u001b[31m--- false positive: \u001b[0m 82824.0 \n",
       "\n",
       "('BJ00000016',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9728284813448417\n",
       "[8] numerical analysis:\n",
       "prediction:  2327033.0  true:  2371452.0 intersection:  2285410.0 dice:  0.9728284755618034\n",
       "\u001b[31m--- false negative: \u001b[0m 86042.0 \u001b[31m--- false positive: \u001b[0m 41623.0 \n",
       "\n",
       "('BJ00000044',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9721361363830194\n",
       "[8] numerical analysis:\n",
       "prediction:  1761583.0  true:  1709112.0 intersection:  1686994.0 dice:  0.9721361283546955\n",
       "\u001b[31m--- false negative: \u001b[0m 22118.0 \u001b[31m--- false positive: \u001b[0m 74589.0 \n",
       "\n",
       "('BJ00000114',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9489217853409411\n",
       "[8] numerical analysis:\n",
       "prediction:  1595163.0  true:  1494666.0 intersection:  1466003.0 dice:  0.9489217688098597\n",
       "\u001b[31m--- false negative: \u001b[0m 28663.0 \u001b[31m--- false positive: \u001b[0m 129160.0 \n",
       "\n",
       "('BJ00000100',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9581033892502567\n",
       "[8] numerical analysis:\n",
       "prediction:  1415251.0  true:  1359698.0 intersection:  1329344.0 dice:  0.9581033741521016\n",
       "\u001b[31m--- false negative: \u001b[0m 30354.0 \u001b[31m--- false positive: \u001b[0m 85907.0 \n",
       "\n",
       "('BJ00000002',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9722247060263315\n",
       "[8] numerical analysis:\n",
       "prediction:  1218015.0  true:  1208817.0 intersection:  1179713.0 dice:  0.9722246945812483\n",
       "\u001b[31m--- false negative: \u001b[0m 29104.0 \u001b[31m--- false positive: \u001b[0m 38302.0 \n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "('BJ00000075',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9701602630808448\n[8] numerical analysis:\nprediction:  2276439.0  true:  2235329.0 intersection:  2188569.0 dice:  0.9701602564670878\n\u001b[31m--- false negative: \u001b[0m 46760.0 \u001b[31m--- false positive: \u001b[0m 87870.0 \n\n('BJ00000060',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9714418922825719\n[8] numerical analysis:\nprediction:  2188717.0  true:  2231848.0 intersection:  2147161.0 dice:  0.9714418858222874\n\u001b[31m--- false negative: \u001b[0m 84687.0 \u001b[31m--- false positive: \u001b[0m 41556.0 \n\n('BJ00000054',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9790736583116917\n[8] numerical analysis:\nprediction:  2143195.0  true:  2153685.0 intersection:  2103481.0 dice:  0.9790736534415669\n\u001b[31m--- false negative: \u001b[0m 50204.0 \u001b[31m--- false positive: \u001b[0m 39714.0 \n\n('BJ00000050',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9657918170875112\n[8] numerical analysis:\nprediction:  1807174.0  true:  1787462.0 intersection:  1735835.0 dice:  0.9657918075710586\n\u001b[31m--- false negative: \u001b[0m 51627.0 \u001b[31m--- false positive: \u001b[0m 71339.0 \n\n('BJ00000096',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9509074996393237\n[8] numerical analysis:\nprediction:  1610789.0  true:  1522214.0 intersection:  1489598.0 dice:  0.9509074839698526\n\u001b[31m--- false negative: \u001b[0m 32616.0 \u001b[31m--- false positive: \u001b[0m 121191.0 \n\n('BJ00000071',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9536359084111893\n[8] numerical analysis:\nprediction:  1480786.0  true:  1434853.0 intersection:  1390229.0 dice:  0.9536358925093265\n\u001b[31m--- false negative: \u001b[0m 44624.0 \u001b[31m--- false positive: \u001b[0m 90557.0 \n\n('BJ00000150',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9587392178348917\n[8] numerical analysis:\nprediction:  1510995.0  true:  1432986.0 intersection:  1411255.0 dice:  0.9587392038195899\n\u001b[31m--- false negative: \u001b[0m 21731.0 \u001b[31m--- false positive: \u001b[0m 99740.0 \n\n('BJ00000077',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9499994392284652\n[8] numerical analysis:\nprediction:  1393585.0  true:  1370463.0 intersection:  1312922.0 dice:  0.9499994211388514\n\u001b[31m--- false negative: \u001b[0m 57541.0 \u001b[31m--- false positive: \u001b[0m 80663.0 \n\n('BJ00000033',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9635765570892866\n[8] numerical analysis:\nprediction:  1213505.0  true:  1190001.0 intersection:  1157981.0 dice:  0.96357654193499\n\u001b[31m--- false negative: \u001b[0m 32020.0 \u001b[31m--- false positive: \u001b[0m 55524.0 \n\n('BJ00000120',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9517722005900271\n[8] numerical analysis:\nprediction:  1272968.0  true:  1227933.0 intersection:  1190144.0 dice:  0.9517721813058574\n\u001b[31m--- false negative: \u001b[0m 37789.0 \u001b[31m--- false positive: \u001b[0m 82824.0 \n\n('BJ00000016',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9728284813448417\n[8] numerical analysis:\nprediction:  2327033.0  true:  2371452.0 intersection:  2285410.0 dice:  0.9728284755618034\n\u001b[31m--- false negative: \u001b[0m 86042.0 \u001b[31m--- false positive: \u001b[0m 41623.0 \n\n('BJ00000044',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9721361363830194\n[8] numerical analysis:\nprediction:  1761583.0  true:  1709112.0 intersection:  1686994.0 dice:  0.9721361283546955\n\u001b[31m--- false negative: \u001b[0m 22118.0 \u001b[31m--- false positive: \u001b[0m 74589.0 \n\n('BJ00000114',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9489217853409411\n[8] numerical analysis:\nprediction:  1595163.0  true:  1494666.0 intersection:  1466003.0 dice:  0.9489217688098597\n\u001b[31m--- false negative: \u001b[0m 28663.0 \u001b[31m--- false positive: \u001b[0m 129160.0 \n\n('BJ00000100',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9581033892502567\n[8] numerical analysis:\nprediction:  1415251.0  true:  1359698.0 intersection:  1329344.0 dice:  0.9581033741521016\n\u001b[31m--- false negative: \u001b[0m 30354.0 \u001b[31m--- false positive: \u001b[0m 85907.0 \n\n('BJ00000002',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9722247060263315\n[8] numerical analysis:\nprediction:  1218015.0  true:  1208817.0 intersection:  1179713.0 dice:  0.9722246945812483\n\u001b[31m--- false negative: \u001b[0m 29104.0 \u001b[31m--- false positive: \u001b[0m 38302.0 \n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cpu_epoch35_trainloss_0.037_validacc_0.961.pth\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, x, y in test_loader:\n",
    "#       if idx[0] == 'BJ00000060':\n",
    "        print(idx)\n",
    "        print('[1] shape of fat and fat_label: ', x.shape, y.shape)\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        with autocast():\n",
    "            # Step 1, predict\n",
    "            logits = model(x.to(next(model.parameters()).device))\n",
    "            logits = F.softmax(logits, dim=1)\n",
    "            print('[2] shape of logits: ', logits.shape)\n",
    "            \n",
    "            # Step 2, argmax\n",
    "            fat_argmax = np.argmax(logits.cpu(), axis=1)\n",
    "            print('[3] argmax of fat_prediction: ', fat_argmax.shape)\n",
    "            fat_argmax = np.reshape(fat_argmax, dim)\n",
    "            print('[4] reshape of fat_argmax: ', fat_argmax.shape)\n",
    "            \n",
    "            # Step 3, one-hot encoding\n",
    "            fat_argmax = zoom(fat_argmax, (304/fat_argmax.shape[0], 256/fat_argmax.shape[1], 320/fat_argmax.shape[2]), order=0, mode='nearest') # resize to [288,260,320]\n",
    "            prediction_label_origin = one_hot_encoding(fat_argmax)\n",
    "            print('[5] one-hot encoding of prediction: ', prediction_label_origin.shape)\n",
    "            input_y = np.load(data_path + '/' + idx[0] + '/water_label.npy')\n",
    "            slice_y = input_y[slice_interval[0]:slice_interval[1], 2:-2, :]\n",
    "            img_fat_label = one_hot_encoding(slice_y)\n",
    "            print('[6] one-hot encoding of origin: ', img_fat_label.shape)\n",
    "            \n",
    "            # Step 6, output dice for each category\n",
    "            print('[7] dice of muscle: ', dice_coef(img_fat_label[:,:,:,1], prediction_label_origin[:,:,:,1]))\n",
    "            print('[8] numerical analysis:')\n",
    "            numerical_analysis(img_fat_label, prediction_label_origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cae40362-815d-4fea-ad56-f015cb1d62b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BJ00000075',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9677196510816715\n",
       "[8] numerical analysis:\n",
       "prediction:  2317254.0  true:  2235329.0 intersection:  2202812.0 dice:  0.9677196439911145\n",
       "\u001b[31m--- false negative: \u001b[0m 32517.0 \u001b[31m--- false positive: \u001b[0m 114442.0 \n",
       "\n",
       "('BJ00000060',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9748130539065712\n",
       "[8] numerical analysis:\n",
       "prediction:  2231336.0  true:  2231848.0 intersection:  2175385.0 dice:  0.9748130482633026\n",
       "\u001b[31m--- false negative: \u001b[0m 56463.0 \u001b[31m--- false positive: \u001b[0m 55951.0 \n",
       "\n",
       "('BJ00000054',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9775369969718064\n",
       "[8] numerical analysis:\n",
       "prediction:  2183223.0  true:  2153685.0 intersection:  2119744.0 dice:  0.9775369917923091\n",
       "\u001b[31m--- false negative: \u001b[0m 33941.0 \u001b[31m--- false positive: \u001b[0m 63479.0 \n",
       "\n",
       "('BJ00000050',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9635240951510422\n",
       "[8] numerical analysis:\n",
       "prediction:  1847978.0  true:  1787462.0 intersection:  1751417.0 dice:  0.9635240851176199\n",
       "\u001b[31m--- false negative: \u001b[0m 36045.0 \u001b[31m--- false positive: \u001b[0m 96561.0 \n",
       "\n",
       "('BJ00000096',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9465804582844158\n",
       "[8] numerical analysis:\n",
       "prediction:  1649789.0  true:  1522214.0 intersection:  1501278.0 dice:  0.9465804414434664\n",
       "\u001b[31m--- false negative: \u001b[0m 20936.0 \u001b[31m--- false positive: \u001b[0m 148511.0 \n",
       "\n",
       "('BJ00000071',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9510537132583368\n",
       "[8] numerical analysis:\n",
       "prediction:  1515237.0  true:  1434853.0 intersection:  1402847.0 dice:  0.9510536966668813\n",
       "\u001b[31m--- false negative: \u001b[0m 32006.0 \u001b[31m--- false positive: \u001b[0m 112390.0 \n",
       "\n",
       "('BJ00000150',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9526905717378868\n",
       "[8] numerical analysis:\n",
       "prediction:  1544094.0  true:  1432986.0 intersection:  1418118.0 dice:  0.9526905558466685\n",
       "\u001b[31m--- false negative: \u001b[0m 14868.0 \u001b[31m--- false positive: \u001b[0m 125976.0 \n",
       "\n",
       "('BJ00000077',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9481006663795523\n",
       "[8] numerical analysis:\n",
       "prediction:  1427491.0  true:  1370463.0 intersection:  1326371.0 dice:  0.9481006478305218\n",
       "\u001b[31m--- false negative: \u001b[0m 44092.0 \u001b[31m--- false positive: \u001b[0m 101120.0 \n",
       "\n",
       "('BJ00000033',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9584426562853592\n",
       "[8] numerical analysis:\n",
       "prediction:  1249254.0  true:  1190001.0 intersection:  1168943.0 dice:  0.9584426392484591\n",
       "\u001b[31m--- false negative: \u001b[0m 21058.0 \u001b[31m--- false positive: \u001b[0m 80311.0 \n",
       "\n",
       "('BJ00000120',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9484716167534363\n",
       "[8] numerical analysis:\n",
       "prediction:  1308668.0  true:  1227933.0 intersection:  1202947.0 dice:  0.9484715964394873\n",
       "\u001b[31m--- false negative: \u001b[0m 24986.0 \u001b[31m--- false positive: \u001b[0m 105721.0 \n",
       "\n",
       "('BJ00000016',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9749499082512317\n",
       "[8] numerical analysis:\n",
       "prediction:  2370845.0  true:  2371452.0 intersection:  2311751.0 dice:  0.9749499029689621\n",
       "\u001b[31m--- false negative: \u001b[0m 59701.0 \u001b[31m--- false positive: \u001b[0m 59094.0 \n",
       "\n",
       "('BJ00000044',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9645082387161816\n",
       "[8] numerical analysis:\n",
       "prediction:  1800293.0  true:  1709112.0 intersection:  1692425.0 dice:  0.9645082286028543\n",
       "\u001b[31m--- false negative: \u001b[0m 16687.0 \u001b[31m--- false positive: \u001b[0m 107868.0 \n",
       "\n",
       "('BJ00000114',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9418126873973416\n",
       "[8] numerical analysis:\n",
       "prediction:  1644385.0  true:  1494666.0 intersection:  1478199.0 dice:  0.941812668860748\n",
       "\u001b[31m--- false negative: \u001b[0m 16467.0 \u001b[31m--- false positive: \u001b[0m 166186.0 \n",
       "\n",
       "('BJ00000100',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9551323158174806\n",
       "[8] numerical analysis:\n",
       "prediction:  1444123.0  true:  1359698.0 intersection:  1339010.0 dice:  0.9551322998151451\n",
       "\u001b[31m--- false negative: \u001b[0m 20688.0 \u001b[31m--- false positive: \u001b[0m 105113.0 \n",
       "\n",
       "('BJ00000002',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9709775051391707\n",
       "[8] numerical analysis:\n",
       "prediction:  1242453.0  true:  1208817.0 intersection:  1190064.0 dice:  0.9709774932993918\n",
       "\u001b[31m--- false negative: \u001b[0m 18753.0 \u001b[31m--- false positive: \u001b[0m 52389.0 \n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "('BJ00000075',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9677196510816715\n[8] numerical analysis:\nprediction:  2317254.0  true:  2235329.0 intersection:  2202812.0 dice:  0.9677196439911145\n\u001b[31m--- false negative: \u001b[0m 32517.0 \u001b[31m--- false positive: \u001b[0m 114442.0 \n\n('BJ00000060',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9748130539065712\n[8] numerical analysis:\nprediction:  2231336.0  true:  2231848.0 intersection:  2175385.0 dice:  0.9748130482633026\n\u001b[31m--- false negative: \u001b[0m 56463.0 \u001b[31m--- false positive: \u001b[0m 55951.0 \n\n('BJ00000054',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9775369969718064\n[8] numerical analysis:\nprediction:  2183223.0  true:  2153685.0 intersection:  2119744.0 dice:  0.9775369917923091\n\u001b[31m--- false negative: \u001b[0m 33941.0 \u001b[31m--- false positive: \u001b[0m 63479.0 \n\n('BJ00000050',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9635240951510422\n[8] numerical analysis:\nprediction:  1847978.0  true:  1787462.0 intersection:  1751417.0 dice:  0.9635240851176199\n\u001b[31m--- false negative: \u001b[0m 36045.0 \u001b[31m--- false positive: \u001b[0m 96561.0 \n\n('BJ00000096',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9465804582844158\n[8] numerical analysis:\nprediction:  1649789.0  true:  1522214.0 intersection:  1501278.0 dice:  0.9465804414434664\n\u001b[31m--- false negative: \u001b[0m 20936.0 \u001b[31m--- false positive: \u001b[0m 148511.0 \n\n('BJ00000071',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9510537132583368\n[8] numerical analysis:\nprediction:  1515237.0  true:  1434853.0 intersection:  1402847.0 dice:  0.9510536966668813\n\u001b[31m--- false negative: \u001b[0m 32006.0 \u001b[31m--- false positive: \u001b[0m 112390.0 \n\n('BJ00000150',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9526905717378868\n[8] numerical analysis:\nprediction:  1544094.0  true:  1432986.0 intersection:  1418118.0 dice:  0.9526905558466685\n\u001b[31m--- false negative: \u001b[0m 14868.0 \u001b[31m--- false positive: \u001b[0m 125976.0 \n\n('BJ00000077',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9481006663795523\n[8] numerical analysis:\nprediction:  1427491.0  true:  1370463.0 intersection:  1326371.0 dice:  0.9481006478305218\n\u001b[31m--- false negative: \u001b[0m 44092.0 \u001b[31m--- false positive: \u001b[0m 101120.0 \n\n('BJ00000033',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9584426562853592\n[8] numerical analysis:\nprediction:  1249254.0  true:  1190001.0 intersection:  1168943.0 dice:  0.9584426392484591\n\u001b[31m--- false negative: \u001b[0m 21058.0 \u001b[31m--- false positive: \u001b[0m 80311.0 \n\n('BJ00000120',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9484716167534363\n[8] numerical analysis:\nprediction:  1308668.0  true:  1227933.0 intersection:  1202947.0 dice:  0.9484715964394873\n\u001b[31m--- false negative: \u001b[0m 24986.0 \u001b[31m--- false positive: \u001b[0m 105721.0 \n\n('BJ00000016',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9749499082512317\n[8] numerical analysis:\nprediction:  2370845.0  true:  2371452.0 intersection:  2311751.0 dice:  0.9749499029689621\n\u001b[31m--- false negative: \u001b[0m 59701.0 \u001b[31m--- false positive: \u001b[0m 59094.0 \n\n('BJ00000044',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9645082387161816\n[8] numerical analysis:\nprediction:  1800293.0  true:  1709112.0 intersection:  1692425.0 dice:  0.9645082286028543\n\u001b[31m--- false negative: \u001b[0m 16687.0 \u001b[31m--- false positive: \u001b[0m 107868.0 \n\n('BJ00000114',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9418126873973416\n[8] numerical analysis:\nprediction:  1644385.0  true:  1494666.0 intersection:  1478199.0 dice:  0.941812668860748\n\u001b[31m--- false negative: \u001b[0m 16467.0 \u001b[31m--- false positive: \u001b[0m 166186.0 \n\n('BJ00000100',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9551323158174806\n[8] numerical analysis:\nprediction:  1444123.0  true:  1359698.0 intersection:  1339010.0 dice:  0.9551322998151451\n\u001b[31m--- false negative: \u001b[0m 20688.0 \u001b[31m--- false positive: \u001b[0m 105113.0 \n\n('BJ00000002',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9709775051391707\n[8] numerical analysis:\nprediction:  1242453.0  true:  1208817.0 intersection:  1190064.0 dice:  0.9709774932993918\n\u001b[31m--- false negative: \u001b[0m 18753.0 \u001b[31m--- false positive: \u001b[0m 52389.0 \n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cpu_epoch34_trainloss_0.037_validacc_0.959.pth\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, x, y in test_loader:\n",
    "#       if idx[0] == 'BJ00000060':\n",
    "        print(idx)\n",
    "        print('[1] shape of fat and fat_label: ', x.shape, y.shape)\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        with autocast():\n",
    "            # Step 1, predict\n",
    "            logits = model(x.to(next(model.parameters()).device))\n",
    "            logits = F.softmax(logits, dim=1)\n",
    "            print('[2] shape of logits: ', logits.shape)\n",
    "            \n",
    "            # Step 2, argmax\n",
    "            fat_argmax = np.argmax(logits.cpu(), axis=1)\n",
    "            print('[3] argmax of fat_prediction: ', fat_argmax.shape)\n",
    "            fat_argmax = np.reshape(fat_argmax, dim)\n",
    "            print('[4] reshape of fat_argmax: ', fat_argmax.shape)\n",
    "            \n",
    "            # Step 3, one-hot encoding\n",
    "            fat_argmax = zoom(fat_argmax, (304/fat_argmax.shape[0], 256/fat_argmax.shape[1], 320/fat_argmax.shape[2]), order=0, mode='nearest') # resize to [288,260,320]\n",
    "            prediction_label_origin = one_hot_encoding(fat_argmax)\n",
    "            print('[5] one-hot encoding of prediction: ', prediction_label_origin.shape)\n",
    "            input_y = np.load(data_path + '/' + idx[0] + '/water_label.npy')\n",
    "            slice_y = input_y[slice_interval[0]:slice_interval[1], 2:-2, :]\n",
    "            img_fat_label = one_hot_encoding(slice_y)\n",
    "            print('[6] one-hot encoding of origin: ', img_fat_label.shape)\n",
    "            \n",
    "            # Step 6, output dice for each category\n",
    "            print('[7] dice of muscle: ', dice_coef(img_fat_label[:,:,:,1], prediction_label_origin[:,:,:,1]))\n",
    "            print('[8] numerical analysis:')\n",
    "            numerical_analysis(img_fat_label, prediction_label_origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "847170b6-76ba-4960-a228-903f3569cafd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea0c149b-a9a6-438e-8cb6-45dd7b891330",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ------ gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fad21b1d-8c4e-49b5-8cfc-4a66038af940",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BJ00000075',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.7073269423695667\n",
       "[8] numerical analysis:\n",
       "prediction:  1585377.0  true:  2235329.0 intersection:  1351244.0 dice:  0.7073268657677403\n",
       "\u001b[31m--- false negative: \u001b[0m 884085.0 \u001b[31m--- false positive: \u001b[0m 234133.0 \n",
       "\n",
       "('BJ00000060',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.412001094481966\n",
       "[8] numerical analysis:\n",
       "prediction:  976975.0  true:  2231848.0 intersection:  661019.0 dice:  0.41200091123754723\n",
       "\u001b[31m--- false negative: \u001b[0m 1570829.0 \u001b[31m--- false positive: \u001b[0m 315956.0 \n",
       "\n",
       "('BJ00000054',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.36696324066331304\n",
       "[8] numerical analysis:\n",
       "prediction:  1053608.0  true:  2153685.0 intersection:  588479.0 dice:  0.3669630432891538\n",
       "\u001b[31m--- false negative: \u001b[0m 1565206.0 \u001b[31m--- false positive: \u001b[0m 465129.0 \n",
       "\n",
       "('BJ00000050',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.7864822696508544\n",
       "[8] numerical analysis:\n",
       "prediction:  1486415.0  true:  1787462.0 intersection:  1287423.0 dice:  0.7864822044322374\n",
       "\u001b[31m--- false negative: \u001b[0m 500039.0 \u001b[31m--- false positive: \u001b[0m 198992.0 \n",
       "\n",
       "('BJ00000096',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.7268121668137858\n",
       "[8] numerical analysis:\n",
       "prediction:  1072055.0  true:  1522214.0 intersection:  942773.0 dice:  0.7268120615094271\n",
       "\u001b[31m--- false negative: \u001b[0m 579441.0 \u001b[31m--- false positive: \u001b[0m 129282.0 \n",
       "\n",
       "('BJ00000071',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9533588915141971\n",
       "[8] numerical analysis:\n",
       "prediction:  1481629.0  true:  1434853.0 intersection:  1390227.0 dice:  0.9533588755219473\n",
       "\u001b[31m--- false negative: \u001b[0m 44626.0 \u001b[31m--- false positive: \u001b[0m 91402.0 \n",
       "\n",
       "('BJ00000150',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9503041407997156\n",
       "[8] numerical analysis:\n",
       "prediction:  1537019.0  true:  1432986.0 intersection:  1411204.0 dice:  0.9503041240671312\n",
       "\u001b[31m--- false negative: \u001b[0m 21782.0 \u001b[31m--- false positive: \u001b[0m 125815.0 \n",
       "\n",
       "('BJ00000077',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9132995002248623\n",
       "[8] numerical analysis:\n",
       "prediction:  1504629.0  true:  1370463.0 intersection:  1312910.0 dice:  0.9132994700691317\n",
       "\u001b[31m--- false negative: \u001b[0m 57553.0 \u001b[31m--- false positive: \u001b[0m 191719.0 \n",
       "\n",
       "('BJ00000033',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.81733980903372\n",
       "[8] numerical analysis:\n",
       "prediction:  1182354.0  true:  1190001.0 intersection:  969510.0 dice:  0.8173397320384175\n",
       "\u001b[31m--- false negative: \u001b[0m 220491.0 \u001b[31m--- false positive: \u001b[0m 212844.0 \n",
       "\n",
       "('BJ00000120',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.613709104203163\n",
       "[8] numerical analysis:\n",
       "prediction:  979475.0  true:  1227933.0 intersection:  677353.0 dice:  0.6137089292056566\n",
       "\u001b[31m--- false negative: \u001b[0m 550580.0 \u001b[31m--- false positive: \u001b[0m 302122.0 \n",
       "\n",
       "('BJ00000016',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.5289263388141133\n",
       "[8] numerical analysis:\n",
       "prediction:  1249382.0  true:  2371452.0 intersection:  957577.0 dice:  0.5289262087132413\n",
       "\u001b[31m--- false negative: \u001b[0m 1413875.0 \u001b[31m--- false positive: \u001b[0m 291805.0 \n",
       "\n",
       "('BJ00000044',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.439365895962372\n",
       "[8] numerical analysis:\n",
       "prediction:  773508.0  true:  1709112.0 intersection:  545389.0 dice:  0.43936567013880495\n",
       "\u001b[31m--- false negative: \u001b[0m 1163723.0 \u001b[31m--- false positive: \u001b[0m 228119.0 \n",
       "\n",
       "('BJ00000114',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.6042410933625127\n",
       "[8] numerical analysis:\n",
       "prediction:  907025.0  true:  1494666.0 intersection:  725600.0 dice:  0.604240928579072\n",
       "\u001b[31m--- false negative: \u001b[0m 769066.0 \u001b[31m--- false positive: \u001b[0m 181425.0 \n",
       "\n",
       "('BJ00000100',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9338263057311641\n",
       "[8] numerical analysis:\n",
       "prediction:  1487384.0  true:  1359698.0 intersection:  1329340.0 dice:  0.9338262824885268\n",
       "\u001b[31m--- false negative: \u001b[0m 30358.0 \u001b[31m--- false positive: \u001b[0m 158044.0 \n",
       "\n",
       "('BJ00000002',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n",
       "[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n",
       "[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n",
       "[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n",
       "[6] one-hot encoding of origin:  (304, 256, 320, 2)\n",
       "[7] dice of muscle:  0.9226532556580177\n",
       "[8] numerical analysis:\n",
       "prediction:  1307125.0  true:  1208817.0 intersection:  1160671.0 dice:  0.9226532249153597\n",
       "\u001b[31m--- false negative: \u001b[0m 48146.0 \u001b[31m--- false positive: \u001b[0m 146454.0 \n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "('BJ00000075',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.7073269423695667\n[8] numerical analysis:\nprediction:  1585377.0  true:  2235329.0 intersection:  1351244.0 dice:  0.7073268657677403\n\u001b[31m--- false negative: \u001b[0m 884085.0 \u001b[31m--- false positive: \u001b[0m 234133.0 \n\n('BJ00000060',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.412001094481966\n[8] numerical analysis:\nprediction:  976975.0  true:  2231848.0 intersection:  661019.0 dice:  0.41200091123754723\n\u001b[31m--- false negative: \u001b[0m 1570829.0 \u001b[31m--- false positive: \u001b[0m 315956.0 \n\n('BJ00000054',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.36696324066331304\n[8] numerical analysis:\nprediction:  1053608.0  true:  2153685.0 intersection:  588479.0 dice:  0.3669630432891538\n\u001b[31m--- false negative: \u001b[0m 1565206.0 \u001b[31m--- false positive: \u001b[0m 465129.0 \n\n('BJ00000050',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.7864822696508544\n[8] numerical analysis:\nprediction:  1486415.0  true:  1787462.0 intersection:  1287423.0 dice:  0.7864822044322374\n\u001b[31m--- false negative: \u001b[0m 500039.0 \u001b[31m--- false positive: \u001b[0m 198992.0 \n\n('BJ00000096',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.7268121668137858\n[8] numerical analysis:\nprediction:  1072055.0  true:  1522214.0 intersection:  942773.0 dice:  0.7268120615094271\n\u001b[31m--- false negative: \u001b[0m 579441.0 \u001b[31m--- false positive: \u001b[0m 129282.0 \n\n('BJ00000071',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9533588915141971\n[8] numerical analysis:\nprediction:  1481629.0  true:  1434853.0 intersection:  1390227.0 dice:  0.9533588755219473\n\u001b[31m--- false negative: \u001b[0m 44626.0 \u001b[31m--- false positive: \u001b[0m 91402.0 \n\n('BJ00000150',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9503041407997156\n[8] numerical analysis:\nprediction:  1537019.0  true:  1432986.0 intersection:  1411204.0 dice:  0.9503041240671312\n\u001b[31m--- false negative: \u001b[0m 21782.0 \u001b[31m--- false positive: \u001b[0m 125815.0 \n\n('BJ00000077',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9132995002248623\n[8] numerical analysis:\nprediction:  1504629.0  true:  1370463.0 intersection:  1312910.0 dice:  0.9132994700691317\n\u001b[31m--- false negative: \u001b[0m 57553.0 \u001b[31m--- false positive: \u001b[0m 191719.0 \n\n('BJ00000033',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.81733980903372\n[8] numerical analysis:\nprediction:  1182354.0  true:  1190001.0 intersection:  969510.0 dice:  0.8173397320384175\n\u001b[31m--- false negative: \u001b[0m 220491.0 \u001b[31m--- false positive: \u001b[0m 212844.0 \n\n('BJ00000120',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.613709104203163\n[8] numerical analysis:\nprediction:  979475.0  true:  1227933.0 intersection:  677353.0 dice:  0.6137089292056566\n\u001b[31m--- false negative: \u001b[0m 550580.0 \u001b[31m--- false positive: \u001b[0m 302122.0 \n\n('BJ00000016',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.5289263388141133\n[8] numerical analysis:\nprediction:  1249382.0  true:  2371452.0 intersection:  957577.0 dice:  0.5289262087132413\n\u001b[31m--- false negative: \u001b[0m 1413875.0 \u001b[31m--- false positive: \u001b[0m 291805.0 \n\n('BJ00000044',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.439365895962372\n[8] numerical analysis:\nprediction:  773508.0  true:  1709112.0 intersection:  545389.0 dice:  0.43936567013880495\n\u001b[31m--- false negative: \u001b[0m 1163723.0 \u001b[31m--- false positive: \u001b[0m 228119.0 \n\n('BJ00000114',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.6042410933625127\n[8] numerical analysis:\nprediction:  907025.0  true:  1494666.0 intersection:  725600.0 dice:  0.604240928579072\n\u001b[31m--- false negative: \u001b[0m 769066.0 \u001b[31m--- false positive: \u001b[0m 181425.0 \n\n('BJ00000100',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9338263057311641\n[8] numerical analysis:\nprediction:  1487384.0  true:  1359698.0 intersection:  1329340.0 dice:  0.9338262824885268\n\u001b[31m--- false negative: \u001b[0m 30358.0 \u001b[31m--- false positive: \u001b[0m 158044.0 \n\n('BJ00000002',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 304, 256, 320]) torch.Size([1, 2, 304, 256, 320])\n[2] shape of logits:  torch.Size([1, 2, 304, 256, 320])\n[3] argmax of fat_prediction:  torch.Size([1, 304, 256, 320])\n[4] reshape of fat_argmax:  torch.Size([304, 256, 320])\n[5] one-hot encoding of prediction:  (304, 256, 320, 2)\n[6] one-hot encoding of origin:  (304, 256, 320, 2)\n[7] dice of muscle:  0.9226532556580177\n[8] numerical analysis:\nprediction:  1307125.0  true:  1208817.0 intersection:  1160671.0 dice:  0.9226532249153597\n\u001b[31m--- false negative: \u001b[0m 48146.0 \u001b[31m--- false positive: \u001b[0m 146454.0 \n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# epoch35_trainloss_0.037_validacc_0.961.pth\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, x, y in test_loader:\n",
    "#       if idx[0] == 'BJ00000060':\n",
    "        print(idx)\n",
    "        print('[1] shape of fat and fat_label: ', x.shape, y.shape)\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        with autocast():\n",
    "            # Step 1, predict\n",
    "            logits = model(x.to(next(model.parameters()).device))\n",
    "            logits = F.softmax(logits, dim=1)\n",
    "            print('[2] shape of logits: ', logits.shape)\n",
    "            \n",
    "            # Step 2, argmax\n",
    "            fat_argmax = np.argmax(logits.cpu(), axis=1)\n",
    "            print('[3] argmax of fat_prediction: ', fat_argmax.shape)\n",
    "            fat_argmax = np.reshape(fat_argmax, dim)\n",
    "            print('[4] reshape of fat_argmax: ', fat_argmax.shape)\n",
    "            \n",
    "            # Step 3, one-hot encoding\n",
    "            fat_argmax = zoom(fat_argmax, (304/fat_argmax.shape[0], 256/fat_argmax.shape[1], 320/fat_argmax.shape[2]), order=0, mode='nearest') # resize to [288,260,320]\n",
    "            prediction_label_origin = one_hot_encoding(fat_argmax)\n",
    "            print('[5] one-hot encoding of prediction: ', prediction_label_origin.shape)\n",
    "            input_y = np.load(data_path + '/' + idx[0] + '/water_label.npy')\n",
    "            slice_y = input_y[slice_interval[0]:slice_interval[1], 2:-2, :]\n",
    "            img_fat_label = one_hot_encoding(slice_y)\n",
    "            print('[6] one-hot encoding of origin: ', img_fat_label.shape)\n",
    "            \n",
    "            # Step 6, output dice for each category\n",
    "            print('[7] dice of muscle: ', dice_coef(img_fat_label[:,:,:,1], prediction_label_origin[:,:,:,1]))\n",
    "            print('[8] numerical analysis:')\n",
    "            numerical_analysis(img_fat_label, prediction_label_origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b64c0063-c613-4cab-b3d1-56833fdbc7c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b238c903-d3c0-46bf-83d9-460030022250",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# --- run single case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaa6c315-f9f5-448f-939a-0543ff4cc705",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BJ00000075',)\n",
       "[1] shape of fat and fat_label:  torch.Size([1, 1, 288, 208, 256]) torch.Size([1, 2, 288, 208, 256])\n",
       "[2] shape of logits:  torch.Size([1, 2, 288, 208, 256])\n",
       "[3] argmax of fat_prediction:  torch.Size([1, 288, 208, 256])\n",
       "[4] reshape of fat_argmax:  torch.Size([288, 208, 256])\n",
       "[5] one-hot encoding of prediction:  (288, 260, 320, 2)\n",
       "[6] one-hot encoding of origin:  (288, 260, 320, 2)\n",
       "[7] dice of muscle:  0.9105527919746781\n",
       "[8] numerical analysis:\n",
       "prediction:  2440769.0  true:  2235329.0 intersection:  2128917.0 dice:  0.910552772846078\n",
       "\u001b[31m--- false negative: \u001b[0m 106412.0 \u001b[31m--- false positive: \u001b[0m 311852.0 \n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "('BJ00000075',)\n[1] shape of fat and fat_label:  torch.Size([1, 1, 288, 208, 256]) torch.Size([1, 2, 288, 208, 256])\n[2] shape of logits:  torch.Size([1, 2, 288, 208, 256])\n[3] argmax of fat_prediction:  torch.Size([1, 288, 208, 256])\n[4] reshape of fat_argmax:  torch.Size([288, 208, 256])\n[5] one-hot encoding of prediction:  (288, 260, 320, 2)\n[6] one-hot encoding of origin:  (288, 260, 320, 2)\n[7] dice of muscle:  0.9105527919746781\n[8] numerical analysis:\nprediction:  2440769.0  true:  2235329.0 intersection:  2128917.0 dice:  0.910552772846078\n\u001b[31m--- false negative: \u001b[0m 106412.0 \u001b[31m--- false positive: \u001b[0m 311852.0 \n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pid = None\n",
    "prediction_label_origin = None\n",
    "img_fat_label = None\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, x, y in test_loader:\n",
    "      if idx[0] == 'BJ00000075':\n",
    "        print(idx)\n",
    "        pid = idx[0]\n",
    "        print('[1] shape of fat and fat_label: ', x.shape, y.shape)\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        with autocast():\n",
    "            # Step 1, predict\n",
    "            logits = model(x.to(next(model.parameters()).device))\n",
    "            logits = F.softmax(logits, dim=1)\n",
    "            print('[2] shape of logits: ', logits.shape)\n",
    "            \n",
    "            # Step 2, argmax\n",
    "            fat_argmax = np.argmax(logits.cpu(), axis=1)\n",
    "            print('[3] argmax of fat_prediction: ', fat_argmax.shape)\n",
    "            fat_argmax = np.reshape(fat_argmax, dim)\n",
    "            print('[4] reshape of fat_argmax: ', fat_argmax.shape)\n",
    "            \n",
    "            # Step 3, one-hot encoding\n",
    "            fat_argmax = zoom(fat_argmax, (288/fat_argmax.shape[0], 260/fat_argmax.shape[1], 320/fat_argmax.shape[2]), order=0, mode='nearest') # resize to [288,260,320]\n",
    "            prediction_label_origin = one_hot_encoding(fat_argmax)\n",
    "            print('[5] one-hot encoding of prediction: ', prediction_label_origin.shape)\n",
    "            input_y = np.load(data_path + '/' + idx[0] + '/water_label.npy')\n",
    "            slice_y = input_y[slice_interval[0]:slice_interval[1], :, :]\n",
    "            img_fat_label = one_hot_encoding(slice_y)\n",
    "            print('[6] one-hot encoding of origin: ', img_fat_label.shape)\n",
    "            \n",
    "            # Step 6, output dice for each category\n",
    "            print('[7] dice of muscle: ', dice_coef(img_fat_label[:,:,:,1], prediction_label_origin[:,:,:,1]))\n",
    "            print('[8] numerical analysis:')\n",
    "            numerical_analysis(img_fat_label, prediction_label_origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dfe19e1-9ef7-426c-becb-5e608e82129d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "[0] torch_vnet_float_parallel_water_full_size",
   "notebookOrigID": 424248,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
